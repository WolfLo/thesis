\BOOKMARK [0][-]{chapter.1}{Preliminaries}{}% 1
\BOOKMARK [1][-]{section.1.1}{Multi Armed Bandits}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Exploration and exploitation}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Stochastic Bandits With Finitely Many Arms}{section.1.1}% 4
\BOOKMARK [2][-]{subsection.1.1.3}{X-armed bandits}{section.1.1}% 5
\BOOKMARK [1][-]{section.1.2}{Markov Decision Processes}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.2.1}{Policies}{section.1.2}% 7
\BOOKMARK [2][-]{subsection.1.2.2}{Performance}{section.1.2}% 8
\BOOKMARK [2][-]{subsection.1.2.3}{Value functions}{section.1.2}% 9
\BOOKMARK [1][-]{section.1.3}{Reinforcement Learning}{chapter.1}% 10
\BOOKMARK [2][-]{subsection.1.3.1}{Problem formulation}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.2}{Taxonomy}{section.1.3}% 12
\BOOKMARK [1][-]{section.1.4}{Policy Search}{chapter.1}% 13
\BOOKMARK [2][-]{subsection.1.4.1}{Overview}{section.1.4}% 14
\BOOKMARK [2][-]{subsection.1.4.2}{Policy gradient}{section.1.4}% 15
\BOOKMARK [2][-]{subsection.1.4.3}{Policy gradient estimation}{section.1.4}% 16
\BOOKMARK [2][-]{subsection.1.4.4}{Algorithms}{section.1.4}% 17
\BOOKMARK [1][-]{section.1.5}{Multiple Importance Sampling}{chapter.1}% 18
\BOOKMARK [0][-]{chapter.2}{Exploration Techniques}{}% 19
\BOOKMARK [1][-]{section.2.1}{Exploration in Multi Armed Bandits}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.1.1}{Undirected Exploration}{section.2.1}% 21
\BOOKMARK [2][-]{subsection.2.1.2}{Count-based exploration and Upper Confidence Bound}{section.2.1}% 22
\BOOKMARK [2][-]{subsection.2.1.3}{Optimism in the Face of Uncertainty}{section.2.1}% 23
\BOOKMARK [2][-]{subsection.2.1.4}{ Hierarchical Optimistic Optimization}{section.2.1}% 24
\BOOKMARK [2][-]{subsection.2.1.5}{Posterior Sampling}{section.2.1}% 25
\BOOKMARK [2][-]{subsection.2.1.6}{Gaussian Process Upper Confidence Bound}{section.2.1}% 26
\BOOKMARK [1][-]{section.2.2}{Exploration in Reinforcement Learning}{chapter.2}% 27
\BOOKMARK [2][-]{subsection.2.2.1}{Undirected Exploration}{section.2.2}% 28
\BOOKMARK [2][-]{subsection.2.2.2}{Count-based exploration and Intrinsic Motivation}{section.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.3}{Posterior Sampling}{section.2.2}% 30
\BOOKMARK [0][-]{chapter.3}{Optimistic Policy Optimization via Multiple Importance Sampling}{}% 31
\BOOKMARK [1][-]{section.3.1}{Robust Importance Sampling Estimation}{chapter.3}% 32
\BOOKMARK [1][-]{section.3.2}{Problem Formalization}{chapter.3}% 33
\BOOKMARK [1][-]{section.3.3}{Algorithms}{chapter.3}% 34
\BOOKMARK [1][-]{section.3.4}{Regret Analysis}{chapter.3}% 35
\BOOKMARK [2][-]{subsection.3.4.1}{Discrete arm set}{section.3.4}% 36
\BOOKMARK [2][-]{subsection.3.4.2}{Compact arm set}{section.3.4}% 37
\BOOKMARK [2][-]{subsection.3.4.3}{Discretization}{section.3.4}% 38
\BOOKMARK [0][-]{chapter.4}{Numerical Simulations}{}% 39
\BOOKMARK [1][-]{section.4.1}{Practical Aspects}{chapter.4}% 40
\BOOKMARK [2][-]{subsection.4.1.1}{Divergence Between Gaussian Multivariate Distributions}{section.4.1}% 41
\BOOKMARK [2][-]{subsection.4.1.2}{Uniformly Bounded R\351nyi divergence}{section.4.1}% 42
\BOOKMARK [1][-]{section.4.2}{Action-based setting}{chapter.4}% 43
\BOOKMARK [1][-]{section.4.3}{Linear Quadratic Gaussian Regulator}{chapter.4}% 44
\BOOKMARK [0][-]{section*.7}{Appendices}{}% 45
\BOOKMARK [0][-]{Appendix.1.A}{Proofs}{}% 46
\BOOKMARK [0][-]{chapter*.8}{Bibliography}{}% 47
