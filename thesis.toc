\select@language {english}
\contentsline {chapter}{\numberline {1}Preliminaries}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Multi Armed Bandits}{1}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Exploration and exploitation}{3}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Stochastic Bandits With Finitely Many Arms}{4}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}$\mathcal {X}$-armed bandits}{5}{subsection.1.1.3}
\contentsline {section}{\numberline {1.2}Markov Decision Processes}{6}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Policies}{8}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Performance}{9}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Value functions}{10}{subsection.1.2.3}
\contentsline {section}{\numberline {1.3}Reinforcement Learning}{11}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Problem formulation}{11}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Taxonomy}{12}{subsection.1.3.2}
\contentsline {section}{\numberline {1.4}Policy Search}{14}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Overview}{15}{subsection.1.4.1}
\contentsline {subsection}{\numberline {1.4.2}Policy gradient}{16}{subsection.1.4.2}
\contentsline {subsection}{\numberline {1.4.3}Policy gradient estimation}{19}{subsection.1.4.3}
\contentsline {subsection}{\numberline {1.4.4}Algorithms}{21}{subsection.1.4.4}
\contentsline {chapter}{\numberline {2}Exploration Techniques}{25}{chapter.2}
\contentsline {section}{\numberline {2.1}Exploration in Multi Armed Bandits}{26}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Undirected Exploration}{26}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Count-based exploration and Upper Confidence Bound}{27}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Optimism in the Face of Uncertainty}{29}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4} Hierarchical Optimistic Optimization}{30}{subsection.2.1.4}
\contentsline {subsection}{\numberline {2.1.5}Posterior Sampling}{32}{subsection.2.1.5}
\contentsline {subsection}{\numberline {2.1.6}Gaussian Process Upper Confidence Bound}{34}{subsection.2.1.6}
\contentsline {section}{\numberline {2.2}Exploration in Reinforcement Learning}{36}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Undirected Exploration}{36}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Counter-based exploration and OFU}{37}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Value Difference and Recency-based exploration}{38}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Intrinsic Motivation}{38}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Thompson Sampling}{38}{subsection.2.2.5}
\contentsline {chapter}{Bibliography}{39}{chapter*.4}
