\select@language {english}
\contentsline {chapter}{\numberline {1}Preliminaries}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Multi Armed Bandits}{1}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Exploration and exploitation}{3}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Stochastic Bandits With Finitely Many Arms}{4}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}$\mathcal {X}$-armed bandits}{5}{subsection.1.1.3}
\contentsline {section}{\numberline {1.2}Markov Decision Processes}{6}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Policies}{8}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Performance}{9}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Value functions}{10}{subsection.1.2.3}
\contentsline {section}{\numberline {1.3}Reinforcement Learning}{11}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Problem formulation}{11}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Taxonomy}{12}{subsection.1.3.2}
\contentsline {section}{\numberline {1.4}Policy Search}{14}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Overview}{15}{subsection.1.4.1}
\contentsline {subsection}{\numberline {1.4.2}Policy gradient}{16}{subsection.1.4.2}
\contentsline {subsection}{\numberline {1.4.3}Policy gradient estimation}{19}{subsection.1.4.3}
\contentsline {subsection}{\numberline {1.4.4}Algorithms}{21}{subsection.1.4.4}
\contentsline {section}{\numberline {1.5}Multiple Importance Sampling}{23}{section.1.5}
\contentsline {chapter}{\numberline {2}Exploration Techniques}{27}{chapter.2}
\contentsline {section}{\numberline {2.1}Exploration in Multi Armed Bandits}{28}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Undirected Exploration}{28}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Count-based exploration and Upper Confidence Bound}{29}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Optimism in the Face of Uncertainty}{31}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4} Hierarchical Optimistic Optimization}{32}{subsection.2.1.4}
\contentsline {subsection}{\numberline {2.1.5}Posterior Sampling}{34}{subsection.2.1.5}
\contentsline {subsection}{\numberline {2.1.6}Gaussian Process Upper Confidence Bound}{36}{subsection.2.1.6}
\contentsline {section}{\numberline {2.2}Exploration in Reinforcement Learning}{38}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Undirected Exploration}{38}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Count-based exploration and Intrinsic Motivation}{39}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Posterior Sampling}{44}{subsection.2.2.3}
\contentsline {chapter}{\numberline {3}Optimistic Policy Optimization via Multiple Importance Sampling}{45}{chapter.3}
\contentsline {section}{\numberline {3.1}Robust Importance Sampling Estimation}{45}{section.3.1}
\contentsline {section}{\numberline {3.2}Problem Formalization}{47}{section.3.2}
\contentsline {section}{\numberline {3.3}Algorithms}{48}{section.3.3}
\contentsline {section}{\numberline {3.4}Regret Analysis}{49}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Discrete arm set}{50}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Compact arm set}{51}{subsection.3.4.2}
\contentsline {subsection}{\numberline {3.4.3}Discretization}{52}{subsection.3.4.3}
\contentsline {chapter}{Appendices}{54}{section*.4}
\contentsline {chapter}{\numberline {A}Implementation}{54}{Appendix.1.A}
\contentsline {section}{\numberline {A.1}Upper Bound for the Exponentiated Renyi Divergence between mixtures}{54}{section.1.A.1}
\contentsline {section}{\numberline {A.2}Proofs}{54}{section.1.A.2}
\contentsline {chapter}{Bibliography}{69}{chapter*.5}
