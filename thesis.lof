\select@language {english}
\select@language {italian}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Interaction protocol for Markov decision processes\relax }}{12}{figure.caption.8}
\contentsline {figure}{\numberline {2.2}{\ignorespaces A taxonomy of \gls {PS} methods \cite {deisenroth2013survey}\relax }}{21}{figure.caption.9}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Cumulative regret in the \gls {LQG} experiment. Comparison between \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning the hyperpolicy mean. (30 runs, 95\% c.i.)\relax }}{67}{figure.caption.11}
\contentsline {figure}{\numberline {5.2}{\ignorespaces The gain parameter $\mu $ selected at each iteration of \gls {GPUCB} (left) and \gls {OPTIMIST} (right) in the \gls {LQG} experiment.\relax }}{67}{figure.caption.12}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Mean return of arms $\mu \in [-0.9,-0,5]$, calculated by averaging the return collected over 2000 trajectories in the \gls {LQG} experiment.\relax }}{68}{figure.caption.13}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Cumulative regret in the \gls {LQG} experiment, comparing \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning both the mean and the standard deviation hyperparameters. (30 runs, 95\% c.i.)\relax }}{70}{figure.caption.14}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Graphical representation of the Mountain Car problem \cite {brockman2016openai}.\relax }}{70}{figure.caption.15}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Cumulative regret in the Continuous Mountain Car experiment. Comparison between \gls {OPTIMIST}, \gls {PGPE} and \gls {PBPOIS} when learning the two-dimensional hyperpolicy mean. (5 runs, 95\% c.i.)\relax }}{71}{figure.caption.17}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Gaussian kernel density estimation \cite {scott2015multivariate} of the probability distribution of the arm set $\bm {\xi }=\bm {\mu }$ induced by \gls {PGPE} (left) and \gls {OPTIMIST} (right).\relax }}{73}{figure.caption.18}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Graphical representation of the Inverted Pendulum task \cite {wawrzynski2005intensive}.\relax }}{74}{figure.caption.20}
\contentsline {figure}{\numberline {5.9}{\ignorespaces The hyperpolicy mean parameter selected at each iteration of \gls {OPTIMIST} in the Inverted Pendulum experiment.\relax }}{77}{figure.caption.22}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Truncated \gls {MIS} estimator (left) and exploration bonus (right) of the arms selected by \gls {OPTIMIST} at each iteration of the Inverted Pendulum experiment.\relax }}{77}{figure.caption.23}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
