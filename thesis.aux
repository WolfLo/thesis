\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.xdy}
\@glsorder{word}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{deisenroth2013survey}
\citation{gym}
\citation{scott2015multivariate}
\citation{wawrzynski2005intensive}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vi}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vii}{chapter*.3}}
\@writefile{toc}{\contentsline {chapter}{List of Algorithms}{viii}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{Acronyms}{ix}{section*.5}}
\citation{goodfellow2016deep}
\citation{sutton2018reinforcement}
\citation{thrun1992efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{deisenroth2013survey}
\citation{sutton2000policy}
\citation{sehnke2008policy}
\citation{silver2014deterministic}
\citation{schulman2015trust}
\citation{mnih2016asynchronous}
\citation{espeholt2018impala}
\citation{ziebart2008maximum}
\citation{haarnoja2017reinforcement}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation and goal}{2}{section.1.1}}
\citation{auer2002finite}
\citation{lattimore2019bandit}
\citation{lai1985asymptotically}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Preliminaries}{{2}{4}{Preliminaries}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Multi Armed Bandits}{4}{section.2.1}}
\newlabel{sec:mab}{{2.1}{4}{Multi Armed Bandits}{section.2.1}{}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\numberline {2.1.1}Definition}{5}{definition.2.1.1}}
\@writefile{loe}{\contentsline {definition}{\numberline {2.1.2}Definition}{5}{definition.2.1.2}}
\newlabel{def:immediateregret}{{2.1.2}{5}{}{definition.2.1.2}{}}
\citation{lattimore2019bandit}
\@writefile{loe}{\contentsline {definition}{\numberline {2.1.3}Definition}{6}{definition.2.1.3}}
\newlabel{def:regret}{{2.1.3}{6}{}{definition.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Exploration and exploitation}{6}{subsection.2.1.1}}
\newlabel{eedilemma}{{2.1.1}{6}{Exploration and exploitation}{subsection.2.1.1}{}}
\citation{lattimore2019bandit}
\citation{lattimore2019bandit}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Stochastic Bandits With Finitely Many Arms}{7}{subsection.2.1.2}}
\newlabel{finiteMABs}{{2.1.2}{7}{Stochastic Bandits With Finitely Many Arms}{subsection.2.1.2}{}}
\newlabel{eq:averagereward}{{2.3}{7}{Stochastic Bandits With Finitely Many Arms}{equation.2.1.3}{}}
\newlabel{thmt@@regretETC@data}{{\def \theequation {2.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )2.1.\@arabic {\c@equation }}\setcounter {equation}{3}}{7}{Stochastic Bandits With Finitely Many Arms}{algorithm.2.1}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {2.1.1}Theorem}{7}{theorem.2.1.1}}
\newlabel{thmt@@regretETC}{{2.1.1}{7}{}{theorem.2.1.1}{}}
\newlabel{th:regretETC}{{2.1.1}{7}{}{theorem.2.1.1}{}}
\citation{agrawal1995continuum}
\citation{magureanu2014lipschitz}
\citation{bubeck2011x}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Explore-then-commit\relax }}{8}{algorithm.2.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:explorethencommit}{{2.1}{8}{Explore-then-commit\relax }{algorithm.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}$\mathcal  {X}$-armed bandits}{8}{subsection.2.1.3}}
\newlabel{sub:xarmedbandits}{{2.1.3}{8}{$\mathcal {X}$-armed bandits}{subsection.2.1.3}{}}
\citation{puterman2014markov}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{9}{section.2.2}}
\@writefile{loe}{\contentsline {definition}{\numberline {2.2.1}Definition}{9}{definition.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Interaction protocol for Markov decision processes\relax }}{10}{figure.caption.7}}
\newlabel{fig:MDP}{{2.1}{10}{Interaction protocol for Markov decision processes\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Policies}{11}{subsection.2.2.1}}
\@writefile{loe}{\contentsline {definition}{\numberline {2.2.2}Definition}{11}{definition.2.2.2}}
\@writefile{loe}{\contentsline {remark}{\numberline {2.2.1}Remark}{11}{remark.2.2.1}}
\citation{sutton2000policy}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Performance}{12}{subsection.2.2.2}}
\newlabel{eq:Jstartstate}{{2.6}{12}{Performance}{equation.2.2.6}{}}
\newlabel{eq:ssdistribution}{{2.7}{12}{Performance}{equation.2.2.7}{}}
\citation{baird1993advantage}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Value functions}{13}{subsection.2.2.3}}
\newlabel{eq:BellmanV}{{2.11}{13}{Value functions}{equation.2.2.11}{}}
\newlabel{eq:BellmanQ}{{2.13}{13}{Value functions}{equation.2.2.13}{}}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning}{14}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Problem formulation}{14}{subsection.2.3.1}}
\citation{puterman2014markov}
\citation{sutton2018reinforcement}
\newlabel{eq:optimalpolicy}{{2.15}{15}{Problem formulation}{equation.2.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Taxonomy}{15}{subsection.2.3.2}}
\citation{sutton1991dyna}
\citation{kupcsik2013data}
\citation{watkins1989learning}
\citation{rummery1994line}
\citation{williams1992simple}
\citation{watkins1989learning}
\citation{degris2012off}
\citation{rummery1994line}
\citation{sutton1988learning}
\citation{williams1992simple}
\citation{baxter2001infinite}
\citation{sutton2018reinforcement}
\citation{antos2008fitted}
\citation{grondman2012survey}
\citation{lange2012batch}
\citation{more1994line}
\citation{deisenroth2013survey}
\citation{sutton2000policy}
\citation{dayan1997using}
\citation{kober2009policy}
\citation{still2012information}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Policy Search}{18}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Overview}{18}{subsection.2.4.1}}
\newlabel{subsec:PSoverview}{{2.4.1}{18}{Overview}{subsection.2.4.1}{}}
\newlabel{eq:PSproblem}{{2.16}{18}{Overview}{equation.2.4.16}{}}
\citation{deisenroth2013survey}
\citation{deisenroth2013survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A taxonomy of \gls {PS} methods \cite  {deisenroth2013survey}\relax }}{19}{figure.caption.8}}
\newlabel{fig:PStaxonomy}{{2.2}{19}{A taxonomy of \gls {PS} methods \cite {deisenroth2013survey}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Policy gradient}{19}{subsection.2.4.2}}
\citation{sutton2000policy}
\newlabel{eq:paramsupdate}{{2.20}{20}{Policy gradient}{equation.2.4.20}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {2.4.1}Theorem}{20}{theorem.2.4.1}}
\newlabel{th:PGtheorem}{{2.4.1}{20}{}{theorem.2.4.1}{}}
\newlabel{eq:PGtheorem}{{2.21}{20}{}{equation.2.4.21}{}}
\citation{williams1992simple}
\citation{sutton2000policy}
\citation{amari1998natural1}
\newlabel{eq:reinforcetrick}{{2.21}{21}{Policy gradient}{equation.2.4.21}{}}
\newlabel{eq:trajectoryPG}{{2.24}{21}{Policy gradient}{equation.2.4.24}{}}
\newlabel{eq:logpolicyestimate}{{2.26}{21}{Policy gradient}{equation.2.4.26}{}}
\citation{amari1998natural1}
\citation{amari1998natural2}
\citation{peters2008reinforcement}
\citation{glynn1990likelihood}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Policy gradient estimation}{22}{subsection.2.4.3}}
\citation{williams1992simple}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces Generic \gls {PG} algorithm\relax }}{23}{algorithm.2.2}}
\newlabel{alg:PG}{{2.2}{23}{Generic \gls {PG} algorithm\relax }{algorithm.2.2}{}}
\newlabel{step:PGestimation}{{5}{23}{Generic \gls {PG} algorithm\relax }{algorithm.2.2}{}}
\citation{williams1992simple}
\citation{peters2008reinforcement}
\citation{zhao2011analysis}
\newlabel{eq:PGbaseline}{{2.29}{24}{Policy gradient estimation}{equation.2.4.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Algorithms}{24}{subsection.2.4.4}}
\newlabel{subsec:algorithms}{{2.4.4}{24}{Algorithms}{subsection.2.4.4}{}}
\newlabel{eq:REINFORCE}{{2.30}{24}{Algorithms}{equation.2.4.30}{}}
\citation{baxter2001infinite}
\citation{sutton2000policy}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.3}{\ignorespaces Episodic REINFORCE with component-wise optimal baseline.\relax }}{25}{algorithm.2.3}}
\newlabel{alg:REINFORCE}{{2.3}{25}{Episodic REINFORCE with component-wise optimal baseline.\relax }{algorithm.2.3}{}}
\citation{peters2008reinforcement}
\citation{zhao2011analysis}
\citation{sehnke2008policy}
\citation{sehnke2010parameter}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.4}{\ignorespaces Episodic \gls {PGPE}\relax }}{27}{algorithm.2.4}}
\newlabel{alg:PGPE}{{2.4}{27}{Episodic \gls {PGPE}\relax }{algorithm.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Multiple Importance Sampling}{27}{section.2.5}}
\newlabel{sec:mis}{{2.5}{27}{Multiple Importance Sampling}{section.2.5}{}}
\citation{cochran2007sampling}
\citation{mcbook}
\citation{mcbook}
\citation{veach_optimally_1995}
\citation{veach_optimally_1995}
\newlabel{eq:ise}{{2.37}{28}{Multiple Importance Sampling}{equation.2.5.37}{}}
\newlabel{eq:mise}{{2.38}{28}{Multiple Importance Sampling}{equation.2.5.38}{}}
\newlabel{eq:bhw}{{2.39}{28}{Multiple Importance Sampling}{equation.2.5.39}{}}
\citation{veach_optimally_1995}
\citation{renyi1961measures}
\citation{cortes2010learning}
\citation{metelli2018policy}
\newlabel{eq:bhe}{{2.40}{29}{Multiple Importance Sampling}{equation.2.5.40}{}}
\newlabel{eq:renyi}{{2.41}{29}{Multiple Importance Sampling}{equation.2.5.41}{}}
\newlabel{thmt@@misevarbound@data}{{\def \theequation {2.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )2.5.\@arabic {\c@equation }}\setcounter {equation}{43}}{29}{Multiple Importance Sampling}{equation.2.5.43}{}}
\@writefile{loe}{\contentsline {lemma}{\numberline {2.5.1}Lemma}{29}{lemma.2.5.1}}
\newlabel{thmt@@misevarbound}{{2.5.1}{29}{}{lemma.2.5.1}{}}
\newlabel{lem:misevarbound}{{2.5.1}{29}{}{lemma.2.5.1}{}}
\citation{thrun1992efficient}
\citation{whitehead1991study}
\citation{thrun1992efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Exploration Techniques}{31}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:sota}{{3}{31}{Exploration Techniques}{chapter.3}{}}
\citation{lattimore2019bandit}
\citation{auer2002finite}
\citation{cesa2017boltzmann}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Exploration in Multi Armed Bandits}{32}{section.3.1}}
\newlabel{sec:expinmab}{{3.1}{32}{Exploration in Multi Armed Bandits}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Undirected Exploration}{32}{subsection.3.1.1}}
\citation{lai1985asymptotically}
\citation{agrawal1995continuum}
\citation{auer2002finite}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Count-based exploration and Upper Confidence Bound}{33}{subsection.3.1.2}}
\newlabel{sebsec:count&UCB}{{3.1.2}{33}{Count-based exploration and Upper Confidence Bound}{subsection.3.1.2}{}}
\citation{lattimore2019bandit}
\citation{auer2002finite}
\citation{agrawal1995continuum}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\numberline {3.1.1}Definition}{34}{definition.3.1.1}}
\newlabel{def:subgaussianity}{{3.1.1}{34}{}{definition.3.1.1}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {3.1.1}Theorem}{34}{theorem.3.1.1}}
\@writefile{loe}{\contentsline {corollary}{\numberline {3.1.1}Corollary}{34}{corollary.3.1.1}}
\newlabel{cor:subgaussian}{{3.1.1}{34}{}{corollary.3.1.1}{}}
\newlabel{eq:ucb}{{3.4}{34}{Count-based exploration and Upper Confidence Bound}{equation.3.1.4}{}}
\citation{auer2002finite}
\citation{auer2010ucb}
\citation{garivier2011kl}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces \gls {UCB}($\delta $) algorithm\relax }}{35}{algorithm.3.1}}
\newlabel{alg:ucb}{{3.1}{35}{\gls {UCB}($\delta $) algorithm\relax }{algorithm.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Optimism in the Face of Uncertainty}{35}{subsection.3.1.3}}
\citation{bubeck2011x}
\citation{kleinberg2005nearly}
\citation{kleinberg2008multi}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4} Hierarchical Optimistic Optimization}{36}{subsection.3.1.4}}
\newlabel{sub:hoo}{{3.1.4}{36}{Hierarchical Optimistic Optimization}{subsection.3.1.4}{}}
\@writefile{loe}{\contentsline {definition}{\numberline {3.1.2}Definition}{37}{definition.3.1.2}}
\newlabel{def:treeofcoverings}{{3.1.2}{37}{}{definition.3.1.2}{}}
\@writefile{loe}{\contentsline {remark}{\numberline {3.1.1}Remark}{37}{remark.3.1.1}}
\citation{bubeck2011x}
\@writefile{loe}{\contentsline {assumption}{\numberline {3.1.1}Assumption}{38}{assumption.3.1.1}}
\newlabel{eq:weaklipschitz}{{3.12}{38}{}{equation.3.1.12}{}}
\citation{thompson1933likelihood}
\citation{russo2013eluder}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces \gls {HOO} algorithm\relax }}{39}{algorithm.3.2}}
\newlabel{alg:hoo}{{3.2}{39}{\gls {HOO} algorithm\relax }{algorithm.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Posterior Sampling}{39}{subsection.3.1.5}}
\newlabel{sub:posteriorMAB}{{3.1.5}{39}{Posterior Sampling}{subsection.3.1.5}{}}
\citation{agrawal2013further}
\citation{bubeck2012regret}
\citation{russo2018tutorial}
\citation{srinivas2010gaussian}
\citation{srinivas2010gaussian}
\citation{williams2006gaussian}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.3}{\ignorespaces \gls {TS} for Bernoulli MAB with Beta priors\relax }}{41}{algorithm.3.3}}
\newlabel{alg:bernoulliTS}{{3.3}{41}{\gls {TS} for Bernoulli MAB with Beta priors\relax }{algorithm.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Gaussian Process Upper Confidence Bound}{41}{subsection.3.1.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.4}{\ignorespaces \gls {GPUCB}\relax }}{42}{algorithm.3.4}}
\newlabel{alg:gpucb}{{3.4}{42}{\gls {GPUCB}\relax }{algorithm.3.4}{}}
\newlabel{eq:gpucbrule}{{3}{42}{\gls {GPUCB}\relax }{algorithm.3.4}{}}
\citation{sutton2018reinforcement}
\citation{thrun1992efficient}
\citation{cesa2017boltzmann}
\citation{sallans2004reinforcement}
\citation{ziebart2008maximum}
\citation{haarnoja2017reinforcement}
\citation{haarnoja2018soft}
\citation{haarnoja2017reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Exploration in Reinforcement Learning}{43}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Undirected Exploration}{43}{subsection.3.2.1}}
\citation{haarnoja2017reinforcement}
\citation{haarnoja2018soft}
\citation{kearns2002near}
\citation{brafman2002r}
\citation{auer2007logarithmic}
\citation{bellemare2016unifying}
\citation{tang2017exploration}
\citation{ostrovski2017count}
\citation{choshen2018dora}
\citation{bellemare2016unifying}
\citation{ostrovski2017count}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Count-based exploration and Intrinsic Motivation}{45}{subsection.3.2.2}}
\citation{ostrovski2017count}
\citation{oord2016pixel}
\citation{lopes2012exploration}
\citation{oudeyer2007intrinsic}
\citation{chentanez2005intrinsically}
\citation{schmidhuber1991possibility}
\citation{bellemare2016unifying}
\citation{houthooft2016vime}
\citation{blei2017variational}
\citation{blundell2015weight}
\citation{houthooft2016vime}
\citation{strens2000bayesian}
\citation{osband2013more}
\citation{osband2015bootstrapped}
\citation{osband2016deep}
\citation{strens2000bayesian}
\citation{osband2017posterior}
\newlabel{eq:augmentedreward}{{3.29}{48}{Count-based exploration and Intrinsic Motivation}{equation.3.2.29}{}}
\newlabel{eq:elb}{{3.30}{48}{Count-based exploration and Intrinsic Motivation}{equation.3.2.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Posterior Sampling}{48}{subsection.3.2.3}}
\newlabel{sub:posteriorRL}{{3.2.3}{48}{Posterior Sampling}{subsection.3.2.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.5}{\ignorespaces \gls {VIME}\relax }}{49}{algorithm.3.5}}
\newlabel{alg:VIME}{{3.5}{49}{\gls {VIME}\relax }{algorithm.3.5}{}}
\citation{metelli2018policy}
\citation{ionides2008truncated}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Optimistic Policy Search via Multiple Importance Sampling}{50}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:core}{{4}{50}{Optimistic Policy Search via Multiple Importance Sampling}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Robust Importance Sampling Estimation}{50}{section.4.1}}
\newlabel{sec:robust}{{4.1}{50}{Robust Importance Sampling Estimation}{section.4.1}{}}
\newlabel{eq:truncatedise}{{4.1}{51}{Robust Importance Sampling Estimation}{equation.4.1.1}{}}
\newlabel{eq:truncatedmise}{{4.2}{51}{Robust Importance Sampling Estimation}{equation.4.1.2}{}}
\newlabel{thmt@@truncatedbias@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.1.\@arabic {\c@equation }}\setcounter {equation}{2}}{51}{Robust Importance Sampling Estimation}{equation.4.1.2}{}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {lemma}{\numberline {4.1.1}Lemma}{51}{lemma.4.1.1}}
\newlabel{thmt@@truncatedbias}{{4.1.1}{51}{}{lemma.4.1.1}{}}
\newlabel{lem:truncatedbias}{{4.1.1}{51}{}{lemma.4.1.1}{}}
\newlabel{eq:biastruncated}{{4.3}{51}{}{equation.4.1.3}{}}
\newlabel{eq:variancetruncated}{{4.4}{51}{}{equation.4.1.4}{}}
\citation{bubeck2013bandits}
\citation{bubeck2013bandits}
\citation{ionides2008truncated}
\newlabel{thmt@@thrucatedconcentration@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.1.\@arabic {\c@equation }}\setcounter {equation}{4}}{52}{Robust Importance Sampling Estimation}{equation.4.1.4}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.1.1}Theorem}{52}{theorem.4.1.1}}
\newlabel{thmt@@thrucatedconcentration}{{4.1.1}{52}{}{theorem.4.1.1}{}}
\newlabel{lem:thrucatedconcentration}{{4.1.1}{52}{}{theorem.4.1.1}{}}
\newlabel{eq:1.1}{{4.5}{52}{}{equation.4.1.5}{}}
\newlabel{eq:1.2}{{4.6}{52}{}{equation.4.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Problem Formalization}{52}{section.4.2}}
\newlabel{sec:problem}{{4.2}{52}{Problem Formalization}{section.4.2}{}}
\newlabel{eq:theproblem}{{4.7}{52}{Problem Formalization}{equation.4.2.7}{}}
\citation{kleinberg2013bandits}
\@writefile{loe}{\contentsline {remark}{\numberline {4.2.1}Remark}{53}{remark.4.2.1}}
\citation{srinivas2010gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Algorithms}{54}{section.4.3}}
\newlabel{sec:algo}{{4.3}{54}{Algorithms}{section.4.3}{}}
\newlabel{eq:wcmu}{{4.8}{54}{Algorithms}{equation.4.3.8}{}}
\newlabel{eq:optimistindex}{{4.9}{54}{Algorithms}{equation.4.3.9}{}}
\citation{srinivas2010gaussian}
\citation{bubeck2013bandits}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces \gls {OPTIMIST}\relax }}{55}{algorithm.4.1}}
\newlabel{alg:1}{{4.1}{55}{\gls {OPTIMIST}\relax }{algorithm.4.1}{}}
\newlabel{line:sampling}{{2}{55}{\gls {OPTIMIST}\relax }{algorithm.4.1}{}}
\newlabel{line:optstep}{{4}{55}{\gls {OPTIMIST}\relax }{algorithm.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regret Analysis}{55}{section.4.4}}
\newlabel{sec:regret}{{4.4}{55}{Regret Analysis}{section.4.4}{}}
\newlabel{eq:inregret}{{4.10}{55}{Regret Analysis}{equation.4.4.10}{}}
\newlabel{eq:regret}{{4.11}{55}{Regret Analysis}{equation.4.4.11}{}}
\newlabel{thmt@@boundrenyi@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{11}}{55}{Regret Analysis}{equation.4.4.11}{}}
\@writefile{loe}{\contentsline {assumption}{\numberline {4.4.1}Assumption}{56}{assumption.4.4.1}}
\newlabel{thmt@@boundrenyi}{{4.4.1}{56}{}{assumption.4.4.1}{}}
\newlabel{ass:boundrenyi}{{4.4.1}{56}{}{assumption.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Discrete arm set}{56}{subsection.4.4.1}}
\newlabel{thmt@@regretdiscrete@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{11}}{56}{Discrete arm set}{subsection.4.4.1}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.1}Theorem}{56}{theorem.4.4.1}}
\newlabel{thmt@@regretdiscrete}{{4.4.1}{56}{}{theorem.4.4.1}{}}
\newlabel{th:regretdiscrete}{{4.4.1}{56}{}{theorem.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Compact arm set}{56}{subsection.4.4.2}}
\newlabel{thmt@@lipschitz@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{11}}{56}{Compact arm set}{subsection.4.4.2}{}}
\@writefile{loe}{\contentsline {assumption}{\numberline {4.4.2}Assumption}{57}{assumption.4.4.2}}
\newlabel{thmt@@lipschitz}{{4.4.2}{57}{}{assumption.4.4.2}{}}
\newlabel{ass:lipschitz}{{4.4.2}{57}{}{assumption.4.4.2}{}}
\newlabel{thmt@@lipschitzpol@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{11}}{57}{Compact arm set}{assumption.4.4.2}{}}
\@writefile{loe}{\contentsline {lemma}{\numberline {4.4.1}Lemma}{57}{lemma.4.4.1}}
\newlabel{thmt@@lipschitzpol}{{4.4.1}{57}{}{lemma.4.4.1}{}}
\newlabel{lem:lispschitzpol}{{4.4.1}{57}{}{lemma.4.4.1}{}}
\newlabel{eq:lp1}{{4.12}{57}{}{equation.4.4.12}{}}
\newlabel{eq:lp2}{{4.13}{57}{}{equation.4.4.13}{}}
\newlabel{thmt@@regretcompact@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{13}}{57}{Compact arm set}{equation.4.4.13}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.2}Theorem}{57}{theorem.4.4.2}}
\newlabel{thmt@@regretcompact}{{4.4.2}{57}{}{theorem.4.4.2}{}}
\newlabel{th:regretcompact}{{4.4.2}{57}{}{theorem.4.4.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces \gls {OPTIMIST}2\relax }}{58}{algorithm.4.2}}
\newlabel{alg:2}{{4.2}{58}{\gls {OPTIMIST}2\relax }{algorithm.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Discretization}{58}{subsection.4.4.3}}
\newlabel{thmt@@regretdiscretized@data}{{\def \theequation {4.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.4.\@arabic {\c@equation }}\setcounter {equation}{13}}{58}{Discretization}{subsection.4.4.3}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.3}Theorem}{58}{theorem.4.4.3}}
\newlabel{thmt@@regretdiscretized}{{4.4.3}{58}{}{theorem.4.4.3}{}}
\newlabel{th:regretdiscretized}{{4.4.3}{58}{}{theorem.4.4.3}{}}
\@writefile{loe}{\contentsline {remark}{\numberline {4.4.1}Remark}{58}{remark.4.4.1}}
\citation{baselines}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Numerical Simulations}{59}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experiments}{{5}{59}{Numerical Simulations}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Practical Aspects}{59}{section.5.1}}
\newlabel{sec:practical}{{5.1}{59}{Practical Aspects}{section.5.1}{}}
\citation{gil2013renyi}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Divergence Between Gaussian Multivariate Distributions}{60}{subsection.5.1.1}}
\newlabel{thmt@@armonic@data}{{\def \theequation {5.\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )5.1.\@arabic {\c@equation }}\setcounter {equation}{2}}{60}{Divergence Between Gaussian Multivariate Distributions}{subsection.5.1.1}{}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {theorem}{\numberline {5.1.1}Theorem}{60}{theorem.5.1.1}}
\newlabel{thmt@@armonic}{{5.1.1}{60}{}{theorem.5.1.1}{}}
\newlabel{th:armonic}{{5.1.1}{60}{}{theorem.5.1.1}{}}
\newlabel{eq:gaussianrenyi}{{5.4}{60}{Divergence Between Gaussian Multivariate Distributions}{equation.5.1.4}{}}
\citation{gil2013renyi}
\citation{gil2013renyi}
\citation{peters2008reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Uniformly Bounded R\IeC {\'e}nyi divergence}{61}{subsection.5.1.2}}
\newlabel{subsec:boundrenyi}{{5.1.2}{61}{Uniformly Bounded RÃ©nyi divergence}{subsection.5.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Linear Quadratic Gaussian Regulator}{61}{section.5.2}}
\citation{dorato1995linear}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Environmental coefficients (left-side), task coefficients (center) and \gls {OPTIMIST} input parameters (right-side) for the \gls {LQG} experiments.\relax }}{63}{table.caption.9}}
\newlabel{tab:LQGcoeff}{{5.1}{63}{Environmental coefficients (left-side), task coefficients (center) and \gls {OPTIMIST} input parameters (right-side) for the \gls {LQG} experiments.\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Cumulative regret in the \gls {LQG} experiment. Comparison between \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning the hyperpolicy mean. (30 runs, 95\% c.i.)\relax }}{63}{figure.caption.10}}
\newlabel{fig:LQGcomparison}{{5.1}{63}{Cumulative regret in the \gls {LQG} experiment. Comparison between \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning the hyperpolicy mean. (30 runs, 95\% c.i.)\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Gain only}{63}{subsection.5.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The gain parameter $\mu $ selected at each iteration of \gls {GPUCB} (left) and \gls {OPTIMIST} (right) in the \gls {LQG} experiment.\relax }}{64}{figure.caption.11}}
\newlabel{fig:LQGmu}{{5.2}{64}{The gain parameter $\mu $ selected at each iteration of \gls {GPUCB} (left) and \gls {OPTIMIST} (right) in the \gls {LQG} experiment.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Mean return of arms $\mu \in [-0.9,-0,5]$, calculated by averaging the return collected over 2000 trajectories in the \gls {LQG} experiment.\relax }}{64}{figure.caption.12}}
\newlabel{fig:LQGoptimalgain}{{5.3}{64}{Mean return of arms $\mu \in [-0.9,-0,5]$, calculated by averaging the return collected over 2000 trajectories in the \gls {LQG} experiment.\relax }{figure.caption.12}{}}
\citation{gym}
\citation{gym}
\citation{brockman2016openai}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Task parameters (left side) and \gls {OPTIMIST} input parameters (right side) for the Continuous Mountain Car experiment.\relax }}{65}{table.caption.15}}
\newlabel{tab:MCcoeff}{{5.2}{65}{Task parameters (left side) and \gls {OPTIMIST} input parameters (right side) for the Continuous Mountain Car experiment.\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Gain and standard deviation}{65}{subsection.5.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Cumulative regret in the \gls {LQG} experiment, comparing \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning both the mean and the standard deviation hyperparameters. (30 runs, 95\% c.i.)\relax }}{66}{figure.caption.13}}
\newlabel{fig:LQGcomparisonVar}{{5.4}{66}{Cumulative regret in the \gls {LQG} experiment, comparing \gls {OPTIMIST}, \gls {UCB}1 and \gls {GPUCB} when learning both the mean and the standard deviation hyperparameters. (30 runs, 95\% c.i.)\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Graphical representation of the Mountain Car problem \cite  {gym}.\relax }}{66}{figure.caption.14}}
\newlabel{fig:MC}{{5.5}{66}{Graphical representation of the Mountain Car problem \cite {gym}.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Continuous Mountain Car}{66}{section.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Cumulative regret in the Continuous Mountain Car experiment. Comparison between \gls {OPTIMIST}, \gls {PGPE} and \gls {PBPOIS} when learning the two-dimensional hyperpolicy mean. (5 runs, 95\% c.i.)\relax }}{67}{figure.caption.16}}
\newlabel{fig:MCcomparison}{{5.6}{67}{Cumulative regret in the Continuous Mountain Car experiment. Comparison between \gls {OPTIMIST}, \gls {PGPE} and \gls {PBPOIS} when learning the two-dimensional hyperpolicy mean. (5 runs, 95\% c.i.)\relax }{figure.caption.16}{}}
\citation{metelli2018policy}
\citation{scott2015multivariate}
\citation{scott2015multivariate}
\citation{scott2015multivariate}
\@writefile{loe}{\contentsline {remark}{\numberline {5.3.1}Remark}{68}{remark.5.3.1}}
\newlabel{rk:undiscounted}{{5.3.1}{68}{}{remark.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Gaussian kernel density estimation \cite  {scott2015multivariate} of the probability distribution of the arm set $\bm  {\xi }=\bm  {\mu }$ induced by \gls {PGPE} (left) and \gls {OPTIMIST} (right).\relax }}{69}{figure.caption.17}}
\newlabel{fig:MCgain}{{5.7}{69}{Gaussian kernel density estimation \cite {scott2015multivariate} of the probability distribution of the arm set $\vxi =\vmu $ induced by \gls {PGPE} (left) and \gls {OPTIMIST} (right).\relax }{figure.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Number of points in the discretized state space, following discretization schedule $\tau _t=\delimiter "4D72E50 t^{\frac  {1}{\kappa }} \delimiter "5D73E54 $, where $d$ refers to the number of space dimensions and $k$ is a free-parameter.\relax }}{69}{table.caption.18}}
\newlabel{tab:granularity}{{5.3}{69}{Number of points in the discretized state space, following discretization schedule $\tau _t=\lceil t^{\frac {1}{\kappa }} \rceil $, where $d$ refers to the number of space dimensions and $k$ is a free-parameter.\relax }{table.caption.18}{}}
\@writefile{loe}{\contentsline {remark}{\numberline {5.3.2}Remark}{69}{remark.5.3.2}}
\newlabel{rk:discretization}{{5.3.2}{69}{}{remark.5.3.2}{}}
\citation{wawrzynski2005intensive}
\citation{wawrzynski2005intensive}
\citation{tornio2006variational}
\citation{duan2016benchmarking}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Graphical representation of the Inverted Pendulum task \cite  {wawrzynski2005intensive}.\relax }}{70}{figure.caption.19}}
\newlabel{fig:invpend}{{5.8}{70}{Graphical representation of the Inverted Pendulum task \cite {wawrzynski2005intensive}.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Task parameters (left side) and \gls {OPTIMIST} input parameters (right side) for the Inverted Pendulum experiment.\relax }}{70}{table.caption.20}}
\newlabel{tab:IPcoeff}{{5.4}{70}{Task parameters (left side) and \gls {OPTIMIST} input parameters (right side) for the Inverted Pendulum experiment.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Inverted Pendulum}{70}{section.5.4}}
\citation{kimura1999efficient}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Action-based setting}{72}{section.5.5}}
\newlabel{sec:actionbased}{{5.5}{72}{Action-based setting}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The hyperpolicy mean parameter selected at each iteration of \gls {OPTIMIST} in the Inverted Pendulum experiment.\relax }}{73}{figure.caption.21}}
\newlabel{fig:IPmu1}{{5.9}{73}{The hyperpolicy mean parameter selected at each iteration of \gls {OPTIMIST} in the Inverted Pendulum experiment.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Truncated \gls {MIS} estimator (left) and exploration bonus (right) of the arms selected by \gls {OPTIMIST} at each iteration of the Inverted Pendulum experiment.\relax }}{73}{figure.caption.22}}
\newlabel{fig:IPbound}{{5.10}{73}{Truncated \gls {MIS} estimator (left) and exploration bonus (right) of the arms selected by \gls {OPTIMIST} at each iteration of the Inverted Pendulum experiment.\relax }{figure.caption.22}{}}
\citation{martino2017effective}
\citation{kong1992note}
\citation{kong1992note}
\newlabel{eq:essrenyi}{{5.13}{74}{Action-based setting}{equation.5.5.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{75}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{6}{75}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Recapitulation}{75}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Limitations and future works}{76}{section.6.2}}
\citation{metelli2018policy}
\@writefile{toc}{\contentsline {chapter}{Appendices}{78}{section*.23}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs}{78}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:proof}{{A}{78}{Proofs}{Appendix.1.A}{}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {lemma}{\numberline {2.5.1}Lemma}{78}{lemma.dummy.41}}
\newlabel{p:misevarboundline2}{{A.1}{78}{Proofs}{equation.1.A.0.1}{}}
\newlabel{p:misevarboundline5}{{A.2}{78}{Proofs}{equation.1.A.0.2}{}}
\newlabel{eq:expectationdecomposition}{{A.3}{79}{Proofs}{equation.1.A.0.3}{}}
\@writefile{loe}{\contentsline {lemma}{\numberline {4.1.1}Lemma}{79}{lemma.dummy.43}}
\newlabel{p:truncatedbias1}{{A.5}{79}{Proofs}{equation.1.A.0.5}{}}
\newlabel{p:truncatedbias2}{{A.6}{79}{Proofs}{equation.1.A.0.6}{}}
\newlabel{p:truncatedbias3}{{A.7}{80}{Proofs}{equation.1.A.0.7}{}}
\newlabel{p:truncatedbias4}{{A.8}{80}{Proofs}{equation.1.A.0.8}{}}
\newlabel{p:truncatedbias5}{{A.9}{80}{Proofs}{equation.1.A.0.9}{}}
\citation{boucheron2013concentration}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.1.1}Theorem}{81}{theorem.dummy.45}}
\newlabel{p:thrucatedconcentration1}{{A.11}{81}{Proofs}{equation.1.A.0.11}{}}
\newlabel{p:thrucatedconcentration2}{{A.12}{81}{Proofs}{equation.1.A.0.12}{}}
\newlabel{p:thrucatedconcentration3}{{A.13}{81}{Proofs}{equation.1.A.0.13}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.1}Theorem}{82}{theorem.dummy.47}}
\newlabel{aux:1}{{A.15}{82}{Proofs}{equation.1.A.0.15}{}}
\newlabel{pp:1}{{A.16}{82}{Proofs}{equation.1.A.0.16}{}}
\newlabel{pp:2}{{A.17}{82}{Proofs}{equation.1.A.0.17}{}}
\newlabel{pp:3}{{A.18}{82}{Proofs}{equation.1.A.0.18}{}}
\newlabel{pp:4}{{A.19}{83}{Proofs}{equation.1.A.0.19}{}}
\newlabel{pp:5}{{A.20}{83}{Proofs}{equation.1.A.0.20}{}}
\newlabel{aux:2}{{A.21}{83}{Proofs}{equation.1.A.0.21}{}}
\newlabel{pp:6}{{A.22}{83}{Proofs}{equation.1.A.0.22}{}}
\newlabel{pp:7}{{A.23}{83}{Proofs}{equation.1.A.0.23}{}}
\newlabel{pp:8}{{A.24}{83}{Proofs}{equation.1.A.0.24}{}}
\newlabel{pp:9}{{A.25}{83}{Proofs}{equation.1.A.0.25}{}}
\newlabel{aux:3}{{A.26}{83}{Proofs}{equation.1.A.0.26}{}}
\@writefile{loe}{\contentsline {lemma}{\numberline {4.4.1}Lemma}{83}{lemma.dummy.49}}
\citation{sutton2000policy}
\citation{sutton2000policy}
\newlabel{aux:4}{{A.27}{84}{Proofs}{equation.1.A.0.27}{}}
\newlabel{pp:10}{{A.28}{84}{Proofs}{equation.1.A.0.28}{}}
\newlabel{pp:11}{{A.29}{84}{Proofs}{equation.1.A.0.29}{}}
\newlabel{eq:gauss}{{A.30}{84}{Proofs}{equation.1.A.0.30}{}}
\newlabel{pp:14}{{A.31}{84}{Proofs}{equation.1.A.0.31}{}}
\citation{srinivas2010gaussian}
\newlabel{aux:5}{{A.32}{85}{Proofs}{equation.1.A.0.32}{}}
\newlabel{pp:12}{{A.33}{85}{Proofs}{equation.1.A.0.33}{}}
\newlabel{pp:13}{{A.34}{85}{Proofs}{equation.1.A.0.34}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.2}Theorem}{85}{theorem.dummy.51}}
\newlabel{aux:6}{{A.35}{86}{Proofs}{equation.1.A.0.35}{}}
\newlabel{aux:7}{{A.36}{86}{Proofs}{equation.1.A.0.36}{}}
\newlabel{aux:8}{{A.37}{86}{Proofs}{equation.1.A.0.37}{}}
\newlabel{aux:9}{{A.38}{86}{Proofs}{equation.1.A.0.38}{}}
\newlabel{pp:15}{{A.39}{86}{Proofs}{equation.1.A.0.39}{}}
\newlabel{pp:16}{{A.40}{86}{Proofs}{equation.1.A.0.40}{}}
\newlabel{aux:10}{{A.41}{86}{Proofs}{equation.1.A.0.41}{}}
\newlabel{pp:17}{{A.42}{87}{Proofs}{equation.1.A.0.42}{}}
\newlabel{pp:18}{{A.43}{87}{Proofs}{equation.1.A.0.43}{}}
\newlabel{pp:19}{{A.44}{87}{Proofs}{equation.1.A.0.44}{}}
\newlabel{pp:20}{{A.46}{87}{Proofs}{equation.1.A.0.46}{}}
\newlabel{pp:21}{{A.48}{87}{Proofs}{equation.1.A.0.48}{}}
\newlabel{pp:22}{{A.52}{88}{Proofs}{equation.1.A.0.52}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {4.4.3}Theorem}{88}{theorem.dummy.53}}
\newlabel{aux:11}{{A.53}{88}{Proofs}{equation.1.A.0.53}{}}
\newlabel{pp:23}{{A.54}{88}{Proofs}{equation.1.A.0.54}{}}
\newlabel{pp:24}{{A.55}{88}{Proofs}{equation.1.A.0.55}{}}
\newlabel{aux:12}{{A.56}{88}{Proofs}{equation.1.A.0.56}{}}
\newlabel{pp:25}{{A.57}{89}{Proofs}{equation.1.A.0.57}{}}
\newlabel{pp:26}{{A.58}{89}{Proofs}{equation.1.A.0.58}{}}
\newlabel{pp:27}{{A.59}{89}{Proofs}{equation.1.A.0.59}{}}
\newlabel{pp:28}{{A.61}{89}{Proofs}{equation.1.A.0.61}{}}
\newlabel{pp:29}{{A.63}{89}{Proofs}{equation.1.A.0.63}{}}
\newlabel{pp:30}{{A.65}{89}{Proofs}{equation.1.A.0.65}{}}
\citation{hershey2007approximating}
\@writefile{loe}{\contentsline {lemma}{\numberline {A.0.1}Lemma}{90}{lemma.1.A.0.1}}
\newlabel{eq:strangeBound}{{A.69}{90}{Proofs}{equation.1.A.0.69}{}}
\@writefile{loe}{\contentsline {theorem}{\numberline {5.1.1}Theorem}{91}{theorem.dummy.56}}
\bibstyle{acm}
\bibdata{Bibliography/bibliography}
\bibcite{agrawal1995continuum}{1}
\bibcite{agrawal2013further}{2}
\bibcite{amari1998natural2}{3}
\bibcite{amari1998natural1}{4}
\bibcite{antos2008fitted}{5}
\bibcite{auer2002finite}{6}
\bibcite{auer2007logarithmic}{7}
\bibcite{auer2010ucb}{8}
\bibcite{baird1993advantage}{9}
\bibcite{baxter2001infinite}{10}
\bibcite{bellemare2016unifying}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{93}{chapter*.24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{blei2017variational}{12}
\bibcite{blundell2015weight}{13}
\bibcite{boucheron2013concentration}{14}
\bibcite{brafman2002r}{15}
\bibcite{gym}{16}
\bibcite{brockman2016openai}{17}
\bibcite{bubeck2012regret}{18}
\bibcite{bubeck2013bandits}{19}
\bibcite{bubeck2011x}{20}
\bibcite{cesa2017boltzmann}{21}
\bibcite{chentanez2005intrinsically}{22}
\bibcite{choshen2018dora}{23}
\bibcite{cochran2007sampling}{24}
\bibcite{cortes2010learning}{25}
\bibcite{dayan1997using}{26}
\bibcite{degris2012off}{27}
\bibcite{deisenroth2013survey}{28}
\bibcite{baselines}{29}
\bibcite{dorato1995linear}{30}
\bibcite{duan2016benchmarking}{31}
\bibcite{espeholt2018impala}{32}
\bibcite{garivier2011kl}{33}
\bibcite{gil2013renyi}{34}
\bibcite{glynn1990likelihood}{35}
\bibcite{goodfellow2016deep}{36}
\bibcite{grondman2012survey}{37}
\bibcite{haarnoja2017reinforcement}{38}
\bibcite{haarnoja2018soft}{39}
\bibcite{hershey2007approximating}{40}
\bibcite{houthooft2016vime}{41}
\bibcite{ionides2008truncated}{42}
\bibcite{kearns2002near}{43}
\bibcite{kimura1999efficient}{44}
\bibcite{kleinberg2008multi}{45}
\bibcite{kleinberg2013bandits}{46}
\bibcite{kleinberg2005nearly}{47}
\bibcite{kober2009policy}{48}
\bibcite{kong1992note}{49}
\bibcite{kupcsik2013data}{50}
\bibcite{lai1985asymptotically}{51}
\bibcite{lange2012batch}{52}
\bibcite{lattimore2019bandit}{53}
\bibcite{lopes2012exploration}{54}
\bibcite{magureanu2014lipschitz}{55}
\bibcite{martino2017effective}{56}
\bibcite{metelli2018policy}{57}
\bibcite{mnih2016asynchronous}{58}
\bibcite{more1994line}{59}
\bibcite{oord2016pixel}{60}
\bibcite{osband2016deep}{61}
\bibcite{osband2013more}{62}
\bibcite{osband2015bootstrapped}{63}
\bibcite{osband2017posterior}{64}
\bibcite{ostrovski2017count}{65}
\bibcite{oudeyer2007intrinsic}{66}
\bibcite{mcbook}{67}
\bibcite{peters2008reinforcement}{68}
\bibcite{puterman2014markov}{69}
\bibcite{renyi1961measures}{70}
\bibcite{rummery1994line}{71}
\bibcite{russo2013eluder}{72}
\bibcite{russo2018tutorial}{73}
\bibcite{sallans2004reinforcement}{74}
\bibcite{schmidhuber1991possibility}{75}
\bibcite{schulman2015trust}{76}
\bibcite{scott2015multivariate}{77}
\bibcite{sehnke2008policy}{78}
\bibcite{sehnke2010parameter}{79}
\bibcite{silver2014deterministic}{80}
\bibcite{srinivas2010gaussian}{81}
\bibcite{still2012information}{82}
\bibcite{strens2000bayesian}{83}
\bibcite{sutton1988learning}{84}
\bibcite{sutton1991dyna}{85}
\bibcite{sutton2018reinforcement}{86}
\bibcite{sutton2000policy}{87}
\bibcite{tang2017exploration}{88}
\bibcite{thompson1933likelihood}{89}
\bibcite{thrun1992efficient}{90}
\bibcite{tornio2006variational}{91}
\bibcite{veach_optimally_1995}{92}
\bibcite{watkins1989learning}{93}
\bibcite{wawrzynski2005intensive}{94}
\bibcite{whitehead1991study}{95}
\bibcite{williams2006gaussian}{96}
\bibcite{williams1992simple}{97}
\bibcite{zhao2011analysis}{98}
\bibcite{ziebart2008maximum}{99}
\providecommand\@xdylanguage[2]{}
\@xdylanguage{acronym}{english}
\providecommand\@gls@codepage[2]{}
\@gls@codepage{acronym}{utf8}
