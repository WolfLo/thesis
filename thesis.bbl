\begin{thebibliography}{100}

\bibitem{abbasi2011improved}
{\sc Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.}
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2011),
  pp.~2312--2320.

\bibitem{agrawal1995continuum}
{\sc Agrawal, R.}
\newblock The continuum-armed bandit problem.
\newblock {\em SIAM Journal on Control and Optimization 33}, 6 (1995),
  1926--1951.

\bibitem{agrawal1995sample}
{\sc Agrawal, R.}
\newblock Sample mean based index policies by o (log n) regret for the
  multi-armed bandit problem.
\newblock {\em Advances in Applied Probability 27}, 4 (1995), 1054--1078.

\bibitem{agrawal2013further}
{\sc Agrawal, S., and Goyal, N.}
\newblock Further optimal regret bounds for thompson sampling.
\newblock In {\em Artificial intelligence and statistics\/} (2013),
  pp.~99--107.

\bibitem{amari1998natural2}
{\sc Amari, S.-I.}
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation 10}, 2 (1998), 251--276.

\bibitem{amari1998natural1}
{\sc Amari, S.-I., and Douglas, S.~C.}
\newblock Why natural gradient?
\newblock In {\em Proceedings of the 1998 IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)\/}
  (1998), vol.~2, IEEE, pp.~1213--1216.

\bibitem{antos2008fitted}
{\sc Antos, A., Szepesv{\'a}ri, C., and Munos, R.}
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In {\em Advances in neural information processing systems\/} (2008),
  pp.~9--16.

\bibitem{atan2015global}
{\sc Atan, O., Tekin, C., and Schaar, M.}
\newblock Global multi-armed bandits with h{\"o}lder continuity.
\newblock In {\em Artificial Intelligence and Statistics\/} (2015), pp.~28--36.

\bibitem{auer2002using}
{\sc Auer, P.}
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock {\em Journal of Machine Learning Research 3}, Nov (2002), 397--422.

\bibitem{auer2002finite}
{\sc Auer, P., Cesa-Bianchi, N., and Fischer, P.}
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning 47}, 2-3 (2002), 235--256.

\bibitem{auer2007logarithmic}
{\sc Auer, P., and Ortner, R.}
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2007),
  pp.~49--56.

\bibitem{auer2010ucb}
{\sc Auer, P., and Ortner, R.}
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock {\em Periodica Mathematica Hungarica 61}, 1-2 (2010), 55--65.

\bibitem{auer2007improved}
{\sc Auer, P., Ortner, R., and Szepesv{\'a}ri, C.}
\newblock Improved rates for the stochastic continuum-armed bandit problem.
\newblock In {\em International Conference on Computational Learning Theory\/}
  (2007), Springer, pp.~454--468.

\bibitem{DBLP:conf/icml/2015}
{\sc Bach, F.~R., and Blei, D.~M.}, Eds.
\newblock {\em Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015\/} (2015), vol.~37 of
  {\em {JMLR} Workshop and Conference Proceedings}, JMLR.org.

\bibitem{baird1993advantage}
{\sc Baird~III, L.~C.}
\newblock Advantage updating.
\newblock Tech. rep., WRIGHT LAB WRIGHT-PATTERSON AFB OH, 1993.

\bibitem{baxter2001infinite}
{\sc Baxter, J., and Bartlett, P.~L.}
\newblock Infinite-horizon policy-gradient estimation.
\newblock {\em Journal of Artificial Intelligence Research 15\/} (2001),
  319--350.

\bibitem{bellemare2016unifying}
{\sc Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.}
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2016),
  pp.~1471--1479.

\bibitem{DBLP:conf/nips/2018}
{\sc Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K., Cesa{-}Bianchi,
  N., and Garnett, R.}, Eds.
\newblock {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
  December 2018, Montr{\'{e}}al, Canada\/} (2018).

\bibitem{blei2017variational}
{\sc Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D.}
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association 112}, 518
  (2017), 859--877.

\bibitem{blundell2015weight}
{\sc Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.}
\newblock Weight uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:1505.05424\/} (2015).

\bibitem{boucheron2013concentration}
{\sc Boucheron, S., Lugosi, G., and Massart, P.}
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem{brafman2002r}
{\sc Brafman, R.~I., and Tennenholtz, M.}
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research 3}, Oct (2002), 213--231.

\bibitem{brockman2016openai}
{\sc Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J.,
  Tang, J., and Zaremba, W.}
\newblock Openai gym, 2016.

\bibitem{bubeck2012regret}
{\sc Bubeck, S., Cesa-Bianchi, N., et~al.}
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning 5},
  1 (2012), 1--122.

\bibitem{bubeck2013bandits}
{\sc Bubeck, S., Cesa-Bianchi, N., and Lugosi, G.}
\newblock Bandits with heavy tail.
\newblock {\em IEEE Transactions on Information Theory 59}, 11 (2013),
  7711--7717.

\bibitem{bubeck2010open}
{\sc Bubeck, S., and Munos, R.}
\newblock Open loop optimistic planning.
\newblock In {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010\/} (2010), pp.~477--489.

\bibitem{bubeck2011x}
{\sc Bubeck, S., Munos, R., Stoltz, G., and Szepesv{\'a}ri, C.}
\newblock X-armed bandits.
\newblock {\em Journal of Machine Learning Research 12}, May (2011),
  1655--1695.

\bibitem{bubeck2009online}
{\sc Bubeck, S., Stoltz, G., Szepesv{\'a}ri, C., and Munos, R.}
\newblock Online optimization in x-armed bandits.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2009),
  pp.~201--208.

\bibitem{cesa2017boltzmann}
{\sc Cesa-Bianchi, N., Gentile, C., Lugosi, G., and Neu, G.}
\newblock Boltzmann exploration done right.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2017),
  pp.~6284--6293.

\bibitem{cesa2012combinatorial}
{\sc Cesa-Bianchi, N., and Lugosi, G.}
\newblock Combinatorial bandits.
\newblock {\em Journal of Computer and System Sciences 78}, 5 (2012),
  1404--1422.

\bibitem{chapelle2011empirical}
{\sc Chapelle, O., and Li, L.}
\newblock An empirical evaluation of thompson sampling.
\newblock In {\em Advances in neural information processing systems\/} (2011),
  pp.~2249--2257.

\bibitem{chen2016combinatorial}
{\sc Chen, W., Wang, Y., Yuan, Y., and Wang, Q.}
\newblock Combinatorial multi-armed bandit and its extension to
  probabilistically triggered arms.
\newblock {\em The Journal of Machine Learning Research 17}, 1 (2016),
  1746--1778.

\bibitem{chentanez2005intrinsically}
{\sc Chentanez, N., Barto, A.~G., and Singh, S.~P.}
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in neural information processing systems\/} (2005),
  pp.~1281--1288.

\bibitem{choshen2018dora}
{\sc Choshen, L., Fox, L., and Loewenstein, Y.}
\newblock Dora the explorer: Directed outreaching reinforcement
  action-selection.
\newblock {\em arXiv preprint arXiv:1804.04012\/} (2018).

\bibitem{sayak2018online}
{\sc Chowdhury, S.~R., and Gopalan, A.}
\newblock Online learning in kernelized markov decision processes.
\newblock {\em CoRR abs/1805.08052\/} (2018).

\bibitem{cochran2007sampling}
{\sc Cochran, W.~G.}
\newblock {\em Sampling techniques}.
\newblock John Wiley \& Sons, 2007.

\bibitem{cortes2010learning}
{\sc Cortes, C., Mansour, Y., and Mohri, M.}
\newblock Learning bounds for importance weighting.
\newblock In {\em Advances in Neural Information Processing Systems 23}, J.~D.
  Lafferty, C.~K.~I. Williams, J.~Shawe-Taylor, R.~S. Zemel, and A.~Culotta,
  Eds. Curran Associates, Inc., 2010, pp.~442--450.

\bibitem{dann2015sample}
{\sc Dann, C., and Brunskill, E.}
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2015),
  pp.~2818--2826.

\bibitem{dann2017unifying}
{\sc Dann, C., Lattimore, T., and Brunskill, E.}
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2017),
  pp.~5713--5723.

\bibitem{dayan1997using}
{\sc Dayan, P., and Hinton, G.~E.}
\newblock Using expectation-maximization for reinforcement learning.
\newblock {\em Neural Computation 9}, 2 (1997), 271--278.

\bibitem{degris2012off}
{\sc Degris, T., White, M., and Sutton, R.~S.}
\newblock Off-policy actor-critic.
\newblock {\em arXiv preprint arXiv:1205.4839\/} (2012).

\bibitem{deisenroth2013survey}
{\sc Deisenroth, M.~P., Neumann, G., Peters, J., et~al.}
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends{\textregistered} in Robotics 2}, 1--2
  (2013), 1--142.

\bibitem{dorato1995linear}
{\sc Dorato, P., Abdallah, C.~T., Cerone, V., and Jacobson, D.~H.}
\newblock {\em Linear-quadratic control: an introduction}.
\newblock Prentice Hall Englewood Cliffs, NJ, 1995.

\bibitem{DBLP:conf/icml/2018}
{\sc Dy, J.~G., and Krause, A.}, Eds.
\newblock {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018\/} (2018), vol.~80 of {\em {JMLR} Workshop and Conference Proceedings},
  JMLR.org.

\bibitem{espeholt2018impala}
{\sc Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T.,
  Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu,
  K.}
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018\/} (2018), pp.~1406--1415.

\bibitem{garivier2011kl}
{\sc Garivier, A., and Capp{\'e}, O.}
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In {\em Proceedings of the 24th annual conference on learning
  theory\/} (2011), pp.~359--376.

\bibitem{gil2013renyi}
{\sc Gil, M., Alajaji, F., and Linder, T.}
\newblock R{\'e}nyi divergence measures for commonly used univariate continuous
  distributions.
\newblock {\em Information Sciences 249\/} (2013), 124--131.

\bibitem{glynn1990likelihood}
{\sc Glynn, P.~W.}
\newblock Likelihood ratio gradient estimation for stochastic systems.
\newblock {\em Communications of the ACM 33}, 10 (1990), 75--84.

\bibitem{grondman2012survey}
{\sc Grondman, I., Busoniu, L., Lopes, G.~A., and Babuska, R.}
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews) 42}, 6 (2012), 1291--1307.

\bibitem{haarnoja2017reinforcement}
{\sc Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.}
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017\/} (2017),
  pp.~1352--1361.

\bibitem{haarnoja2018soft}
{\sc Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.}
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018\/} (2018), pp.~1856--1865.

\bibitem{hershey2007approximating}
{\sc Hershey, J.~R., and Olsen, P.~A.}
\newblock Approximating the kullback leibler divergence between gaussian
  mixture models.
\newblock In {\em Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.
  IEEE International Conference on\/} (2007), vol.~4, IEEE, pp.~IV--317.

\bibitem{houthooft2016vime}
{\sc Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel,
  P.}
\newblock Vime: Variational information maximizing exploration.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2016),
  pp.~1109--1117.

\bibitem{ionides2008truncated}
{\sc Ionides, E.~L.}
\newblock Truncated importance sampling.
\newblock {\em Journal of Computational and Graphical Statistics 17}, 2 (2008),
  295--311.

\bibitem{jaksch2010near}
{\sc Jaksch, T., Ortner, R., and Auer, P.}
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research 11}, Apr (2010),
  1563--1600.

\bibitem{jin2018q}
{\sc Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.}
\newblock Is q-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems\/} (2018),
  pp.~4868--4878.

\bibitem{DBLP:conf/colt/2010}
{\sc Kalai, A.~T., and Mohri, M.}, Eds.
\newblock {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010\/} (2010), Omnipress.

\bibitem{kallus2018instrument}
{\sc Kallus, N.}
\newblock Instrument-armed bandits.
\newblock In {\em Algorithmic Learning Theory\/} (2018), pp.~529--546.

\bibitem{kappen2005path}
{\sc Kappen, H.~J.}
\newblock Path integrals and symmetry breaking for optimal control theory.
\newblock {\em Journal of statistical mechanics: theory and experiment 2005},
  11 (2005), P11011.

\bibitem{kaufmann2012thompson}
{\sc Kaufmann, E., Korda, N., and Munos, R.}
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In {\em International Conference on Algorithmic Learning Theory\/}
  (2012), Springer, pp.~199--213.

\bibitem{kearns2002near}
{\sc Kearns, M., and Singh, S.}
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning 49}, 2-3 (2002), 209--232.

\bibitem{kleinberg2008multi}
{\sc Kleinberg, R., Slivkins, A., and Upfal, E.}
\newblock Multi-armed bandits in metric spaces.
\newblock In {\em Proceedings of the fortieth annual ACM symposium on Theory of
  computing\/} (2008), ACM, pp.~681--690.

\bibitem{kleinberg2013bandits}
{\sc Kleinberg, R., Slivkins, A., and Upfal, E.}
\newblock Bandits and experts in metric spaces.
\newblock {\em arXiv preprint arXiv:1312.1277\/} (2013).

\bibitem{kleinberg2005nearly}
{\sc Kleinberg, R.~D.}
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2005),
  pp.~697--704.

\bibitem{kober2009policy}
{\sc Kober, J., and Peters, J.~R.}
\newblock Policy search for motor primitives in robotics.
\newblock In {\em Advances in neural information processing systems\/} (2009),
  pp.~849--856.

\bibitem{kong1992note}
{\sc Kong, A.}
\newblock A note on importance sampling using standardized weights.
\newblock {\em University of Chicago, Dept. of Statistics, Tech. Rep 348\/}
  (1992).

\bibitem{kupcsik2013data}
{\sc Kupcsik, A.~G., Deisenroth, M.~P., Peters, J., and Neumann, G.}
\newblock Data-efficient generalization of robot skills with contextual policy
  search.
\newblock In {\em Twenty-Seventh AAAI Conference on Artificial Intelligence\/}
  (2013).

\bibitem{lai1985asymptotically}
{\sc Lai, T.~L., and Robbins, H.}
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics 6}, 1 (1985), 4--22.

\bibitem{lakshmanan2015improved}
{\sc Lakshmanan, K., Ortner, R., and Ryabko, D.}
\newblock Improved regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015\/} (2015), pp.~524--532.

\bibitem{lange2012batch}
{\sc Lange, S., Gabel, T., and Riedmiller, M.}
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}. Springer, 2012, pp.~45--73.

\bibitem{lattimore2014near}
{\sc Lattimore, T., and Hutter, M.}
\newblock Near-optimal pac bounds for discounted mdps.
\newblock {\em Theoretical Computer Science 558\/} (2014), 125--143.

\bibitem{lattimore2019bandit}
{\sc Lattimore, T., and Szepesv\'{a}ri, C.}
\newblock {\em Bandit Algorithms}.
\newblock Cambridge University Press (preprint), 2019.

\bibitem{lopes2012exploration}
{\sc Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y.}
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2012),
  pp.~206--214.

\bibitem{magureanu2014lipschitz}
{\sc Magureanu, S., Combes, R., and Proutiere, A.}
\newblock Lipschitz bandits: Regret lower bounds and optimal algorithms.
\newblock {\em arXiv preprint arXiv:1405.4758\/} (2014).

\bibitem{martino2017effective}
{\sc Martino, L., Elvira, V., and Louzada, F.}
\newblock Effective sample size for importance sampling based on discrepancy
  measures.
\newblock {\em Signal Processing 131\/} (2017), 386--401.

\bibitem{masooddiversity}
{\sc Masood, M.~A., and Doshi-Velez, F.}
\newblock Diversity-inducing policy gradient: Using mmd to find a set of
  policies that are diverse in terms of state-visitation.

\bibitem{DBLP:conf/aips/2012}
{\sc McCluskey, L., Williams, B.~C., Silva, J.~R., and Bonet, B.}, Eds.
\newblock {\em Proceedings of the Twenty-Second International Conference on
  Automated Planning and Scheduling, {ICAPS} 2012, Atibaia, S{\~{a}}o Paulo,
  Brazil, June 25-19, 2012\/} (2012), {AAAI}.

\bibitem{pmlr-v48-medina16}
{\sc Medina, A.~M., and Yang, S.}
\newblock No-regret algorithms for heavy-tailed linear bandits.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning\/} (New York, New York, USA, 20--22 Jun 2016), M.~F. Balcan and
  K.~Q. Weinberger, Eds., vol.~48 of {\em Proceedings of Machine Learning
  Research}, PMLR, pp.~1642--1650.

\bibitem{mersereau2009structured}
{\sc Mersereau, A.~J., Rusmevichientong, P., and Tsitsiklis, J.~N.}
\newblock A structured multiarmed bandit problem and the greedy policy.
\newblock {\em IEEE Transactions on Automatic Control 54}, 12 (2009),
  2787--2802.

\bibitem{metelli2018policy}
{\sc Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.}
\newblock Policy optimization via importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2018),
  pp.~5447--5459.

\bibitem{mnih2016asynchronous}
{\sc Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley,
  T., Silver, D., and Kavukcuoglu, K.}
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016\/} (2016),
  pp.~1928--1937.

\bibitem{more1994line}
{\sc Mor{\'e}, J.~J., and Thuente, D.~J.}
\newblock Line search algorithms with guaranteed sufficient decrease.
\newblock {\em ACM Transactions on Mathematical Software (TOMS) 20}, 3 (1994),
  286--307.

\bibitem{jungseul2018exploration}
{\sc Ok, J., Prouti{\`{e}}re, A., and Tranos, D.}
\newblock Exploration in structured reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
  December 2018, Montr{\'{e}}al, Canada.\/} (2018), pp.~8888--8896.

\bibitem{oord2016pixel}
{\sc Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K.}
\newblock Pixel recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1601.06759\/} (2016).

\bibitem{ortner2012online}
{\sc Ortner, R., and Ryabko, D.}
\newblock Online regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In {\em Proceedings of the 25th International Conference on Neural
  Information Processing Systems-Volume 2\/} (2012), Curran Associates Inc.,
  pp.~1763--1771.

\bibitem{osband2016deep}
{\sc Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.}
\newblock Deep exploration via bootstrapped dqn.
\newblock In {\em Advances in neural information processing systems\/} (2016),
  pp.~4026--4034.

\bibitem{osband2013more}
{\sc Osband, I., Russo, D., and Roy, B.~V.}
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In {\em Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.\/}
  (2013), pp.~3003--3011.

\bibitem{osband2015bootstrapped}
{\sc Osband, I., and Van~Roy, B.}
\newblock Bootstrapped thompson sampling and deep exploration.
\newblock {\em arXiv preprint arXiv:1507.00300\/} (2015).

\bibitem{osband2017posterior}
{\sc Osband, I., and Van~Roy, B.}
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70\/} (2017), JMLR. org, pp.~2701--2710.

\bibitem{osband2016generalization}
{\sc Osband, I., Van~Roy, B., and Wen, Z.}
\newblock Generalization and exploration via randomized value functions.
\newblock In {\em International Conference on Machine Learning\/} (2016),
  pp.~2377--2386.

\bibitem{ostrovski2017count}
{\sc Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R.}
\newblock Count-based exploration with neural density models.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70\/} (2017), JMLR. org, pp.~2721--2730.

\bibitem{oudeyer2007intrinsic}
{\sc Oudeyer, P.-Y., Kaplan, F., and Hafner, V.~V.}
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock {\em IEEE transactions on evolutionary computation 11}, 2 (2007),
  265--286.

\bibitem{owen_safe_2000}
{\sc Owen, A., and Zhou, Y.}
\newblock Safe and {Effective} {Importance} {Sampling}.
\newblock {\em Journal of the American Statistical Association\/} (Mar. 2000),
  135--143.

\bibitem{mcbook}
{\sc Owen, A.~B.}
\newblock {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem{mcbook_2013}
{\sc Owen, A.~B.}
\newblock {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem{pandey2007multi}
{\sc Pandey, S., Chakrabarti, D., and Agarwal, D.}
\newblock Multi-armed bandit problems with dependent arms.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning\/} (2007), ACM, pp.~721--728.

\bibitem{pathak2017curiosity}
{\sc Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.}
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops\/} (2017), pp.~16--17.

\bibitem{peters2008reinforcement}
{\sc Peters, J., and Schaal, S.}
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock {\em Neural networks 21}, 4 (2008), 682--697.

\bibitem{DBLP:conf/icml/2017}
{\sc Precup, D., and Teh, Y.~W.}, Eds.
\newblock {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017\/} (2017),
  vol.~70 of {\em Proceedings of Machine Learning Research}, {PMLR}.

\bibitem{puterman2014markov}
{\sc Puterman, M.~L.}
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{renyi1961measures}
{\sc R{\'{e}}nyi, A.}
\newblock On measures of entropy and information.
\newblock Tech. rep., Hungarian Academy of Sciences Budapest Hungary, 1961.

\bibitem{robbins1985some}
{\sc Robbins, H.}
\newblock Some aspects of the sequential design of experiments.
\newblock In {\em Herbert Robbins Selected Papers}. Springer, 1985,
  pp.~169--177.

\bibitem{rummery1994line}
{\sc Rummery, G.~A., and Niranjan, M.}
\newblock {\em On-line Q-learning using connectionist systems}, vol.~37.
\newblock University of Cambridge, Department of Engineering Cambridge,
  England, 1994.

\bibitem{russo2013eluder}
{\sc Russo, D., and Van~Roy, B.}
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2013),
  pp.~2256--2264.

\bibitem{russo2018tutorial}
{\sc Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al.}
\newblock A tutorial on thompson sampling.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning 11},
  1 (2018), 1--96.

\bibitem{sallans2004reinforcement}
{\sc Sallans, B., and Hinton, G.~E.}
\newblock Reinforcement learning with factored states and actions.
\newblock {\em Journal of Machine Learning Research 5}, Aug (2004), 1063--1088.

\bibitem{saritacc2017combinatorial}
{\sc Sarita{\c{c}}, A.~{\"O}., and Tekin, C.}
\newblock Combinatorial multi-armed bandit problem with probabilistically
  triggered arms: A case with bounded regret.
\newblock In {\em Signal and Information Processing (GlobalSIP), 2017 IEEE
  Global Conference on\/} (2017), IEEE, pp.~111--115.

\bibitem{schmidhuber1991possibility}
{\sc Schmidhuber, J.}
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In {\em Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats\/} (1991), pp.~222--227.

\bibitem{schulman2015trust}
{\sc Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.}
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning\/} (2015),
  pp.~1889--1897.

\bibitem{sehnke2008policy}
{\sc Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J.,
  and Schmidhuber, J.}
\newblock Policy gradients with parameter-based exploration for control.
\newblock In {\em International Conference on Artificial Neural Networks\/}
  (2008), Springer, pp.~387--396.

\bibitem{sehnke2010parameter}
{\sc Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J.,
  and Schmidhuber, J.}
\newblock Parameter-exploring policy gradients.
\newblock {\em Neural Networks 23}, 4 (2010), 551--559.

\bibitem{silver2014deterministic}
{\sc Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
  Riedmiller, M.}
\newblock Deterministic policy gradient algorithms.
\newblock In {\em ICML\/} (2014).

\bibitem{srinivas2010gaussian}
{\sc Srinivas, N., Krause, A., Kakade, S., and Seeger, M.}
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)\/} (Haifa, Israel, June 2010), J.~F{\"u}rnkranz and
  T.~Joachims, Eds., Omnipress, pp.~1015--1022.

\bibitem{still2012information}
{\sc Still, S., and Precup, D.}
\newblock An information-theoretic approach to curiosity-driven reinforcement
  learning.
\newblock {\em Theory in Biosciences 131}, 3 (2012), 139--148.

\bibitem{strehl2009reinforcement}
{\sc Strehl, A.~L., Li, L., and Littman, M.~L.}
\newblock Reinforcement learning in finite mdps: Pac analysis.
\newblock {\em Journal of Machine Learning Research 10}, Nov (2009),
  2413--2444.

\bibitem{strens2000bayesian}
{\sc Strens, M.}
\newblock A bayesian framework for reinforcement learning.
\newblock In {\em ICML\/} (2000), vol.~2000, pp.~943--950.

\bibitem{sutton1988learning}
{\sc Sutton, R.~S.}
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning 3}, 1 (1988), 9--44.

\bibitem{sutton1991dyna}
{\sc Sutton, R.~S.}
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock {\em ACM SIGART Bulletin 2}, 4 (1991), 160--163.

\bibitem{sutton2018reinforcement}
{\sc Sutton, R.~S., and Barto, A.~G.}
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
{\sc Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.}
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems\/} (2000),
  pp.~1057--1063.

\bibitem{tang2017exploration}
{\sc Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.}
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In {\em Advances in neural information processing systems\/} (2017),
  pp.~2753--2762.

\bibitem{thompson1933likelihood}
{\sc Thompson, W.~R.}
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika 25}, 3/4 (1933), 285--294.

\bibitem{thrun1992efficient}
{\sc Thrun, S.~B.}
\newblock Efficient exploration in reinforcement learning.

\bibitem{tokic2011value}
{\sc Tokic, M., and Palm, G.}
\newblock Value-difference based exploration: adaptive control between
  epsilon-greedy and softmax.
\newblock In {\em Annual Conference on Artificial Intelligence\/} (2011),
  Springer, pp.~335--346.

\bibitem{vakili2011deterministic}
{\sc Vakili, S., Liu, K., and Zhao, Q.}
\newblock Deterministic sequencing of exploration and exploitation for
  multi-armed bandit problems.
\newblock {\em arXiv preprint arXiv:1106.6104\/} (2011).

\bibitem{veach_optimally_1995}
{\sc Veach, E., and Guibas, L.~J.}
\newblock Optimally combining sampling techniques for {Monte} {Carlo}
  rendering.
\newblock In {\em Proceedings of the 22nd annual conference on Computer
  graphics and interactive techniques - SIGGRAPH '95\/} (1995), ACM Press,
  pp.~419--428.

\bibitem{wang2018regional}
{\sc Wang, Z., Zhou, R., and Shen, C.}
\newblock Regional multi-armed bandits.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics\/} (2018), pp.~510--518.

\bibitem{watkins1989learning}
{\sc Watkins, C. J. C.~H.}
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\bibitem{weinstein2012bandit}
{\sc Weinstein, A., and Littman, M.~L.}
\newblock Bandit-based planning and learning in continuous-action markov
  decision processes.
\newblock In {\em Proceedings of the Twenty-Second International Conference on
  Automated Planning and Scheduling, {ICAPS} 2012, Atibaia, S{\~{a}}o Paulo,
  Brazil, June 25-19, 2012\/} (2012).

\bibitem{whitehead1991study}
{\sc Whitehead, S.~D., and Ballard, D.~H.}
\newblock {\em A study of cooperative mechanisms for faster reinforcement
  learning}.
\newblock University of Rochester, Department of Computer Science Rochester,
  NY, 1991.

\bibitem{williams2006gaussian}
{\sc Williams, C.~K., and Rasmussen, C.~E.}
\newblock {\em Gaussian processes for machine learning}, vol.~2.
\newblock MIT Press Cambridge, MA, 2006.

\bibitem{williams1992simple}
{\sc Williams, R.~J.}
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning 8}, 3-4 (1992), 229--256.

\bibitem{yupure}
{\sc Yu, X., Shao, H., Lyu, M.~R., and King, I.}
\newblock Pure exploration of multi-armed bandits with heavy-tailed payoffs.

\bibitem{zhao2011analysis}
{\sc Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M.}
\newblock Analysis and improvement of policy gradient estimation.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2011),
  pp.~262--270.

\bibitem{ziebart2008maximum}
{\sc Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.}
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai\/} (2008), vol.~8, Chicago, IL, USA, pp.~1433--1438.

\end{thebibliography}
