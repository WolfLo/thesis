\begin{thebibliography}{}

\bibitem[Agrawal, 1995a]{agrawal1995continuum}
Agrawal, R. (1995a).
\newblock The continuum-armed bandit problem.
\newblock {\em SIAM Journal on Control and Optimization}, 33(6):1926--1951.

\bibitem[Agrawal, 1995b]{agrawal1995sample}
Agrawal, R. (1995b).
\newblock Sample mean based index policies by o (log n) regret for the
  multi-armed bandit problem.
\newblock {\em Advances in Applied Probability}, 27(4):1054--1078.

\bibitem[Agrawal and Goyal, 2013]{agrawal2013further}
Agrawal, S. and Goyal, N. (2013).
\newblock Further optimal regret bounds for thompson sampling.
\newblock In {\em Artificial intelligence and statistics}, pages 99--107.

\bibitem[Amari, 1998]{amari1998natural2}
Amari, S.-I. (1998).
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276.

\bibitem[Amari and Douglas, 1998]{amari1998natural1}
Amari, S.-I. and Douglas, S.~C. (1998).
\newblock Why natural gradient?
\newblock In {\em Proceedings of the 1998 IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)},
  volume~2, pages 1213--1216. IEEE.

\bibitem[Antos et~al., 2008]{antos2008fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R. (2008).
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In {\em Advances in neural information processing systems}, pages
  9--16.

\bibitem[Auer, 2002]{auer2002using}
Auer, P. (2002).
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):397--422.

\bibitem[Auer et~al., 2002]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002).
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning}, 47(2-3):235--256.

\bibitem[Auer and Ortner, 2007]{auer2007logarithmic}
Auer, P. and Ortner, R. (2007).
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  49--56.

\bibitem[Auer and Ortner, 2010]{auer2010ucb}
Auer, P. and Ortner, R. (2010).
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock {\em Periodica Mathematica Hungarica}, 61(1-2):55--65.

\bibitem[Baird~III, 1993]{baird1993advantage}
Baird~III, L.~C. (1993).
\newblock Advantage updating.
\newblock Technical report, WRIGHT LAB WRIGHT-PATTERSON AFB OH.

\bibitem[Baxter and Bartlett, 2001]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L. (2001).
\newblock Infinite-horizon policy-gradient estimation.
\newblock {\em Journal of Artificial Intelligence Research}, 15:319--350.

\bibitem[Bellemare et~al., 2016]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R. (2016).
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1471--1479.

\bibitem[Blei et~al., 2017]{blei2017variational}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. (2017).
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association},
  112(518):859--877.

\bibitem[Blundell et~al., 2015]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015).
\newblock Weight uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:1505.05424}.

\bibitem[Boucheron et~al., 2013]{boucheron2013concentration}
Boucheron, S., Lugosi, G., and Massart, P. (2013).
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press.

\bibitem[Brafman and Tennenholtz, 2002]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M. (2002).
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 3(Oct):213--231.

\bibitem[Brockman et~al., 2016a]{gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016a).
\newblock Openai gym.

\bibitem[Brockman et~al., 2016b]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016b).
\newblock Openai gym.

\bibitem[Bubeck et~al., 2012]{bubeck2012regret}
Bubeck, S., Cesa-Bianchi, N., et~al. (2012).
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  5(1):1--122.

\bibitem[Bubeck et~al., 2013]{bubeck2013bandits}
Bubeck, S., Cesa-Bianchi, N., and Lugosi, G. (2013).
\newblock Bandits with heavy tail.
\newblock {\em IEEE Transactions on Information Theory}, 59(11):7711--7717.

\bibitem[Bubeck et~al., 2011]{bubeck2011x}
Bubeck, S., Munos, R., Stoltz, G., and Szepesv{\'a}ri, C. (2011).
\newblock X-armed bandits.
\newblock {\em Journal of Machine Learning Research}, 12(May):1655--1695.

\bibitem[Cesa-Bianchi et~al., 2017]{cesa2017boltzmann}
Cesa-Bianchi, N., Gentile, C., Lugosi, G., and Neu, G. (2017).
\newblock Boltzmann exploration done right.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6284--6293.

\bibitem[Chentanez et~al., 2005]{chentanez2005intrinsically}
Chentanez, N., Barto, A.~G., and Singh, S.~P. (2005).
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1281--1288.

\bibitem[Choshen et~al., 2018]{choshen2018dora}
Choshen, L., Fox, L., and Loewenstein, Y. (2018).
\newblock Dora the explorer: Directed outreaching reinforcement
  action-selection.
\newblock {\em arXiv preprint arXiv:1804.04012}.

\bibitem[Cochran, 2007]{cochran2007sampling}
Cochran, W.~G. (2007).
\newblock {\em Sampling techniques}.
\newblock John Wiley \& Sons.

\bibitem[Cortes et~al., 2010]{cortes2010learning}
Cortes, C., Mansour, Y., and Mohri, M. (2010).
\newblock Learning bounds for importance weighting.
\newblock In Lafferty, J.~D., Williams, C. K.~I., Shawe-Taylor, J., Zemel,
  R.~S., and Culotta, A., editors, {\em Advances in Neural Information
  Processing Systems 23}, pages 442--450. Curran Associates, Inc.

\bibitem[Dayan and Hinton, 1997]{dayan1997using}
Dayan, P. and Hinton, G.~E. (1997).
\newblock Using expectation-maximization for reinforcement learning.
\newblock {\em Neural Computation}, 9(2):271--278.

\bibitem[Degris et~al., 2012]{degris2012off}
Degris, T., White, M., and Sutton, R.~S. (2012).
\newblock Off-policy actor-critic.
\newblock {\em arXiv preprint arXiv:1205.4839}.

\bibitem[Deisenroth et~al., 2013]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al. (2013).
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends{\textregistered} in Robotics},
  2(1--2):1--142.

\bibitem[Dhariwal et~al., 2017]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P. (2017).
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}.

\bibitem[Dorato et~al., 1995]{dorato1995linear}
Dorato, P., Abdallah, C.~T., Cerone, V., and Jacobson, D.~H. (1995).
\newblock {\em Linear-quadratic control: an introduction}.
\newblock Prentice Hall Englewood Cliffs, NJ.

\bibitem[Duan et~al., 2016]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016).
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International Conference on Machine Learning}, pages
  1329--1338.

\bibitem[Espeholt et~al., 2018]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
  (2018).
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 1406--1415.

\bibitem[Garivier and Capp{\'e}, 2011]{garivier2011kl}
Garivier, A. and Capp{\'e}, O. (2011).
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In {\em Proceedings of the 24th annual conference on learning
  theory}, pages 359--376.

\bibitem[Gil et~al., 2013]{gil2013renyi}
Gil, M., Alajaji, F., and Linder, T. (2013).
\newblock R{\'e}nyi divergence measures for commonly used univariate continuous
  distributions.
\newblock {\em Information Sciences}, 249:124--131.

\bibitem[Glynn, 1990]{glynn1990likelihood}
Glynn, P.~W. (1990).
\newblock Likelihood ratio gradient estimation for stochastic systems.
\newblock {\em Communications of the ACM}, 33(10):75--84.

\bibitem[Goodfellow et~al., 2016]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A. (2016).
\newblock {\em Deep learning}.
\newblock MIT press.

\bibitem[Grondman et~al., 2012]{grondman2012survey}
Grondman, I., Busoniu, L., Lopes, G.~A., and Babuska, R. (2012).
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 42(6):1291--1307.

\bibitem[Haarnoja et~al., 2017]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017).
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pages
  1352--1361.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 1856--1865.

\bibitem[Hershey and Olsen, 2007]{hershey2007approximating}
Hershey, J.~R. and Olsen, P.~A. (2007).
\newblock Approximating the kullback leibler divergence between gaussian
  mixture models.
\newblock In {\em Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.
  IEEE International Conference on}, volume~4, pages IV--317. IEEE.

\bibitem[Houthooft et~al., 2016]{houthooft2016vime}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel, P.
  (2016).
\newblock Vime: Variational information maximizing exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1109--1117.

\bibitem[Ionides, 2008]{ionides2008truncated}
Ionides, E.~L. (2008).
\newblock Truncated importance sampling.
\newblock {\em Journal of Computational and Graphical Statistics},
  17(2):295--311.

\bibitem[Kearns and Singh, 2002]{kearns2002near}
Kearns, M. and Singh, S. (2002).
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49(2-3):209--232.

\bibitem[Kimura, 1999]{kimura1999efficient}
Kimura, H. (1999).
\newblock Efficient non-linear control by combining q-learning with local
  linear controllers.
\newblock In {\em Proceedings of the 16th International Conference on Machine
  Learning}, pages 210--219.

\bibitem[Kleinberg et~al., 2008]{kleinberg2008multi}
Kleinberg, R., Slivkins, A., and Upfal, E. (2008).
\newblock Multi-armed bandits in metric spaces.
\newblock In {\em Proceedings of the fortieth annual ACM symposium on Theory of
  computing}, pages 681--690. ACM.

\bibitem[Kleinberg et~al., 2013]{kleinberg2013bandits}
Kleinberg, R., Slivkins, A., and Upfal, E. (2013).
\newblock Bandits and experts in metric spaces.
\newblock {\em arXiv preprint arXiv:1312.1277}.

\bibitem[Kleinberg, 2005]{kleinberg2005nearly}
Kleinberg, R.~D. (2005).
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  697--704.

\bibitem[Kober and Peters, 2009]{kober2009policy}
Kober, J. and Peters, J.~R. (2009).
\newblock Policy search for motor primitives in robotics.
\newblock In {\em Advances in neural information processing systems}, pages
  849--856.

\bibitem[Kong, 1992]{kong1992note}
Kong, A. (1992).
\newblock A note on importance sampling using standardized weights.
\newblock {\em University of Chicago, Dept. of Statistics, Tech. Rep}, 348.

\bibitem[Kupcsik et~al., 2013]{kupcsik2013data}
Kupcsik, A.~G., Deisenroth, M.~P., Peters, J., and Neumann, G. (2013).
\newblock Data-efficient generalization of robot skills with contextual policy
  search.
\newblock In {\em Twenty-Seventh AAAI Conference on Artificial Intelligence}.

\bibitem[Lai and Robbins, 1985]{lai1985asymptotically}
Lai, T.~L. and Robbins, H. (1985).
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics}, 6(1):4--22.

\bibitem[Lange et~al., 2012]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M. (2012).
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}, pages 45--73. Springer.

\bibitem[Lattimore and Szepesv\'{a}ri, 2019]{lattimore2019bandit}
Lattimore, T. and Szepesv\'{a}ri, C. (2019).
\newblock {\em Bandit Algorithms}.
\newblock Cambridge University Press (preprint).

\bibitem[Lopes et~al., 2012]{lopes2012exploration}
Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y. (2012).
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  206--214.

\bibitem[Magureanu et~al., 2014]{magureanu2014lipschitz}
Magureanu, S., Combes, R., and Proutiere, A. (2014).
\newblock Lipschitz bandits: Regret lower bounds and optimal algorithms.
\newblock {\em arXiv preprint arXiv:1405.4758}.

\bibitem[Martino et~al., 2017]{martino2017effective}
Martino, L., Elvira, V., and Louzada, F. (2017).
\newblock Effective sample size for importance sampling based on discrepancy
  measures.
\newblock {\em Signal Processing}, 131:386--401.

\bibitem[Metelli et~al., 2018]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M. (2018).
\newblock Policy optimization via importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5447--5459.

\bibitem[Mnih et~al., 2016]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley, T.,
  Silver, D., and Kavukcuoglu, K. (2016).
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, pages
  1928--1937.

\bibitem[Mor{\'e} and Thuente, 1994]{more1994line}
Mor{\'e}, J.~J. and Thuente, D.~J. (1994).
\newblock Line search algorithms with guaranteed sufficient decrease.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)},
  20(3):286--307.

\bibitem[Oord et~al., 2016]{oord2016pixel}
Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K. (2016).
\newblock Pixel recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1601.06759}.

\bibitem[Osband et~al., 2016]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B. (2016).
\newblock Deep exploration via bootstrapped dqn.
\newblock In {\em Advances in neural information processing systems}, pages
  4026--4034.

\bibitem[Osband et~al., 2013]{osband2013more}
Osband, I., Russo, D., and Roy, B.~V. (2013).
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In {\em Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.},
  pages 3003--3011.

\bibitem[Osband and Van~Roy, 2015]{osband2015bootstrapped}
Osband, I. and Van~Roy, B. (2015).
\newblock Bootstrapped thompson sampling and deep exploration.
\newblock {\em arXiv preprint arXiv:1507.00300}.

\bibitem[Osband and Van~Roy, 2017]{osband2017posterior}
Osband, I. and Van~Roy, B. (2017).
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2701--2710. JMLR. org.

\bibitem[Ostrovski et~al., 2017]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R. (2017).
\newblock Count-based exploration with neural density models.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2721--2730. JMLR. org.

\bibitem[Oudeyer et~al., 2007]{oudeyer2007intrinsic}
Oudeyer, P.-Y., Kaplan, F., and Hafner, V.~V. (2007).
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock {\em IEEE transactions on evolutionary computation}, 11(2):265--286.

\bibitem[Owen, 2013]{mcbook}
Owen, A.~B. (2013).
\newblock {\em Monte Carlo theory, methods and examples}.

\bibitem[Peters and Schaal, 2008]{peters2008reinforcement}
Peters, J. and Schaal, S. (2008).
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock {\em Neural networks}, 21(4):682--697.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[R{\'{e}}nyi, 1961]{renyi1961measures}
R{\'{e}}nyi, A. (1961).
\newblock On measures of entropy and information.
\newblock Technical report, Hungarian Academy of Sciences Budapest Hungary.

\bibitem[Rummery and Niranjan, 1994]{rummery1994line}
Rummery, G.~A. and Niranjan, M. (1994).
\newblock {\em On-line Q-learning using connectionist systems}, volume~37.
\newblock University of Cambridge, Department of Engineering Cambridge,
  England.

\bibitem[Russo and Van~Roy, 2013]{russo2013eluder}
Russo, D. and Van~Roy, B. (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2256--2264.

\bibitem[Russo et~al., 2018]{russo2018tutorial}
Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al. (2018).
\newblock A tutorial on thompson sampling.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  11(1):1--96.

\bibitem[Sallans and Hinton, 2004]{sallans2004reinforcement}
Sallans, B. and Hinton, G.~E. (2004).
\newblock Reinforcement learning with factored states and actions.
\newblock {\em Journal of Machine Learning Research}, 5(Aug):1063--1088.

\bibitem[Schmidhuber, 1991]{schmidhuber1991possibility}
Schmidhuber, J. (1991).
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In {\em Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats}, pages 222--227.

\bibitem[Schulman et~al., 2015]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015).
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897.

\bibitem[Scott, 2015]{scott2015multivariate}
Scott, D.~W. (2015).
\newblock {\em Multivariate density estimation: theory, practice, and
  visualization}.
\newblock John Wiley \& Sons.

\bibitem[Sehnke et~al., 2008]{sehnke2008policy}
Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J., and
  Schmidhuber, J. (2008).
\newblock Policy gradients with parameter-based exploration for control.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 387--396. Springer.

\bibitem[Sehnke et~al., 2010]{sehnke2010parameter}
Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J., and
  Schmidhuber, J. (2010).
\newblock Parameter-exploring policy gradients.
\newblock {\em Neural Networks}, 23(4):551--559.

\bibitem[Silver et~al., 2014]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
  (2014).
\newblock Deterministic policy gradient algorithms.
\newblock In {\em ICML}.

\bibitem[Srinivas et~al., 2010]{srinivas2010gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In F{\"u}rnkranz, J. and Joachims, T., editors, {\em Proceedings of
  the 27th International Conference on Machine Learning (ICML-10)}, pages
  1015--1022, Haifa, Israel. Omnipress.

\bibitem[Still and Precup, 2012]{still2012information}
Still, S. and Precup, D. (2012).
\newblock An information-theoretic approach to curiosity-driven reinforcement
  learning.
\newblock {\em Theory in Biosciences}, 131(3):139--148.

\bibitem[Strens, 2000]{strens2000bayesian}
Strens, M. (2000).
\newblock A bayesian framework for reinforcement learning.
\newblock In {\em ICML}, volume 2000, pages 943--950.

\bibitem[Sutton, 1988]{sutton1988learning}
Sutton, R.~S. (1988).
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44.

\bibitem[Sutton, 1991]{sutton1991dyna}
Sutton, R.~S. (1991).
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock {\em ACM SIGART Bulletin}, 2(4):160--163.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Sutton et~al., 2000]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063.

\bibitem[Tang et~al., 2017]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P. (2017).
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2753--2762.

\bibitem[Thompson, 1933]{thompson1933likelihood}
Thompson, W.~R. (1933).
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294.

\bibitem[Thrun, 1992]{thrun1992efficient}
Thrun, S.~B. (1992).
\newblock Efficient exploration in reinforcement learning.

\bibitem[Tornio and Raiko, 2006]{tornio2006variational}
Tornio, M. and Raiko, T. (2006).
\newblock Variational bayesian approach for nonlinear identification and
  control.
\newblock In {\em Proc. of the IFAC Workshop on Nonlinear Model Predictive
  Control for Fast Systems, NMPC FS06}, pages 41--46. Citeseer.

\bibitem[Veach and Guibas, 1995]{veach_optimally_1995}
Veach, E. and Guibas, L.~J. (1995).
\newblock Optimally combining sampling techniques for {Monte} {Carlo}
  rendering.
\newblock In {\em Proceedings of the 22nd annual conference on Computer
  graphics and interactive techniques - SIGGRAPH '95}, pages 419--428. ACM
  Press.

\bibitem[Watkins, 1989]{watkins1989learning}
Watkins, C. J. C.~H. (1989).
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge.

\bibitem[Wawrzy{\'n}ski, 2005]{wawrzynski2005intensive}
Wawrzy{\'n}ski, P. (2005).
\newblock {\em Intensive reinforcement learning}.
\newblock PhD thesis, Institute of Control and Computation Engineering,
  Supervisor: Andrzej Pacut,.

\bibitem[Whitehead and Ballard, 1991]{whitehead1991study}
Whitehead, S.~D. and Ballard, D.~H. (1991).
\newblock {\em A study of cooperative mechanisms for faster reinforcement
  learning}.
\newblock University of Rochester, Department of Computer Science Rochester,
  NY.

\bibitem[Williams and Rasmussen, 2006]{williams2006gaussian}
Williams, C.~K. and Rasmussen, C.~E. (2006).
\newblock {\em Gaussian processes for machine learning}, volume~2.
\newblock MIT Press Cambridge, MA.

\bibitem[Williams, 1992]{williams1992simple}
Williams, R.~J. (1992).
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256.

\bibitem[Zhao et~al., 2011]{zhao2011analysis}
Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M. (2011).
\newblock Analysis and improvement of policy gradient estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  262--270.

\bibitem[Ziebart et~al., 2008]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K. (2008).
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA.

\end{thebibliography}
