\begin{thebibliography}{10}

\bibitem{agrawal1995continuum}
{\sc Agrawal, R.}
\newblock The continuum-armed bandit problem.
\newblock {\em SIAM Journal on Control and Optimization 33}, 6 (1995),
  1926--1951.

\bibitem{agrawal2013further}
{\sc Agrawal, S., and Goyal, N.}
\newblock Further optimal regret bounds for thompson sampling.
\newblock In {\em Artificial intelligence and statistics\/} (2013),
  pp.~99--107.

\bibitem{amari1998natural2}
{\sc Amari, S.-I.}
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation 10}, 2 (1998), 251--276.

\bibitem{amari1998natural1}
{\sc Amari, S.-I., and Douglas, S.~C.}
\newblock Why natural gradient?
\newblock In {\em Proceedings of the 1998 IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)\/}
  (1998), vol.~2, IEEE, pp.~1213--1216.

\bibitem{antos2008fitted}
{\sc Antos, A., Szepesv{\'a}ri, C., and Munos, R.}
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In {\em Advances in neural information processing systems\/} (2008),
  pp.~9--16.

\bibitem{auer2002finite}
{\sc Auer, P., Cesa-Bianchi, N., and Fischer, P.}
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning 47}, 2-3 (2002), 235--256.

\bibitem{auer2007logarithmic}
{\sc Auer, P., and Ortner, R.}
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2007),
  pp.~49--56.

\bibitem{auer2010ucb}
{\sc Auer, P., and Ortner, R.}
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock {\em Periodica Mathematica Hungarica 61}, 1-2 (2010), 55--65.

\bibitem{baird1993advantage}
{\sc Baird~III, L.~C.}
\newblock Advantage updating.
\newblock Tech. rep., WRIGHT LAB WRIGHT-PATTERSON AFB OH, 1993.

\bibitem{baxter2001infinite}
{\sc Baxter, J., and Bartlett, P.~L.}
\newblock Infinite-horizon policy-gradient estimation.
\newblock {\em Journal of Artificial Intelligence Research 15\/} (2001),
  319--350.

\bibitem{bellemare2016unifying}
{\sc Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.}
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2016),
  pp.~1471--1479.

\bibitem{blei2017variational}
{\sc Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D.}
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association 112}, 518
  (2017), 859--877.

\bibitem{blundell2015weight}
{\sc Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.}
\newblock Weight uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:1505.05424\/} (2015).

\bibitem{boucheron2013concentration}
{\sc Boucheron, S., Lugosi, G., and Massart, P.}
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem{brafman2002r}
{\sc Brafman, R.~I., and Tennenholtz, M.}
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research 3}, Oct (2002), 213--231.

\bibitem{gym}
{\sc Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J.,
  Tang, J., and Zaremba, W.}
\newblock Openai gym, 2016.

\bibitem{brockman2016openai}
{\sc Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J.,
  Tang, J., and Zaremba, W.}
\newblock Openai gym, 2016.

\bibitem{bubeck2012regret}
{\sc Bubeck, S., Cesa-Bianchi, N., et~al.}
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning 5},
  1 (2012), 1--122.

\bibitem{bubeck2013bandits}
{\sc Bubeck, S., Cesa-Bianchi, N., and Lugosi, G.}
\newblock Bandits with heavy tail.
\newblock {\em IEEE Transactions on Information Theory 59}, 11 (2013),
  7711--7717.

\bibitem{bubeck2011x}
{\sc Bubeck, S., Munos, R., Stoltz, G., and Szepesv{\'a}ri, C.}
\newblock X-armed bandits.
\newblock {\em Journal of Machine Learning Research 12}, May (2011),
  1655--1695.

\bibitem{cesa2017boltzmann}
{\sc Cesa-Bianchi, N., Gentile, C., Lugosi, G., and Neu, G.}
\newblock Boltzmann exploration done right.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2017),
  pp.~6284--6293.

\bibitem{chentanez2005intrinsically}
{\sc Chentanez, N., Barto, A.~G., and Singh, S.~P.}
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in neural information processing systems\/} (2005),
  pp.~1281--1288.

\bibitem{choshen2018dora}
{\sc Choshen, L., Fox, L., and Loewenstein, Y.}
\newblock Dora the explorer: Directed outreaching reinforcement
  action-selection.
\newblock {\em arXiv preprint arXiv:1804.04012\/} (2018).

\bibitem{cochran2007sampling}
{\sc Cochran, W.~G.}
\newblock {\em Sampling techniques}.
\newblock John Wiley \& Sons, 2007.

\bibitem{cortes2010learning}
{\sc Cortes, C., Mansour, Y., and Mohri, M.}
\newblock Learning bounds for importance weighting.
\newblock In {\em Advances in Neural Information Processing Systems 23}, J.~D.
  Lafferty, C.~K.~I. Williams, J.~Shawe-Taylor, R.~S. Zemel, and A.~Culotta,
  Eds. Curran Associates, Inc., 2010, pp.~442--450.

\bibitem{dayan1997using}
{\sc Dayan, P., and Hinton, G.~E.}
\newblock Using expectation-maximization for reinforcement learning.
\newblock {\em Neural Computation 9}, 2 (1997), 271--278.

\bibitem{degris2012off}
{\sc Degris, T., White, M., and Sutton, R.~S.}
\newblock Off-policy actor-critic.
\newblock {\em arXiv preprint arXiv:1205.4839\/} (2012).

\bibitem{deisenroth2013survey}
{\sc Deisenroth, M.~P., Neumann, G., Peters, J., et~al.}
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends{\textregistered} in Robotics 2}, 1--2
  (2013), 1--142.

\bibitem{baselines}
{\sc Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford,
  A., Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.}
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem{dorato1995linear}
{\sc Dorato, P., Abdallah, C.~T., Cerone, V., and Jacobson, D.~H.}
\newblock {\em Linear-quadratic control: an introduction}.
\newblock Prentice Hall Englewood Cliffs, NJ, 1995.

\bibitem{duan2016benchmarking}
{\sc Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.}
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International Conference on Machine Learning\/} (2016),
  pp.~1329--1338.

\bibitem{espeholt2018impala}
{\sc Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T.,
  Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu,
  K.}
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018\/} (2018), pp.~1406--1415.

\bibitem{garivier2011kl}
{\sc Garivier, A., and Capp{\'e}, O.}
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In {\em Proceedings of the 24th annual conference on learning
  theory\/} (2011), pp.~359--376.

\bibitem{gil2013renyi}
{\sc Gil, M., Alajaji, F., and Linder, T.}
\newblock R{\'e}nyi divergence measures for commonly used univariate continuous
  distributions.
\newblock {\em Information Sciences 249\/} (2013), 124--131.

\bibitem{glynn1990likelihood}
{\sc Glynn, P.~W.}
\newblock Likelihood ratio gradient estimation for stochastic systems.
\newblock {\em Communications of the ACM 33}, 10 (1990), 75--84.

\bibitem{goodfellow2016deep}
{\sc Goodfellow, I., Bengio, Y., and Courville, A.}
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{grondman2012survey}
{\sc Grondman, I., Busoniu, L., Lopes, G.~A., and Babuska, R.}
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews) 42}, 6 (2012), 1291--1307.

\bibitem{haarnoja2017reinforcement}
{\sc Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.}
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017\/} (2017),
  pp.~1352--1361.

\bibitem{haarnoja2018soft}
{\sc Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.}
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018\/} (2018), pp.~1856--1865.

\bibitem{hershey2007approximating}
{\sc Hershey, J.~R., and Olsen, P.~A.}
\newblock Approximating the kullback leibler divergence between gaussian
  mixture models.
\newblock In {\em Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.
  IEEE International Conference on\/} (2007), vol.~4, IEEE, pp.~IV--317.

\bibitem{houthooft2016vime}
{\sc Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel,
  P.}
\newblock Vime: Variational information maximizing exploration.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2016),
  pp.~1109--1117.

\bibitem{ionides2008truncated}
{\sc Ionides, E.~L.}
\newblock Truncated importance sampling.
\newblock {\em Journal of Computational and Graphical Statistics 17}, 2 (2008),
  295--311.

\bibitem{kearns2002near}
{\sc Kearns, M., and Singh, S.}
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning 49}, 2-3 (2002), 209--232.

\bibitem{kimura1999efficient}
{\sc Kimura, H.}
\newblock Efficient non-linear control by combining q-learning with local
  linear controllers.
\newblock In {\em Proceedings of the 16th International Conference on Machine
  Learning\/} (1999), pp.~210--219.

\bibitem{kleinberg2008multi}
{\sc Kleinberg, R., Slivkins, A., and Upfal, E.}
\newblock Multi-armed bandits in metric spaces.
\newblock In {\em Proceedings of the fortieth annual ACM symposium on Theory of
  computing\/} (2008), ACM, pp.~681--690.

\bibitem{kleinberg2013bandits}
{\sc Kleinberg, R., Slivkins, A., and Upfal, E.}
\newblock Bandits and experts in metric spaces.
\newblock {\em arXiv preprint arXiv:1312.1277\/} (2013).

\bibitem{kleinberg2005nearly}
{\sc Kleinberg, R.~D.}
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2005),
  pp.~697--704.

\bibitem{kober2009policy}
{\sc Kober, J., and Peters, J.~R.}
\newblock Policy search for motor primitives in robotics.
\newblock In {\em Advances in neural information processing systems\/} (2009),
  pp.~849--856.

\bibitem{kong1992note}
{\sc Kong, A.}
\newblock A note on importance sampling using standardized weights.
\newblock {\em University of Chicago, Dept. of Statistics, Tech. Rep 348\/}
  (1992).

\bibitem{kupcsik2013data}
{\sc Kupcsik, A.~G., Deisenroth, M.~P., Peters, J., and Neumann, G.}
\newblock Data-efficient generalization of robot skills with contextual policy
  search.
\newblock In {\em Twenty-Seventh AAAI Conference on Artificial Intelligence\/}
  (2013).

\bibitem{lai1985asymptotically}
{\sc Lai, T.~L., and Robbins, H.}
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics 6}, 1 (1985), 4--22.

\bibitem{lange2012batch}
{\sc Lange, S., Gabel, T., and Riedmiller, M.}
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}. Springer, 2012, pp.~45--73.

\bibitem{lattimore2019bandit}
{\sc Lattimore, T., and Szepesv\'{a}ri, C.}
\newblock {\em Bandit Algorithms}.
\newblock Cambridge University Press (preprint), 2019.

\bibitem{lopes2012exploration}
{\sc Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y.}
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2012),
  pp.~206--214.

\bibitem{magureanu2014lipschitz}
{\sc Magureanu, S., Combes, R., and Proutiere, A.}
\newblock Lipschitz bandits: Regret lower bounds and optimal algorithms.
\newblock {\em arXiv preprint arXiv:1405.4758\/} (2014).

\bibitem{martino2017effective}
{\sc Martino, L., Elvira, V., and Louzada, F.}
\newblock Effective sample size for importance sampling based on discrepancy
  measures.
\newblock {\em Signal Processing 131\/} (2017), 386--401.

\bibitem{metelli2018policy}
{\sc Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.}
\newblock Policy optimization via importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2018),
  pp.~5447--5459.

\bibitem{mnih2016asynchronous}
{\sc Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley,
  T., Silver, D., and Kavukcuoglu, K.}
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016\/} (2016),
  pp.~1928--1937.

\bibitem{more1994line}
{\sc Mor{\'e}, J.~J., and Thuente, D.~J.}
\newblock Line search algorithms with guaranteed sufficient decrease.
\newblock {\em ACM Transactions on Mathematical Software (TOMS) 20}, 3 (1994),
  286--307.

\bibitem{oord2016pixel}
{\sc Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K.}
\newblock Pixel recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1601.06759\/} (2016).

\bibitem{osband2016deep}
{\sc Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.}
\newblock Deep exploration via bootstrapped dqn.
\newblock In {\em Advances in neural information processing systems\/} (2016),
  pp.~4026--4034.

\bibitem{osband2013more}
{\sc Osband, I., Russo, D., and Roy, B.~V.}
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In {\em Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.\/}
  (2013), pp.~3003--3011.

\bibitem{osband2015bootstrapped}
{\sc Osband, I., and Van~Roy, B.}
\newblock Bootstrapped thompson sampling and deep exploration.
\newblock {\em arXiv preprint arXiv:1507.00300\/} (2015).

\bibitem{osband2017posterior}
{\sc Osband, I., and Van~Roy, B.}
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70\/} (2017), JMLR. org, pp.~2701--2710.

\bibitem{ostrovski2017count}
{\sc Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R.}
\newblock Count-based exploration with neural density models.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70\/} (2017), JMLR. org, pp.~2721--2730.

\bibitem{oudeyer2007intrinsic}
{\sc Oudeyer, P.-Y., Kaplan, F., and Hafner, V.~V.}
\newblock Intrinsic motivation systems for autonomous mental development.
\newblock {\em IEEE transactions on evolutionary computation 11}, 2 (2007),
  265--286.

\bibitem{mcbook}
{\sc Owen, A.~B.}
\newblock {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem{peters2008reinforcement}
{\sc Peters, J., and Schaal, S.}
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock {\em Neural networks 21}, 4 (2008), 682--697.

\bibitem{puterman2014markov}
{\sc Puterman, M.~L.}
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{renyi1961measures}
{\sc R{\'{e}}nyi, A.}
\newblock On measures of entropy and information.
\newblock Tech. rep., Hungarian Academy of Sciences Budapest Hungary, 1961.

\bibitem{rummery1994line}
{\sc Rummery, G.~A., and Niranjan, M.}
\newblock {\em On-line Q-learning using connectionist systems}, vol.~37.
\newblock University of Cambridge, Department of Engineering Cambridge,
  England, 1994.

\bibitem{russo2013eluder}
{\sc Russo, D., and Van~Roy, B.}
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2013),
  pp.~2256--2264.

\bibitem{russo2018tutorial}
{\sc Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al.}
\newblock A tutorial on thompson sampling.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning 11},
  1 (2018), 1--96.

\bibitem{sallans2004reinforcement}
{\sc Sallans, B., and Hinton, G.~E.}
\newblock Reinforcement learning with factored states and actions.
\newblock {\em Journal of Machine Learning Research 5}, Aug (2004), 1063--1088.

\bibitem{schmidhuber1991possibility}
{\sc Schmidhuber, J.}
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In {\em Proc. of the international conference on simulation of
  adaptive behavior: From animals to animats\/} (1991), pp.~222--227.

\bibitem{schulman2015trust}
{\sc Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.}
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning\/} (2015),
  pp.~1889--1897.

\bibitem{scott2015multivariate}
{\sc Scott, D.~W.}
\newblock {\em Multivariate density estimation: theory, practice, and
  visualization}.
\newblock John Wiley \& Sons, 2015.

\bibitem{sehnke2008policy}
{\sc Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J.,
  and Schmidhuber, J.}
\newblock Policy gradients with parameter-based exploration for control.
\newblock In {\em International Conference on Artificial Neural Networks\/}
  (2008), Springer, pp.~387--396.

\bibitem{sehnke2010parameter}
{\sc Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J.,
  and Schmidhuber, J.}
\newblock Parameter-exploring policy gradients.
\newblock {\em Neural Networks 23}, 4 (2010), 551--559.

\bibitem{silver2014deterministic}
{\sc Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
  Riedmiller, M.}
\newblock Deterministic policy gradient algorithms.
\newblock In {\em ICML\/} (2014).

\bibitem{srinivas2010gaussian}
{\sc Srinivas, N., Krause, A., Kakade, S., and Seeger, M.}
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)\/} (Haifa, Israel, June 2010), J.~F{\"u}rnkranz and
  T.~Joachims, Eds., Omnipress, pp.~1015--1022.

\bibitem{still2012information}
{\sc Still, S., and Precup, D.}
\newblock An information-theoretic approach to curiosity-driven reinforcement
  learning.
\newblock {\em Theory in Biosciences 131}, 3 (2012), 139--148.

\bibitem{strens2000bayesian}
{\sc Strens, M.}
\newblock A bayesian framework for reinforcement learning.
\newblock In {\em ICML\/} (2000), vol.~2000, pp.~943--950.

\bibitem{sutton1988learning}
{\sc Sutton, R.~S.}
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning 3}, 1 (1988), 9--44.

\bibitem{sutton1991dyna}
{\sc Sutton, R.~S.}
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock {\em ACM SIGART Bulletin 2}, 4 (1991), 160--163.

\bibitem{sutton2018reinforcement}
{\sc Sutton, R.~S., and Barto, A.~G.}
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
{\sc Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.}
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems\/} (2000),
  pp.~1057--1063.

\bibitem{tang2017exploration}
{\sc Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.}
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In {\em Advances in neural information processing systems\/} (2017),
  pp.~2753--2762.

\bibitem{thompson1933likelihood}
{\sc Thompson, W.~R.}
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika 25}, 3/4 (1933), 285--294.

\bibitem{thrun1992efficient}
{\sc Thrun, S.~B.}
\newblock Efficient exploration in reinforcement learning.

\bibitem{tornio2006variational}
{\sc Tornio, M., and Raiko, T.}
\newblock Variational bayesian approach for nonlinear identification and
  control.
\newblock In {\em Proc. of the IFAC Workshop on Nonlinear Model Predictive
  Control for Fast Systems, NMPC FS06\/} (2006), Citeseer, pp.~41--46.

\bibitem{veach_optimally_1995}
{\sc Veach, E., and Guibas, L.~J.}
\newblock Optimally combining sampling techniques for {Monte} {Carlo}
  rendering.
\newblock In {\em Proceedings of the 22nd annual conference on Computer
  graphics and interactive techniques - SIGGRAPH '95\/} (1995), ACM Press,
  pp.~419--428.

\bibitem{watkins1989learning}
{\sc Watkins, C. J. C.~H.}
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\bibitem{wawrzynski2005intensive}
{\sc Wawrzy{\'n}ski, P.}
\newblock {\em Intensive reinforcement learning}.
\newblock PhD thesis, Institute of Control and Computation Engineering,
  Supervisor: Andrzej Pacut,, 2005.

\bibitem{whitehead1991study}
{\sc Whitehead, S.~D., and Ballard, D.~H.}
\newblock {\em A study of cooperative mechanisms for faster reinforcement
  learning}.
\newblock University of Rochester, Department of Computer Science Rochester,
  NY, 1991.

\bibitem{williams2006gaussian}
{\sc Williams, C.~K., and Rasmussen, C.~E.}
\newblock {\em Gaussian processes for machine learning}, vol.~2.
\newblock MIT Press Cambridge, MA, 2006.

\bibitem{williams1992simple}
{\sc Williams, R.~J.}
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning 8}, 3-4 (1992), 229--256.

\bibitem{zhao2011analysis}
{\sc Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M.}
\newblock Analysis and improvement of policy gradient estimation.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2011),
  pp.~262--270.

\bibitem{ziebart2008maximum}
{\sc Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.}
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai\/} (2008), vol.~8, Chicago, IL, USA, pp.~1433--1438.

\end{thebibliography}
