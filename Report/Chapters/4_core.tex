% !TEX root = ../thesis.tex

\chapter{Optimistic Policy Search via Multiple Importance Sampling} \label{ch:core}

In this chapter, we present the main theoretical contributions of this thesis project. In Section \ref{sec:robust}, we extend Section \ref{sec:mis} to develop robust \gls{MIS} estimators that will play an essential role in the algorithms proposed. In Section \ref{sec:problem} we provide a formalization of the online policy optimization problem. The algorithms are presented in Section \ref{sec:algo} and analyzed in Section \ref{sec:regret}. The proposed algorithms, called \gls{OPTIMIST} and \gls{OPTIMIST}2, are based on the \gls{OFU} principle and follows the \gls{UCB} strategy, both introduced in Section \ref{sec:mab} and covered with practical implementations in (\ref{sec:expinmab}). The idea is to leverage on these techniques to deal with the problem of exploration in continuous-action \gls{RL}, for which the solutions proposed so far have been largely heuristic, as we have seen in Chapter \ref{ch:sota}. However, the solutions proposed here should not be interpreted as an application of the \gls{MAB} framework to \gls{PS} but rather as a way to formulate exploration in policy search as a \gls{MAB}-like problem


\section{Robust Importance Sampling Estimation}\label{sec:robust}
In this section, we discuss how to perform a robust importance weighting estimation. Recently, it has been observed that, in many cases of interest, the plain estimator \eqref{eq:ise} presents problematic tail behaviors \cite{metelli2018policy}, preventing the use of exponential concentration inequalities. In fact, unless we require that $d_{\infty}(P \| \Phi)$ is finite, \ie that the importance weight have finite supremum, there always exists a value $\alpha>1$ such that $d_{\alpha}(P \| \Phi)=+\infty$. A common heuristic to address this problem consists in truncating the weight \cite{ionides2008truncated}:
\begin{align}\label{eq:truncatedise}
	\widecheck{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N} \min \left\{ M, {w}_{P/Q}(z_i) \right\} f(z_i),
\end{align}
where 
% $\widecheck{w}_{P/Q}^M(z_i) = $ and $M_N$ 
$M>0$ is a threshold to limit the magnitude of the importance weight. Similarly, for the multiple importance sampling case, restricting to the \gls{BH}, we have:
\begin{align}\label{eq:truncatedmise}
	\widecheck{\mu}_{\text{BH}} =\frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \min \left\{M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} f(z_{ik}).
\end{align}
Clearly, since we are changing the importance weights, we introduce a bias term, but, by reducing the range of the estimation, we get a benefit in terms of variance. Below, we present the bias-variance analysis of the estimator $\widecheck{\mu}_{\text{BH}}$ and we conclude by showing that we are able, using an adaptive truncation, to guarantee an exponential concentration (differently from the non-truncated case).

\begin{restatable}{lemma}{truncatedbias}\label{lem:truncatedbias}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{+}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. Then, the bias of $\widecheck{\mu}_{\text{BH}}$ can be bounded as:
	 \begin{equation}
	 \label{eq:biastruncated}
         0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon},
    \end{equation}
    and the variance of $\widecheck{\mu}_{\text{BH}}$ can be bounded as:
    \begin{equation}
    \label{eq:variancetruncated}
         \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon}}{N},
    \end{equation}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%
It is worth noting that, by selecting $\epsilon=1$, Equation \ref{eq:variancetruncated} reduces to Lemma~\ref{lem:misevarbound}, as the truncation operation can only reduce the variance. Clearly, the smaller we choose $M$, the larger the bias. Overall, we are interested in mining the joint contribution of bias and variance. Keeping $P$ and $\Phi$ fixed we observe that the bias depends on $M$ only, whereas the variance depends on $M$ and on the number of samples $N$. Intuitively, we can allow larger truncation thresholds $M$ as the number of samples $N$ increases. The following result states that, when using an \textit{adaptive threshold} depending on $N$, we are able to reach exponential concentration.
\begin{restatable}{theorem}{thrucatedconcentration}\label{lem:thrucatedconcentration}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{+}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. 
	Let $M_N = \left( \frac{N d_{1+\epsilon}\left( P \| \Phi  \right)^{\epsilon} }{\log \frac{1}{\delta}} \right) ^{\frac{1}{1+\epsilon}}$, then with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.1}
        \widecheck{\mu}_{\text{BH}} \le \mu + \|f\|_{\infty} \left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{equation}
    and also, with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.2}
        \widecheck{\mu}_{\text{BH}} \ge \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}.
    \end{equation}
\end{restatable}

Our adaptive truncation approach and the consequent concentration results resemble the ones proposed in \cite{bubeck2013bandits}. However, unlike \cite{bubeck2013bandits} we do not remove samples with too high value, but we exploit the nature of the importance weighted estimator only to limit the weight magnitude. Indeed, this form of truncation turned out to be very effective in practice \cite{ionides2008truncated}.


\section{Problem Formalization}\label{sec:problem}
The online learning problem that we aim to solve does \textit{not} fall within the traditional \gls{MAB} framework (not in its basic version, anyway) and can benefit from an ad-hoc formalization, provided in this section.

Let $\Xspace\subseteq \Reals^d$ be our decision set, or \textit{arm set} in \gls{MAB} jargon. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let ${\{Z_{\vx}:\Omega\to\mathcal{Z}\mid \vx\in\Xspace\}}$ be a set of continuous random vectors parametrized by $\Xspace$, with common sample space ${\mathcal{Z}\subseteq \Reals^m}$. We denote with $p_{\vx}$ the probability density function of $Z_{\vx}$. Finally, let $f:\mathcal{Z}\to\Reals$ be a bounded \textit{payoff function}, and $\mu(\vx) = \Exp_{z\sim p_{\vx}}\left[f(z)\right]$ its expectation under $p_{\vx}$. For each iteration $t=0,\dots,T$, we select an arm $\vx_t$, draw a sample $z_t$ from $p_{\vx_t}$, and observe payoff $f(z_t)$, up to horizon $T$. The goal is to maximize the expected total payoff:
\begin{align}\label{eq:theproblem}
\max_{\vx_0,\dots,\vx_T\in\Xspace} \sum_{t=0}^T \Exp_{z_t\sim p_{\vx_t}}\left[ f(z_t)\right] = \max_{\vx_0,\dots,\vx_T\in\Xspace}\sum_{t=0}^{T}\mu(\vx_t).
\end{align}
Although we can evaluate $p_{\vx}$ for each $\vx\in\Xspace$, we can only observe $f(z_t)$ for the $z_t$ that are actually sampled. 
This models precisely the online, episodic policy optimization problem. Within this problem formulation, we distinguish between two categories of policy optimization: \emph{action-based} and \emph{parameter-based} policy optimization. 

\begin{itemize}
\item In \emph{action-based} policy optimization, $\Xspace$ corresponds to the parameter space $\Theta$ of a class of stochastic policies ${\{\pi_{\vtheta}\mid\vtheta\in\Theta\}}$; $\mathcal{Z}$ to the set of possible trajectories; $p_{\vx}$ to the density $p_{\vtheta}$ over trajectories induced by policy $\pi_{\vtheta}$; and $f(z)$ to cumulated reward $\Rew(\tau)$.
\item In \emph{parameter-based} policy optimization, $\Xspace$ corresponds to the hyperparameter space $\Xi$ of a class of stochastic hyperpolicies $\{\nu_{\vxi}\mid\vxi\in\Xi\}$; $\mathcal{Z}$ to the set of possible trajectories collected with policy $\pi_{\vtheta}$, with $\vtheta\in\Theta\sim\nu_{\vxi}$; $p_{\vx}$ to hyperpolicy $\nu_{\vxi}$; and $f(z)$ to performance $J(\vtheta)$.
\end{itemize}
 
In both cases, each iteration corresponds to a single episode, and horizon $T$ is the total number of episodes (not to be confused with the trajectory horizon $H$). With reference to the standard \gls{PS} framework, the first category is closely related to standard \gls{PG} methods that perform a search in a parametric policy space by following the gradient of the utility function. The second is more related to methods searching directly in the space of parameters like \gls{PGPE}. From now on, we will refer to (\ref{eq:theproblem}) simply as the policy optimization problem.

\begin{remark}
In abstract terms, (\ref{eq:theproblem}) is a sequential decision problem over a functional space of random variables, and may have applications beyond policy optimization.
\end{remark}

The peculiarity of this framework, compared to the classic \gls{MAB} one, is the special structure existing over the arms. In particular, the expected payoff $\mu$ of different arms is correlated thanks to the stochasticity of the $p_{\vx}$'s on a common sample space $\mathcal{Z}$.
We \textit{could}, of course, frame policy optimization as a \gls{MAB} problem, at the cost of ignoring some structure. It would be enough to regard $\mu(\vx)$ as the expectation of a totally unknown, stochastic reward function. This would put us in the continuous \gls{MAB} framework \cite{kleinberg2013bandits}, but would ignore the special arm correlation. In the following, we will show how this correlation can be exploited to guarantee efficient exploration. This insight stems from the literature on $\mathcal{X}$-armed bandits, as discussed in (\ref{sub:xarmedbandits}) and (\ref{sub:hoo}).

\section{Algorithms}\label{sec:algo}
In this section, we use the mathematical tools presented so far to design a policy search algorithm that efficiently explores the space of solutions. The proposed algorithm, called \gls{OPTIMIST}, is based on the \gls{OFU} principle and follows the \gls{UCB} strategy. 

To apply the \gls{UCB} strategy to the policy optimization problem described above, we need an estimate of the objective $\mu(\vx)$ and a confidence region. We use importance sampling to capture the correlation among the arms. In particular, to better use all the data that we collect, we would like to use a multiple importance sampling estimator like the one from (\ref{eq:mise}). Unfortunately, the heavy-tailed behavior of this estimator would result in an inefficient exploration.
Instead, we use the robust balance heuristic estimator $\wc{\mu}_{\text{BH}}$ from (\ref{eq:truncatedmise}), which has better tail behavior. To simplify the notation, we treat each sample $\vx$ as a distinct one. This is \wlg (as each sample is always multiplied by its number of occurrences anyway) and corresponds to the case $K=t-1$ and $N_k\equiv1$. Hence, at each iteration $t$:
\begin{align}\label{eq:wcmu}
	\wc{\mu}_t(\vx) = \sum_{k=0}^{t-1}
	\min\left\{M_{t-1}, \frac{p_{\vx}(z_k)}
	{\sum_{j=1}^{t-1}p_{\vx_j(z_k)}}\right\}f(z_k),
\end{align}
where ${M_{t} = \left(\frac{td_{1+\epsilon}(p_{\vx}\|\Phi_t)^{\epsilon}}{\log\frac{1}{\delta_t}}\right)^{\frac{1}{1+\epsilon}}}$ and ${\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}}$.
According to Theorem (\ref{lem:thrucatedconcentration}), the following \textit{index}:
\begin{align}
	&B_t^{\epsilon}(\vx,\delta_t) \coloneqq 
	\wc{\mu}_t(\vx)+\norm[\infty]{f}\left(\sqrt{2}+\frac{4}{3}\right)\left(\frac{d_{1+\epsilon}(p_{\vx_t}\|\Phi_{t})\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}}, \label{eq:optimistindex}
\end{align}
is an upper bound on $\mu(\vx)$ with probability at least $1-\delta_t$, \ie an upper confidence bound. The \gls{OPTIMIST} algorithm simply selects, at each iteration $t$, the arm with the largest value of the index $B_t^{\epsilon}(\vx)$, breaking ties deterministically. The pseudocode is provided in Algorithm \ref{alg:1}. The initial arm $\vx_0$ is arbitrary, as no prior information is available. The regret analysis of Section \ref{sec:regret} will provide a confidence schedule $(\delta_t)_{t=1}^T$. Knowledge of the actual horizon $T$ is not needed. Although we can use any $\epsilon\in (0,1]$, we suggest to use $\epsilon=1$ in practice, as it yields the more common $2$-\Renyi divergence. To be able to compute the indexes (or to perform any kind of index maximization), the algorithm needs to store all the $\vx_t$ together with the observed pay-offs $f(z_t)$, hence $\mathcal{O}(Td)$ space is required, where $d$ is the dimensionality of the arm space $\Xspace$ (not to be confused with cardinality $|\Xspace|$, which may be infinite). 

The optimization step (line \ref{line:optstep}) may be very difficult when $\Xspace$ is not discrete \cite{srinivas2010gaussian}, as the index $B_t^{\epsilon}(\vx,\delta_t)$ is non-convex and non-differentiable. Global optimization methods could be applied at the cost of giving up theoretical guarantees. In practice, this direction may be beneficial, but we leave it to future, more application-oriented work. Instead, we propose a general discretization method. The key intuition, common in the continuous \gls{MAB} literature, is to make the discretization progressively finer. The pseudocode for this variant, called \gls{OPTIMIST}2, is reported in Algorithm \ref{alg:2}. Note that the arm space $\Xspace$ itself is fixed (and infinite), as adaptive discretization is performed for optimization purposes only. 
Implementing any variant of \gls{OPTIMIST} to solve a policy optimization problem, whether in the action-based or in the parameter-based formulation, requires some additional caveats, discussed in Section \ref{sec:practical}.

\begin{algorithm}[t]
	\caption{\gls{OPTIMIST}}
	\label{alg:1}
	\begin{algorithmic}[1]
	\State {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$, order ${\epsilon\in(0,1]}$
	\State Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$ \label{line:sampling}
	\For{$t=1,\dots,T$}
		\State Select arm $\vx_t = \arg\max_{\vx\in\Xspace}B_t^{\epsilon}(\vx,\delta_t)$ \label{line:optstep}
		\State Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
	\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Regret Analysis}\label{sec:regret}
In this section, we provide high-probability guarantees on the quality of the solution provided by Algorithm \ref{alg:1}.
First, we rephrase the optimization problem (\ref{alg:1}) in terms of \textit{regret minimization}. The instantaneous regret, as defined in (\ref{def:immediateregret}), is:
\begin{align}\label{eq:inregret}
	\Delta_t = \mu(\vx^*) - \mu(\vx_t),
\end{align}
where $\vx^* \in \arg\max_{\vx\in\Xspace}\mu(\vx)$. Let $\Reg(T) = \sum_{t=0}^T\Delta_t$ be the total regret, as in (\ref{def:regret}).
As $\mu(\vx^*)$ is a constant, problem (\ref{eq:theproblem}) is trivially equivalent to:
\begin{align}\label{eq:regret}
	\min_{\vx_0,\vx_1,\dots,\vx_T\in\Xspace} \Reg(T).
\end{align}
In the following, we will show that Algorithm \ref{alg:1} yields sublinear regret under some mild assumptions. The proofs combine techniques from \cite{srinivas2010gaussian} and \cite{bubeck2013bandits} and are reported in Appendix \ref{app:proof}.
First, we need the following assumption on the \Renyi divergence:
%
\begin{restatable}{assumption}{boundrenyi}\label{ass:boundrenyi}
	For all $t=1,\dots,T$, the $(1+\epsilon)$-\Renyi divergence is uniformly bounded as:
	\begin{align*}
		\sup_{\vx_0,\vx_1,\dots,\vx_T\in\Xspace}d_{1+\epsilon}(p_{\vx_t}\|\Phi_t) = v_{\epsilon} < \infty,
	\end{align*}
	where $\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}$,
\end{restatable}
%
which can be easily enforced through careful policy (or hyperpolicy) design, as discussed in \ref{subsec:boundrenyi}.

\subsection{Discrete arm set}
We start from the discrete case, where $|\mathcal{X}| = K \in \Naturals_{+}$.
This setting is particularly convenient, as the optimization step can be trivially solved in time $\mathcal{O}(Kt)$ per iteration,\footnote{We consider the evaluation of pdf's, payoffs and \Renyi divergences in (\ref{eq:wcmu}) atomic, as its complexity is heavily problem-dependent.} where $t$ is from evaluation of (\ref{eq:wcmu}) via clever caching. This sums up to total time $\mathcal{O}(KT^2)$. This setting is also of practical interest: even in applications where $\Xspace$ is naturally continuous (\eg robotics), the set of solutions that can be actually tried in practice may sometimes be constrained to a discrete, reasonably small set. In this simple setting, \gls{OPTIMIST} achieves $\wt{\mathcal{O}}(T^{\frac{1}{1+\epsilon}})$ regret:

\begin{restatable}{theorem}{regretdiscrete}\label{th:regretdiscrete}
	Let $\Xspace$ be a discrete arm set with ${|\mathcal{X}| = K \in \Naturals_{+}}$. Under Assumption (\ref{ass:boundrenyi}), Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{3\delta}{t^2\pi^2K}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
		&\Reg(T) \leq \Delta_0 + C
			T^{\frac{1}{1+\epsilon}}
			\left[v_{\epsilon}
			\left(2\log T + \log \frac{\pi^2K}{3\delta}\right)
			\right]^{\frac{\epsilon}{1+\epsilon}},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
This yields a $\wt{\mathcal{O}}(\sqrt{T})$ regret when $\epsilon=1$.


\subsection{Compact arm set}
Now, we consider the more general case of a compact arm set $\Xspace\in\Reals^d$. This case is also more interesting as it allows to tackle virtually any RL task. We can assume, \wlg, that $\Xspace$ is entirely contained in a box $[-D,D]^d$, with $D\in\Reals_{+}$. We also need the following assumption on the expected payoff:
%
\begin{restatable}{assumption}{lipschitz}\label{ass:lipschitz}
	The expected payoff $\mu$ is Lipschitz continuous, \ie there exists a constant $L>0$ such that, for every $\vx, \vx'\in\Xspace$:
	\begin{align*}
		|\mu(\vx') - \mu(\vx)| \leq L\norm[1]{\vx - \vx'}.
	\end{align*}
\end{restatable}
%
This assumption is easily satisfied for policy optimization, as shown in the following:
%
\begin{restatable}{lemma}{lipschitzpol}\label{lem:lispschitzpol}
	In the policy optimization problem, Assumption \ref{ass:lipschitz} can be replaced by:
	\begin{align}\label{eq:lp1}
		\sup_{s\in\Sspace,\vtheta\in\Theta}\Exp_{a\sim\pi_{\vtheta}}
		\left[\left|\nabla_{\vtheta}\log\pi_{\vtheta}(a|s)\right|\right] \leq \vu_1,
	\end{align}
	in the action-based paradigm, and by:
	\begin{align}\label{eq:lp2}
		\sup_{\vxi\in\Xi}\Exp_{\vtheta\sim\rho_{\vxi}}
		\left[\left|\nabla_{\vxi}\log\nu_{\vxi}(\vtheta)\right|\right] \leq \vu_2,
	\end{align}
	in the parameter-based paradigm, where $\vu_1$ and $\vu_2$ are $d$-dimensional vectors and the inequalities are component-wise.
\end{restatable}
%
In the proof (Appendix \ref{app:proof}), we show how to derive the corresponding Lipschitz constants, and show how (\ref{eq:lp1}) and (\ref{eq:lp2}) are satisfied by the commonly-used Gaussian policy and hyperpolicy, respectively. This is enough for \gls{OPTIMIST} to achieve $\wt{\mathcal{O}}(d^{\frac{\epsilon}{1+\epsilon}}T^{\frac{1}{1+\epsilon}})$ regret:
%
\begin{restatable}{theorem}{regretcompact}\label{th:regretcompact}
	Let $\Xspace$ be a $d$-dimensional compact arm set with $\Xspace \subseteq [-D,D]^d$. Under Assumptions (\ref{ass:boundrenyi}) and (\ref{ass:lipschitz}), Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{6\delta}{\pi^2t^2(1+d^dt^{2d})}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
	&\Reg(T) \leq \Delta_0 + C
	T^{\frac{1}{1+\epsilon}}
	\left[v_{\epsilon}
	\left(2(d+1)\log T + d\log d + \log \frac{\pi^2}{3\delta}\right)
	\right]^{\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
%
This yields a $\wt{\mathcal{O}}(\sqrt{dT})$ regret when $\epsilon=1$. Unfortunately, the optimization step may be very time-consuming. In some applications, we can assume the time required to draw samples to dominate the computational time. In fact, drawing a sample (Algorithm \ref{alg:1}, line \ref{line:sampling}) corresponds to generating a whole trajectory of experience, which may take a long time, especially in real-world applications.

\begin{algorithm}[t]
	\caption{\gls{OPTIMIST}2}
	\label{alg:2}
	\begin{algorithmic}[1]
		\State {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$, discretization schedule $(\tau_t)_{t=1}^T$, order $\epsilon\in(0,1]$
		\State Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$
		\For{$t=1,\dots,T$}
		\State Discretize $\Xspace$ with a uniform grid $\wt{\Xspace}_t$ of $\tau_t^d$ points
		\State Select arm $\vx_t = \arg\max_{\vx\in\wt{\Xspace}_t}B_t^{\epsilon}(\vx,\delta_t)$
		\State Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
		\EndFor
	\end{algorithmic}
\end{algorithm}
%

\subsection{Discretization}
When optimization over the infinite arm space $\Xspace$ is not feasible, Algorithm \ref{alg:2} can be used instead. This variant restricts the optimization to a progressively finer grid $\wt{\Xspace}_t$ of $(\tau_t)^d$ vertices. A reasonably coarse discretization schedule can be used at the price of a worse (but still sublinear) regret:
%
\begin{restatable}{theorem}{regretdiscretized}\label{th:regretdiscretized}
	Let $\Xspace$ be a $d$-dimensional compact arm set with $\Xspace \subseteq [-D,D]^d$. For any $\kappa\geq2$, under Assumptions (\ref{ass:boundrenyi}) and (\ref{ass:lipschitz}), Algorithm \ref{alg:2} with confidence schedule ${\delta_t = \frac{6\delta}{\pi^2t^2\left(1+\left\lceil t^{\nicefrac{1}{\kappa}}\right\rceil^d\right)}}$ and discretization schedule $\tau_t=\lceil t^{\frac{1}{\kappa}} \rceil$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
	\MoveEqLeft\Reg(T) \notag\\
	&\leq \Delta_0  + C_1T^{\left(1-\frac{1}{\kappa}\right)}d
	+ C_2
	T^{\frac{1}{1+\epsilon}}\cdot\left[v_{\epsilon}
	\left(\left(2+ \nicefrac{d}{\kappa}\right)\log T + d\log 2 + \log\frac{\pi^2}{3\delta}\right)\right]^{\frac{\epsilon}{1+\epsilon}},
	\end{align*}
	where $C_1=\frac{\kappa}{\kappa-1}LD$, $C_2=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
%
Let us focus on the case $\epsilon=1$, which is the only one of practical interest in the scope of this paper.
For $\kappa=2$, we obtain regret $\wt{\mathcal{O}}(d\sqrt{T})$. Unfortunately, the time required for optimization is exponential in arm space dimensionality $d$, \ie $\mathcal{O}(\lceil t^{\frac{1}{\kappa}} \rceil^d)$. For $d\geq 2$, we can break the curse of dimensionality by taking $\kappa=d$. In this case, the regret is $\wt{\mathcal{O}}\left(dT^{\left(1-\frac{1}{d}\right)}\right)$. On the other hand, the time per iteration is only $\mathcal{O}(t^2)$. Note that the regret is sublinear for any choice of $\kappa$. 
Going further: for any $\zeta>0$, $\kappa=\frac{d}{\zeta}$ grants $O(t^{1+\zeta})$ time per iteration at the cost of $\wt{\mathcal{O}}\left(dT^{\left(1-\frac{\zeta}{d}\right)}\right)$ regret. 

\begin{remark}
The worse dependency $\wt{\mathcal{O}}(d)$ of the regret on the arm space dimensionality (\wrt $\wt{\mathcal{O}}(\sqrt{d})$ of Algorithm \ref{alg:1}) is also necessary to prevent the time per iteration from being exponential in $d$.
\end{remark}