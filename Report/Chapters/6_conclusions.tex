% !TEX root = ../thesis.tex

\chapter{Conclusions} \label{ch:conclusions}
In this chapter, we revisit our original contributions to the \gls{RL} literature. Then, we summarize the limitations of our work and suggest some possible
directions for future research that stem from these limitations.


\section{Recapitulation}
In this thesis we studied the exploration-exploitation problem in \gls{PS} by leveraging \gls{MAB} techniques. After a thorough literature review of both \gls{MAB} and \gls{RL}, we developed a threefold contribution to the existing work.

In Section \ref{sec:problem}, we provided an ad-hoc formalization of the online, episodic policy optimization problem. This formulation does not fall within the traditional \gls{MAB} framework, but builds upon it. At every decision epoch, the agent selects a parametrization of its policy (or hyperpolicy) as it would pull a bandit arm, draws a trajectory with it, and observes its payoff, \ie the cumulative return of the trajectory. The goal is to maximize the expected total payoff. The peculiarity of this framework \wrt the classic \gls{MAB} one, is the special structure existing over the arms, induced by the common sample space of the stochastic arms (the agent's policy parametrizations).

In Section \ref{sec:algo} we exploited the correlation between arms to guarantee efficient exploration by leveraging the \gls{OFU} principle and multiple importance sampling. In particular, we devised \gls{OPTIMIST}, a suitable algorithm capable of learning within a discrete set of arms. An intuitive extension of it is discussed in Section \ref{sec:regret}: \gls{OPTIMIST}2 allows learning within a compact set of arms, by means of iterative discretization of the continuous space. Both algorithms have been backed by theoretical guarantees on the convergence of their cumulative regret, \ie sub-linear regret, under assumptions that are easy to meet in practice. Also, they have no equivalents in the \gls{PS} literature, which strongly lacks studies on both exploration and convergence properties of the existing algorithms.

In Chapter \ref{ch:experiments}, we carried out several numerical simulations to test the proposed algorithms on multiple tasks, and to compare them with other classical \gls{MAB} and \gls{RL} algorithms. As we hoped, \gls{OPTIMIST} (and its extension) proved to be very effective in exploring the (hyper)parameter space by leveraging its structure. In particular, its exploitative behaviour revealed to be more efficient than similar \gls{MAB} algorithms such as \gls{UCB}1, and more effective than policy search algorithms such as \gls{PGPE} and \gls{PBPOIS}. Nonetheless, numerical simulations shed light on several limitations of the proposed algorithms.

\section{Limitations and Future Works}
The most evident limitation of \gls{OPTIMIST}, revealed by numerical simulations, consists in the inefficient way it optimizes its upper bound index (\ref{eq:optimistindex}). Taking the \emph{argmax} over a discrete set is very expensive and becomes unfeasible for large, multidimensional state spaces. This problem showed up clearly in the Mountain Car and Inverted Pendulum experiments. Given our limited computing capacity and time, we could neither experiment with finer discretization schedules (smaller $k$), nor optimize the covariance parameters, which would have meant to double the number of arms (the covariance being diagonal). 
Another limitation has been already discussed in Section \ref{sec:actionbased}. Computing the exploration bonus is unfeasible because we lack proper estimators for the \Renyi divergence between a target and a mixture policy. To the best of our knowledge, the estimators available in the literature are insufficient for modelling the \Renyi distance between probability distributions, as they suffer of high variance and generate numerical instabilities during numerical simulations. 
Finally, comparative experiments on \gls{LQG} and  Continuous Mountain Car showed that \gls{OPTIMIST} struggles to keep pace with the performances of more exploitative algorithms whenever an extensive exploration is not required. In particular, after having performed many epochs and extensively explored the parameter space, it would be desirable to start exploiting more heavily. However (we should say, as expected), \gls{OPTIMIST} consistently sticks to the \gls{OFU} principle, which inherently brings to continuous exploration even in tasks in which little exploration is enough.

The limitations discussed above constitute a natural starting point for future developments.
Future works should focus on finding more efficient ways to perform optimization in
the infinite-arms setting. The authors of \gls{GPUCB}, facing a similar optimization problem, suggest the adoption of global search heuristics. Instead, the \gls{RL} community generally favours gradient descent. However, the latter displays bad performances when optimizing multimodal functions, like our upper confidence bound. Nonetheless, there exist many different variations of this technique (natural gradient, stein variational gradient, gradient with momentum etc.), and some of them could reveal effective.
On an other track, parallel to the first one, future work could investigate appropriate estimators for the exploration bonus in the action-based setting. Also, they could study how to mitigate the strong exploration drive of \gls{OPTIMIST} whenever it proves to be excessive.
Finally, an interesting line of research would be to study suitable integrations of our problem formalization and bounds with posterior sampling. Indeed, exploiting the sample efficiency that characterizes \gls{TS} seems promising, and already proved to be a more powerful solution than optimism in some \gls{RL} scenarios, as discussed in Section \ref{sub:posteriorRL}. 

To sum up, we have proposed an innovative method to perform directed exploration in \gls{PS} and we have studied it thoroughly. Hopefully, our results will pave the path for further improvements on this important challenge that remains inadequately studied.