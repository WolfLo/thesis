\section{Exploration in Policy Search}


\begin{frame}{Exploration in Policy Search}
	
	\begin{block}{Undirected exploration}
	Generate actions based on randomness, without any knowledge of the learning process.

	\begin{itemize}[noitemsep,topsep=0pt]
		\item<+(1)-> Ex1: by adopting stochastic policies.
		\item<+(1)-> Ex2: by augmenting rewards with the entropy of the policy: 
		\begin{equation*}
		\Rew(\tau) = \sum_{h=0}^{H-1}\gamma^hr_{h+1} + \mathcal{H}(\pi_{\vtheta}(\cdot|s_h)).
		\end{equation*}
	\end{itemize}
	\end{block}
	
	\onslide<+(1)->{
	\begin{block}{Directed exploration}
	Leverage on the knowledge acquired during learning.
		\begin{itemize}
		\item<+(1)-> Count-based techniques.
		\end{itemize}
	\end{block}
	}
	
\end{frame}

%\begin{frame}{Exploration in Policy Search}
%	
%	\begin{block}{Undirected exploration}
%	\begin{enumerate}
%		\item<+-> Adding noise to action selection by adopting stochastic policies;
%		\item<+-> Augmenting the reward with entropy 
%		\begin{equation*}
%		\theta_{k+1} = \theta_k + \alpha_k 
%       		\tikz[baseline]{
%           		\node[anchor=base] (gradient) {$\nabla_\theta J\left(\theta_k\right)$};
%       	   }
%		\end{equation*}
%	\end{enumerate}
%	\end{block}
%	
%	\onslide<+-|handout:0>{
%		\begin{tikzpicture}[overlay]
%				\node[draw=SteelBlue, circle, line width=3pt, minimum size=1.8cm] at (8.67, 1.65) (g) {};
%				\node[SteelBlue] at (11,-0.2) (q) {\Huge \textbf{?}};
%		        \draw [->, line width=3pt, SteelBlue] (q) to [bend left=35] (g);
%		\end{tikzpicture}
%	}
%\end{frame}

%\begin{frame}{The Keystone of Policy Gradient Algorithms}
%	
%	\onslide<1->
%	\begin{alertblock}{Policy Gradient Theorem}
%		\begin{equation*}
%			\nabla_\theta J(\theta) =
%			\E[\substack{S \sim d^\theta\\A \sim \pi_\theta}]{\nabla_\theta\log
%			\pi_\theta(S,A) Q_{\theta}(S, A)}
%		\end{equation*}
%	\end{alertblock}
%	
%	\onslide<2->{
%	For an episodic environment, the policy gradient can be approximated via Monte-Carlo
%	\begin{equation*}
%		\nabla_\theta J(\theta_k) \approx \frac{1}{M} \sum_{m=0}^M
%		 \sum_{u=0}^{T^{(m)}-1} \nabla_\theta\log \pi_{\theta_k} \left(s_u^{(m)}, a_u^{(m)}\right) \sum_{v \geq u}^{T^{(m)}-1} \gamma^{v-u} r_{v+1}^{(m)}   
%	\end{equation*}
%	}
%	
%	\onslide<3->{
%		However, this estimate is characterized by a large variance. Possible improvements:
%		\begin{enumerate}
%			\item Optimal baseline
%			\item Actor-critic methods
%			\item Natural gradient
%		\end{enumerate}
%	}
%\end{frame}
%
%\begin{frame}{Policy Gradient with Parameter-Based Exploration (PGPE)}
%	\begin{block}{Key Idea}
%		\begin{enumerate}
%			\item<+-> Actions are selected using a deterministic parametric controller $F_\theta$
%			\item<+-> The controller parameters are drawn from a probability distribution $p_\xi$
%			\item<+-> The search for an optimum is performed in the space of the hyperparameters $\xi$
%		\end{enumerate}
%	\end{block}
%	
%	\onslide<+->{
%		More formally, the update scheme becomes
%			\begin{equation*}
%				\xi_{k+1} = \xi_k + \alpha_k \nabla_\xi J(\xi_k)
%			\end{equation*}
%		where the policy gradient is given by
%		\begin{alertblock}{Parameter-Based Policy Gradient Theorem}
%			\begin{equation*}
%				\nabla_\xi J(\xi) = \E[\substack{S \sim d^\xi\\\theta \sim p_\xi}]{\nabla_\xi \log p_\xi(\theta) Q_{\xi}\left(S, F_\theta(S)\right)}
%			\end{equation*}		
%		\end{alertblock}
%	}
%\end{frame}


