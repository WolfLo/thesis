\begin{thebibliography}{}

\bibitem[Bellemare et~al., 2016]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R. (2016).
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1471--1479.

\bibitem[Bubeck et~al., 2013]{bubeck2013bandits}
Bubeck, S., Cesa-Bianchi, N., and Lugosi, G. (2013).
\newblock Bandits with heavy tail.
\newblock {\em IEEE Transactions on Information Theory}, 59(11):7711--7717.

\bibitem[Deisenroth et~al., 2013]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al. (2013).
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends{\textregistered} in Robotics},
  2(1--2):1--142.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 1856--1865.

\bibitem[Lattimore and Szepesv\'{a}ri, 2019]{lattimore2019bandit}
Lattimore, T. and Szepesv\'{a}ri, C. (2019).
\newblock {\em Bandit Algorithms}.
\newblock Cambridge University Press (preprint).

\bibitem[Srinivas et~al., 2010]{srinivas2010gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In F{\"u}rnkranz, J. and Joachims, T., editors, {\em Proceedings of
  the 27th International Conference on Machine Learning (ICML-10)}, pages
  1015--1022, Haifa, Israel. Omnipress.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\end{thebibliography}
