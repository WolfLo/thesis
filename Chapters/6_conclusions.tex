% !TEX root = ../thesis.tex

\chapter{Conclusions} \label{ch:conclusions}
In this chapter, we take the time to revisit our original contributions to the reinforcement learning literature. Then, we recapitulate the limitations of our work and suggest some possible
axis for future research that stem from these limitations.


\section{Recapitulation}
In this thesis we studied the exploration-exploitation problem in \gls{PS} by leveraging \gls{MAB} techniques. After a thorough literature review of both \gls{MAB} and \gls{RL}, we developed a threefold contribution to the existing work. \\
The first contribution is of pure theoretical nature: we provided an ad-hoc formalization of the online, episodic policy optimization problem. This formulation does not fall within the traditional \gls{MAB} framework, but builds upon it. At every decision epoch, the agent selects a parametrization of its policy (or hyperpolicy), as it would pull a bandit arm, draws a trajectory with it, and observes its payoff, \ie the cumulative return of the trajectory. The goal is to maximize the expected total payoff. The peculiarity of this framework \wrt the classic \gls{MAB} one, is the special structure existing over the arms, induced by the common sample space of the stochastic arms (the agent's policy parametrizations). \\
The second contribution is both algorithmic and theoretical: we exploited the correlation between arms to guarantee efficient exploration by leveraging the \gls{OFU} principle and multiple importance sampling. In particular, we devised \gls{OPTIMIST}, a suitable algorithm capable for learning within a discrete set of arms. An intuitive extension of it, \gls{OPTIMIST}2 allows learning within a compact set of arms, by means of iterative discretization of the continuous space. Both algorithms have been backed by theoretical guarantees on the convergence of their cumulative regret, \ie sublinear regret, under assumptions that are easy to met in practice. Also, they have no equivalents in the \gls{PS} literature, which strongly lacks studies on both exploration and convergence properties of the algorithms. \\
The third contribution is mostly experimental: we carried out several numerical simulations to test the algorithms proposed on multiple tasks, and to benchmark them with other classical \gls{MAB} and \gls{RL} algorithms. As we hoped, \gls{OPTIMIST} (and its extension) proved to be very effective in exploring the (hyper)parameter space by leveraging its structure. In particular, its exploitative behaviour revealed to be more efficient than similar \gls{MAB} algorithms such as \gls{UCB}1, and more effective of policy search algorithms such as \gls{PGPE} and \gls{PBPOIS}. Nonetheless, numerical simulations shed light on several limitations of the algorithms proposed.

\section{Limitations and future works}
The most evident limitation of \gls{OPTIMIST}, revealed by numerical simulations, consists in the inefficient way it optimizes its upper bound index (\ref{eq:optimistindex}). Taking the \emph{argmax} over a discrete set is very expensive and becomes unfeasible for large, multidimensional state spaces. This problem showed up clearly in the Mountain Car and Inverted Pendulum experiments. Given our limited computing capacity and time, we could neither experiment with finer discretization schedules (smaller $k$), nor choose to optimize the standard deviation too, which would have meant to double the number of arms.
Another limitation has been already discussed in Section \ref{sec:actionbased}. Computing the exploration bonus is unfeasible because we lack proper estimators for the \Renyi divergence between a target and a mixture policy. To the best of our knowledge, the estimators available in the literature are insufficient for modelling the distance between probability distributions as the \Renyi does. Particularly, when the behavioural is a mixture distribution and it is far away from the target.
Finally, comparative experiments on \gls{LQG} and  Continuous Mountain Car showed that \gls{OPTIMIST} struggles to catch up with the performances of more exploitative algorithms whenever an extensive exploration is not required. In particular, after having performed many epochs and extensively explored the parameter space, it would be desirable to start exploiting more heavily. However (we should say, as expected), \gls{OPTIMIST} consistently sticks to the \gls{OFU} principle, which inherently brings to continuous exploration even in tasks in which little exploration is enough.

The limitations discussed above constitute a natural starting point for future developments.
Future works should focus on finding more efficient ways to perform optimization in
the infinite-arm setting. The authors of \gls{GPUCB}, which faces a similar optimization problem, suggest the adoption of global search heuristics. Instead, the \gls{RL} community generally favours gradient descent. However, this latter is known for performing badly in optimizing multimodal functions, like our bound. Nonetheless, there exist many different variations of this technique (natural gradient, stein variational gradient, gradient with momentum etc.), and some of them could reveal effective.
On an other track, parallel to the first one, future work could investigate appropriate estimators for the exploration bonus in the action-based setting. Also, they could study how to mitigate \gls{OPTIMIST} strong exploration drive whenever it proves to be excessive.
Finally, an interesting line of research would be to study suitable integrations of our problem formalization and bounds with posterior sampling, as described in \ref{sub:posteriorMAB}. Indeed, exploiting the sample efficiency that characterizes \gls{TS} seems promising, and already proved to be a more powerful solution than optimism in some \gls{RL} scenarios, as discussed in \ref{sub:posteriorRL}. 


\textcolor{red}{EXPLAIN CLEVER CASHING?}
\textcolor{red}{Unfortunately, the time required for optimization is exponential in arm space dimensionality=====spendere qualche parola per dire come mai perchè la complessità è quella presentata?}
\textcolor{red}{bigger labels \ref{fig:LQGoptimalgain}}