% !TEX root = ../thesis.tex

\chapter{Exploration Techniques}

In this chapter we present the state of the art of \gls{MAB}s and \gls{RL} techniques that focus on the exploration challenge arising in these learning frameworks. While sketching a sort of taxonomy of the exploration techniques, we describe some emblematic algorithms for each category, with particular attention to those that inspired our work. Indeed, the exploration problem is the main target of our research project and our main contributions, described in Chapters (\ref{ch:core}-\ref{ch:experiments}), have been designed after a careful literature review. In this chapter we try to report the most relevant insights collected along this review. \\
Following the taxonomy outlined by Sebatian Thrun in 1992 for the \gls{RL} framework  \cite{thrun1992efficient}, we divide both \gls{MAB}s and \gls{RL} exploration techniques in two families of exploration schemes: \emph{undirected} and \emph{directed} exploration.
While the former family is closely related to random walk exploration, directed exploration techniques memorize exploration-specific knowledge which is used for guiding the exploration search. In many finite deterministic domains, any learning technique based on undirected exploration is inefficient in terms of learning time, i.e. learning time is expected to scale exponentially with the size of the state space \cite{whitehead1991study}. For all these domains, reinforcement learning using a directed technique can always be performed in polynomial time, demonstrating the important role of exploration in reinforcement learning \cite{thrun1992efficient}. However, an important remark is that we do not address the problem of pure exploration, in which there is no price to be paid for exploration and the only objective is to find the reward-maximizing actions. Instead, we investigate effective exploration in the more general context of learning described in the previous chapter (\ref{eedilemma}), where the agent faces the exploration-exploitation dilemma all along its journey. How can the agent achieve a better balance between exploration and exploitation? How can it explore more effectively? Can it improve the effectiveness of its exploration as long as it goes along the learning process? We will try to answer these questions in the two sections of this chapter. Each of the sections will start with an overview on undirected exploration techniques, then directed strategies will be investigated more thoroughly.

\section{Exploration in Multi Armed Bandits}

\subsection{Undirected Exploration}
%372
%epsilon-greedy @ pag 105
The most uninformed and basic way of exploring an unknown environment is to generate actions randomly with uniform probability. This method is often applied if exploration costs do not matter during learning. Sometimes tasks are divided into two phases, namely an exploration and an exploitation phase, and costs are not considered during the exploration phase. This is the case of the \emph{explore-then-commit} algorithm (Algorithm (\ref{alg:explorethencommit})) presented in the previous chapter, which explores uniformly the finite number of arms $m$ before starting the exploitation phase, after $mK$ steps. Another common algorithm with undirected uniform exploration is the $\epsilon$\emph{-greedy} algorithm \cite{lattimore2019bandit}. One can think of it as an extension of the \emph{explore-then-commit} algorithm which allows to continue exploring with probability $\epsilon$ even during the performance phase (\ie the exploitation phase). The idea is, after a pure exploration phase of $mK$ steps as for the $explore-then-commit$ algorithm, to exploit the estimated best arm $x_t = \arg \max \wh{\mu}_i(t)$ with probability $(1-\epsilon_t)$ at each time step, and explore uniformly the remaining set of arms with probability $\epsilon_t$ at each step. In 2002, Auer et al. \cite{auer2002finite} analyzed the regret of $\epsilon$\emph{-greedy} with slowly decreasing $\epsilon$ and showed its asymptotic convergence to the optimal solution. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods.\\
Another approach to undirected exploration which is more effective when costs are relevant during learning is \emph{non-uniform} exploration utilizing the current utility estimates to influence action-selection. The higher the expected utility by selecting action $i$, the more likely $i$ gets selected. This ensures that the learning system explores and exploits simultaneously, as it happens for the $\epsilon$-greedy algorithm, but in an intuitively more effective way since actions are not chosen uniformly during exploration. An example of utility-driven non-uniform undirected exploration is the  Boltzmann-distributed exploration \cite{cesa2017boltzmann}. By defining the average reward of an arm as in Equation (\ref{eq:averagereward}):

\begin{equation}
\hat{\mu}_i(t) = \frac{1}{T_i(t)}\sum_{s=1}^{t}\mathds{1}_{\{x_s=i\}}r_s,
\end{equation}

the probability for an action $i$ to get selected is a non-linear function of $\hat{\mu}_i$:

\begin{equation}
Pr(i) = \frac{e^{\hat{\mu}_i \tau^{-1}}}{\sum_{a\in\mathcal{X}}e^{\hat{\mu}_{a}\tau^{-1}}}.
\end{equation}

Here $\tau$ is a gain factor, often called \emph{temperature}, which determines the amount of randomness in the action selection procedure. With $\tau\to 0$ pure exploitation is approached, and with $\tau\to \infty$ the resulting distribution approached the uniform distribution. \\
In the rest of this section, we present some techniques of \emph{directed exploration}, which memorizes knowledge about the learning process itself and utilizes this knowledge for directing the exploration.

\subsection{Count-based exploration and Upper Confidence Bound} \label{count&UCB}
%pag107 of bandit algorithms
Count-based exploration memorizes knowledge about the number of times $T_i(t)$ action $i$ has been visited up to time-step $t$, for all $i\in\mathcal{X}$. Strategies based on counting usually evaluate actions by a linear combination of an exploitation term and an exploration term, called the \emph{exploration bonus}, which is a (usually inverse) function of $T_i(t)$. Hence, action $x_t$ is selected in a deterministic fashion as:

\begin{equation}
x_t = \arg\max_{x\in\mathcal{X}}\{\wh{\mu}_x + bonus(T_x(t-1))\}.
\end{equation}

Such strategy is at the core of the celebrated \gls{UCB} algorithm \cite{lai1985asymptotically, agrawal1995continuum, auer2002finite}, that overcomes many of the limitations of strategies based on exploration followed by commitment. The algorithm has many different forms, depending on the distributional assumptions on the noise. Here, we assume the noise is 1-subgaussian, \ie :

\begin{definition} \label{def:subgaussianity}
\emph{Subgaussianity} A random variable $X$ is $\sigma$-subgaussian if, for all $\lambda\in\Reals$, it holds that $\myExp{\exp(\lambda X)}\leq\exp(\lambda^2\sigma^2/2)$.
\end{definition}

Coupling this property with \emph{Markov's inequality}, we can prove that:

\begin{theorem} \cite{lattimore2019bandit} 
If $X$ is $\sigma$-subgaussian, then, for any $\epsilon\geq 0$,
\begin{align*}
P(X\geq\epsilon)\leq\exp\left(-\frac{\epsilon^2}{2\sigma^2}\right)
\end{align*}
\end{theorem}

From this theorem, we can derive a key corollary which gives us precious information about the concentration of the sample mean around the true mean of independent, $\sigma$-subgaussian random variables: 

\begin{corollary} \label{cor:subgaussian}
Assume that $X_i - \mu$, for $i=1,2,\dots,n$ are independent, $\sigma$-subgaussian random variables. Then, for any $\epsilon\geq 0$, with probability at least $1-\delta$, $\delta\in [0,1]$: 
\begin{align*}
\mu\leq\wh{\mu}+\sqrt{\frac{2\sigma^2\log(1/\delta)}{n}} \enspace\enspace and \enspace\enspace \mu\geq\wh{\mu}-\sqrt{\frac{2\sigma^2\log(1/\delta)}{n}},
\end{align*}
where $\wh{\mu}=\frac{1}{n}\sum_{t=1}^{n}X_t$.
\end{corollary}

When considering its options in round t the learner has observed $T_i(t-1)$ samples from arm $i$ and received rewards from that arm with an empirical mean of $\wh{\mu}_i$. Then an \emph{optimist} candidate for the unknown mean of the $i$th arm is:

\begin{align} \label{eq:ucb}
UCB_i(t-1, \delta) = \wh{\mu}_i(t-1) + \sqrt{\frac{2\log(1/\delta)}{T_i(t-1)}}.
\end{align}

Great care is required when comparing (\ref{cor:subgaussian}) and (\ref{eq:ucb}) because in the former the number of samples is the constant $n$, but in the latter it is a random variable
$T_i(t-1)$. A part from this technicality, the intuition remains that $\delta$ is approximately an upper bound on the probability of the event that the above quantity is an underestimate of the true mean. In fact, by Corollary (\ref{cor:subgaussian}), for any $\delta\in [0,1]$:

\begin{align}
Pr\left(\mu\geq\wh{\mu}+\sqrt{\frac{2\sigma^2\log(1/\delta)}{n}}\right)\leq\delta.
\end{align}

After having picked all actions once, the \gls{UCB} policy simply picks at each time step $t$ the action $i$ whose $UCB_i(t-1,\delta)$ index is maximum, as shown in Algorithm (\ref{alg:ucb}).

\begin{algorithm}[t]
	\caption{\gls{UCB}($\delta$) algorithm}
	\label{alg:ucb}
	\begin{algorithmic}[1]
	\State {\bfseries Input:} $K$ arms and $\delta$
	\State Choose each action once
	\For{$t=1,2,\dots,T$}
		\If{$t\leq K$}
			\State $x_{t} = t$
		\Else
			\State $x_t = \arg\max_i UCB_i(t-1, \delta)$
		\EndIf
	\EndFor
	\end{algorithmic}
\end{algorithm}


The value of $1-\delta$ is called the \emph{confidence level}, and different choices lead to different algorithms, each with their pros and cons, and sometimes different analysis. For example, Auer et al. \cite{auer2002finite}, present an algorithm originating in \cite{agrawal1995continuum} called \emph{UCB1} which achieves asymptotic logarithmic regret $\mathcal{O}(\log{T})$, for all reward distributions, with no prior knowledge of the reward distribution. Equivalently, we say that UCB1 achieves asymptotic sub-linear pseudo-regret $\widetilde{\mathcal{O}}(T)$. \emph{UCB1} takes $\delta=1/t$, where $t$ is the current time step, resulting in:

\begin{align}
UCB1_i(t, \delta) = \wh{\mu}_i(t) + \sqrt{\frac{2\log(t)}{T_i(t)}}
\end{align}

However, rather than considering 1-subgaussian environments, Auer et al. \cite{auer2002finite} considers bandits where the payoffs are confined to the $[0,1]$ interval, which are ensured to be 1/2-subgaussian. Better bounds have been proven for modifications of the UCB1 algorithm, for example Improved UCB \cite{auer2010ucb} and Kullback-Leibler Upper confidence Bound (KL-UCB) \cite{garivier2011kl}.

\subsection{Optimism in the Face of Uncertainty}

The family of \gls{UCB} algorithms is of particular relevance because represents a successful implementation of the principle of \gls{OFU}, which is applicable to various exploration problems, not only finite-armed
stochastic bandits. The \gls{OFU} principle states that one should choose their actions as if the environment is as nice as plausibly possible. \\
To illustrate the intuition imagine of being in the same situation depicted in (\ref{eedilemma}), \ie a standard lunch break from work. You can choose between your good old-fashioned favourite restaurant or sampling a restaurant that you never visited before. Taking an optimistic view of the unknown restaurant leads to exploration because without data it \emph{could} be amazing. Then, after trying the new option a few times you can update your statistics about each choice and make a more
informed decision. On the other hand, taking a pessimistic view of the new option discourages exploration and you may suffer significant regret if the local options are delicious. The \gls{UCB} strategy assigns to each arm an upper confidence bound  that with high probability is an overestimate of the unknown mean reward. The intuitive reason why
this leads to sublinear regret is simple. Assuming the upper confidence bound assigned to the optimal arm is indeed an overestimate, then another arm can only be played if its upper confidence bound is larger than that of the optimal arm, which in turn is larger than the mean of the optimal arm. And yet this cannot happen too often because the additional data provided by playing a suboptimal arm means that the upper confidence bound for this arm will eventually fall below that of the optimal arm.\\
The next algorithm that we discuss will be useful to better understand the \gls{OFU} principle and its exploration power, with application to a wider set of arms.

\subsection{ Hierarchical Optimistic Optimization}
In 2011, Bubeck et al. \cite{bubeck2011x} proposed a novel optimistic arm selection strategy whose regret improved upon previous results on continuum and Lipschitz bandits (\eg, \cite{kleinberg2005nearly, kleinberg2008multi}), extending and generalizing the environment class of application to arbitrary topological spaces. In particular, the setting is that of $\mathcal{X}$-armed bandits, introduced in (\ref{sub:xarmedbandits}). This algorithm is particularly interesting to us for two reasons:

\begin{enumerate}
\item it applies the \gls{OFU} principle to continuous action spaces, as we wish to do in the context of \gls{RL} Policy Search;
\item it leverages the correlation between arms (expressed as a dissimilarity function) to explore the environment more effectively.
\end{enumerate} 

The \gls{HOO} strategy assumes that the decision maker is able to cover the space of arms in a recursive manner, successively refining the regions in the covering such that the diameters of these sets shrink at a known geometric rate when measured with the dissimilarity metric. In particular, the authors define a tree of coverings as:

\begin{definition} \label{def:treeofcoverings}
(\emph{Tree of coverings}) A tree of coverings is a family of measurable subsets $(\mathcal{P}_{(h,i)})_{1\leq i\leq 2^h, h\geq 0}$ of $\mathcal{X}$ such that for all fixed integer $h\geq 0$, the covering $\bigcup_{1\leq i\leq 2^h}P_{h,i}=\mathcal{X}$ holds. Moreover, the elements of the covering are obtained recursively: each subset $P_{h,i}$ is covered by the two subsets $P_{h+1,2i-1}$ and $P_{h+1,2i}$.
\end{definition}

\begin{remark}
A typical choice for the coverings in a cubic domain is to let the domains be hyper-rectangles. They can be obtained, for example, in a dyadic manner, by splitting at each step hyper-rectangles in the middle along their longest side, in an axis parallel manner; if all sides are equal, we split them along the first axis.
\end{remark} 

The number of visits of a node $(h,i)$ up to time-step $T$ is given by the number of visits of every node belonging to $\mathcal{C}(h,i)$, \ie the set of the node $(h,i)$ and its descendants:

\begin{align}
T_{h,i}(t) = \sum_{l=1}^{t}\mathds{1}_{\{(h_l, i_l)\in\mathcal{C}(h,i)\}},
\end{align}

where $(h_l, i_l)$ are the coordinates of the node selected at time step $l$. Then, the empirical average of the rewards received for the time-points when the path followed the algorithm has gone through $(h,i)$ is:

\begin{align}
\wh{\mu}_{h,i}(t) = \frac{1}{T_{h,i}(t)}\sum_{l=1}^{t}r_l\mathds{1}_{\{(h_l, i_l)\in\mathcal{C}(h,i)\}}.
\end{align}

Thus, similarly to the $UCB(\delta)$ algorithm, an upper confidence bound can be defined as:

\begin{align}
U_{h,i}(t) = \wh{\mu}_{h,i}(t) + \sqrt{\frac{2\log{t}}{T_{h,i}(t)}} + \nu_1\rho^h,
\end{align}

where $\rho\in(0,1)$ and $\nu_1\in\Reals^+_0$ are parameters of the algorithm. Along with the nodes the algorithm stores what the authors call B-values:

\begin{align}
B_{h,i}(t) = \min\{U_{h,i}(t), \max\{B_{h+1,2i-1}(t), B_{h+1,2i}(t)\}\}
\end{align}

Finally, let us denote by $\Tau$ the infinite tree of coverings, by $\Tau_t$ the set of nodes of the tree that have been picked in previous rounds and by $\mathcal{S}_t$ the nodes which are not in $\Tau_t$. Now, for a node $(h,i)$ in $\mathcal{S}_t$, we define
its B-value to be $B_{h,i}(t)=+\infty$ . We now have everything we need to present the full \gls{HOO} strategy in Algorithm (\ref{alg:hoo}).

\begin{algorithm}[t]
	\caption{\gls{HOO} algorithm}
	\label{alg:hoo}
	\begin{algorithmic}[1]
	\State {\bfseries Input:} the infinite tree of coverings $\Tau$, $\rho\in(0,1)$ and $\nu_1\in\Reals^+_0$
	\State {\bfseries Initialize:} $t=0$, $\Tau_0=\emptyset$, $\mathcal{S}_0=\Tau$, $B_{h,i}(0)=+\infty$ $\forall (h,i)\in\mathcal{S}_0$
	\State Choose the root node $(h_0, i_0)=(0,1)$ and update $t = t+1$
	\For{$t=1,2,\dots,T$}
		\State Choose a node according to the deterministic rule:
		\begin{align}
		(h_t, i_t) = \arg\max_{(h, i)\in\mathcal{S}_t}B_{h,i}(t)
		\end{align}
		\State Choose, possibly at random, an arm $x_t\in\mathcal{P}_{(h_t, i_t)}$ and collect reward $r_t$
		\State Update $B_{h,i}(t)$ for $(h_t,i_t)$ and all its parent nodes
		\State $\Tau_{t+1}=\Tau_t\cup(h_t,i_t)$, $\mathcal{S}_{t+1}=\mathcal{S}_t/(h_t,i_t)$
	\EndFor
	\end{algorithmic}
\end{algorithm}

The definition and use of B-values, that puts in relationship each node (subset of $\Xspace$) with all its children (subsets of the subset of $\Xspace$), shows how the algorithm relies on the correlation between the reward of an arm and that of its neighbours. Indeed, the proof of the regret achieved by \gls{HOO} relies on the following assumption:

\begin{assumption}
The mean-payoff function $\mu$ is weakly Lipschitz \wrt a dissimilarity metric $\ell$, \ie $\forall x_1,x_2\in\Xspace$,
\begin{align}
\mu^*-\mu(x_2)\leq\mu^* - \mu(x_1)+\max\{\mu^* - \mu(x_1), \ell(x_1,x_2)\}. \label{eq:weaklipschitz}
\end{align}
\end{assumption}

Note that weak Lipschitzness is satisfied whenever $\mu$ is 1-Lipschitz, \ie $\forall x_1,x_2\in\Xspace, \; |\mu(x_1)-\mu(x_2)|\leq\ell(x_1,x_2)$. On the other hand, weak Lipschitzness implies local (one-sided) 1-Lipschitzness at any maxima. Indeed, at an optimal arm $x^*$, Equation (\ref{eq:weaklipschitz}) rewrites to $\mu(x^*)-\mu(x_2)\leq\ell(x^*,x_2)$. \\
By carefully choosing the tree of coverings and the algorithm parameters, the authors show that the regret of the proposed approach is bounded by $\widetilde{\mathcal{O}}(\sqrt{T}) = \mathcal{O}(\log\sqrt{T})$. For more details, refer to \cite{bubeck2011x}.


\subsection{Posterior Sampling}
Another way to address the exploration-exploitation dilemma in \gls{MAB}s is by posterior sampling. Particularly relevant is an heuristic called \gls{TS}, firstly introduced by \cite{thompson1933likelihood}, which has been thoroughly studied in the literature for its good properties. This technique is considered to be close to \gls{UCB} algorithms because it also allocates exploratory effort to actions that might be optimal \ie it also builds upon the \gls{OFU} principle \cite{russo2013eluder}. Indeed, similarly to \gls{UCB}, \gls{TS} avoids the under-exploitation of actions that are potentially good and prevents from wasting time on actions that are already known to be bad. The Bayesian approach is what sets \gls{TS} apart from the techniques previously mentioned. It requires a prior distribution for each arm as input, which encodes the initial belief that a learner has on each arm reward. Subsequently, at each round $t$, it samples from each arm distribution, chooses the action with the best reward and updates its posterior distribution by means of the Bayes'rule.\\ The elements of \gls{TS} are the followings:

\begin{enumerate}
\item a set $\Theta$ or parameters $\theta$ of the distribution of $r\sim\Rew_{\theta}(\cdot|x)$;
\item a prior distribution $P(\theta)$ on these parameters;
\item the history $\mathcal{I}_t = \{x_0, r_0, x_1, r_1, \dots, x_{t-1}, r_{t-1}\}$ of past observations;
\item a likelihood function $P(\mathcal{I}_t|\theta)$;
\item a posterior distribution $P(\theta|\mathcal{I}_t)\propto P(\mathcal{I}_t|\theta)P(\theta)$.
\end{enumerate}

\gls{TS} consists in playing the action $x\in\Xspace$ according to the probability that it maximizes the expected reward according to your belief, \ie:

\begin{align}
\arg\max_{x\in\Xspace}\Exp[r|x] &= \arg\max_{x\in\Xspace}\int\Exp[r|\theta,x]P(\theta|\mathcal{I}_t)d\theta
\end{align}

Once having observed the reward from the chosen arm, the posterior distribution can be updated using the Bayes' rule. In practice, it is either impossible or computationally expensive to compute the integral above. Therefore, usually we resort to sampling $\hat{\theta}_t\sim P(\theta|\mathcal{I}_t)$ at time $t$, and then play the arm $x_t=\arg\max_{x\in\Xspace} \Exp[r|\hat{\theta}_t,x]$.  Conceptually, this means that the player instantiates their beliefs randomly in each round, and then acts optimally according to them.  \\
In \cite{agrawal2013further} the authors prove that for the K-armed stochastic bandit problem, Thompson Sampling has expected regret of

\begin{align}
\Exp[Regret(T)]\leq\mathcal(\sqrt{KT\log T}),
\end{align}

which is identical to the best known problem-independent bound for the expected regret of UCB1 \cite{bubeck2012regret}.

\textbf{Example: Thompson Sampling for Bernoulli MAB}
Suppose that the reward function of each arm $i$, $i=1,2,\dots,K$ is a Bernoulli of parameter $p_i$: $r_t,i\sim\mathcal{B}(p_i)$ \ie the result of pulling an arm is either a \emph{success} (with probability $p_i$) or a \emph{failure} (with probability $1-p_i$). We know that, given a uniform prior over the parameter $p\sim\mathcal{U}(0,1)=Beta(1,1)$, and having observed $\alpha-1$ successes and $\beta-1$ failures, the posterior distribution of the parameter $p$ of a Bernoulli is a Beta distribution of parameters $\alpha$ and $\beta$ (Beta distributions are frequently adopted as priors because of their conjugacy properties). Hence, \gls{TS} can be performed as shown in Algorithm(\ref{alg:bernoulliTS}).

\begin{algorithm}[t]
	\caption{\gls{TS} for Bernoulli MAB with Beta priors}
	\label{alg:bernoulliTS}
	\begin{algorithmic}[1]
	\State {\bfseries Initialize:} the prior distribution for each arm as $Beta(1,1)$
	\For{$t=1,2,\dots,T$}
		\For{$i=1,2,\dots,K$}
			\State Sample $\hat{p}_i\sim Beta(\alpha_i,\beta_i)$
		\EndFor
		\State Pull arm $x_t=\arg\max_i\hat{p}_i$ and observe $r_t\in\{0,1\}$
		\State Update distribution:
		\begin{align}
			(\alpha_i,\beta_i) = \begin{cases}(\alpha_i,\beta_i), &\text{if }x_t\neq i \\ (\alpha_i,\beta_i) + (r_t,1-r_t), &\text{if } x_t= i\end{cases}
		\end{align}
	\EndFor
	\end{algorithmic}
\end{algorithm}

For a more extensive discussion on algorithms implementing \gls{TS}  refer to \cite{russo2018tutorial}.
\\

\subsection{Gaussian Process Upper Confidence Bound}
We conclude by presenting an algorithm, \gls{GPUCB} \cite{srinivas2010gaussian}, of particular interest to us for two reasons. First, it serves as a further example of application of the \gls{OFU} principle and of \gls{UCB} variant. Second, because it formalizes the broad problem of optimizing an unknown, noisy function, that is expensive to evaluate, as a multi-armed bandit problem. Indeed, our main contribution builds upon a similar formalization of the \gls{RL} continuous control problem. Moreover, it also represents a case of posterior sampling different from \gls{TS}. \\
Consider the \gls{MAB} problem presented in Chapter (\ref{Preliminaries}), where the reward distribution can be seen as the sum of a reward function (e.g. the expected reward function $\mu(\vx_t)$, that here we note as $f(\vx_t)$) and noise, \ie $y_t = \Rew(\cdot|x_t) = f(\vx_t) + \epsilon_t$. As we already know, the goal is to maximize the sum of the rewards collected by choosing a point of the domain $\vx_t\in\Xspace$ at each round $t$ up to the horizon $T$. For example, we might want to find locations of highest temperature in a building by sequentially activating sensors in a spatial network and regressing on their measurements. Each activation draws battery power, so we want to sample from as few sensors as possible.  In this context, as in many other \gls{MAB} problems, a natural performance metric is the cumulative regret as defined in Definition (\ref{def:regret}). 
In order to enforce some implicit properties like smoothness without relying on any parametric assumption, the authors of \cite{srinivas2010gaussian} model $f$ as a sample from a \gls{GP}: a collection of dependent random variables, one for each $\vx\in\Xspace$, every finite subset of which is multivariate Gaussian distributed \cite{williams2006gaussian}. A \gls{GP} is a stochastic process $GP(\mu(\vx), k(\vx,\vx'))$, specified by its mean $\mu(\vx) = \myExp{f(x)}$ and covariance function $k(\vx,\vx') = \Exp{(f(\vx)-\mu(\vx))(f(\vx')-\mu(\vx'))}$. \gls{GPUCB} generally adopts $GP(0,k(\vx,\vx'))$ as prior distribution over $f$.  A major advantage of working with GPs is the existence of simple analytic formulae for mean and co-variance of the posterior distribution, which allow easy implementation of algorithms. For a noisy sample $\boldsymbol{y}_T = [y_1,y_2,\dots, y_T]^T$ at points $A_T = \{\vx_1,\vx_2,\dots,\vx_T\}$ over $f$, the posterior of $f$ is a GP distribution again specified by mean $\mu_T(\vx)$, covariance $k_T(\vx,\vx')$ and variance $\sigma^2_T(\vx)$:

\begin{align}
\mu_T(\vx) &= \boldsymbol{k}_T(\vx)^T(\boldsymbol{K}_T+\sigma^2\boldsymbol{I})^{-1}\boldsymbol{y}_T, \\
k_T(\vx,\vx') &= k(\vx,\vx')-\boldsymbol{k}_T(\vx)^T(\boldsymbol{K}_T+\sigma^2\boldsymbol{I})^{-1}\boldsymbol{k}_T(\vx'),\\
\sigma^2_T(\vx) &= k_T(\vx,\vx),\\
\end{align}

where $\boldsymbol{k}_T(\vx) = [k(\vx_1,\vx),k(\vx_2,\vx),\dots,k(\vx_T,\vx)]$ and $\boldsymbol{K}_T$ is the positive definite kernel matrix $[k(\vx,\vx')]_{\vx,\vx'\in A_T}$.
The full \gls{GPUCB} procedure, motivated by the \gls{UCB} algorithm, is shown in Algorithm \ref{alg:gpucb}.

\begin{algorithm}[t]
	\caption{\gls{GPUCB}}
	\label{alg:gpucb}
	\begin{algorithmic}[1]
	\State {\bfseries Input:} GP prior $\mu_0=0$, $\mu_0=0$, $\beta_t\in\Reals^+$
	\For{$t=1,2,\dots,T$}
		\State Choose $\vx_t = \arg\max_{\vx\in\Xspace}\mu_{t-1}(\vx)+\sqrt{\beta_t}\sigma_{t-1}(\vx)$
		\State Sample $y_t = f(\vx_t)+\epsilon_t$
		\State Perform Bayesian update to obtain $\mu_t$ and $\sigma_t$
	\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Exploration in Reinforcement Learning}

\subsection{Undirected Exploration}
%check thesis
% pg101 of Bandits
The most classic undirected exploration strategies in \gls{RL} are the same of those introduced earlier for \gls{MAB}s. Indeed, the $\epsilon$\emph{-greedy} policy is one of the most widely used and known algorithm in reinforcement learning \cite{sutton2018reinforcement}. In the same way, Boltzmann exploration, which uses the exponential of the
standard Q-function as the probability of an action, has been extensively studied \cite{thrun1992efficient, cesa2017boltzmann}. Similar policy representations are energy-based models, with
the Q-value obtained from an energy model such as a restricted Boltzmann machine \cite{sallans2004reinforcement}. An interesting family of extensions to these techniques goes under the name of \emph{maximum entropy} \gls{RL}.

\textbf{Maximum Entropy Reinforcement Learning} \\
As stated in Chapter (\ref{Preliminaries}), Equation (\ref{eq:optimalpolicy}), the goal of \gls{RL} is to find the optimal policy $\pi^*$ which maximizes the performance $J(\pi)$. Maximum entropy \gls{RL} augments this objective by introducing an entropy term, such that the optimal policy also aims at maximizing its entropy at each visited state:

\begin{align}
\pi^*=\arg\max_\pi\sum_h\myExpu{s_h,a_h\sim d^{\pi}}{\Rew(s_h,a_h)+\alpha\mathcal{H}(\pi(\cdot|s_h))},
\end{align}

where $\alpha$ is a convenient hyperparameter that can be used to determine the relative importance of entropy and reward. This objective function entails a novel approach to undirected exploration \wrt Boltzmann  exploration. While the latter greedily maximizes entropy at the current time step, maximum entropy \gls{RL} explicitly optimizes for policies that aim to reach states where they will have high entropy in the future. This
distinction is crucial, since the maximum entropy objective can be shown to maximize the entropy of the entire trajectory distribution for the policy $\pi$, while the greedy Boltzmann exploration approach does not. Optimization problems of this type have been covered in a number of scientific papers, \eg \cite{kappen2005path, ziebart2008maximum, haarnoja2017reinforcement}. In \cite{haarnoja2017reinforcement},  Haarnoja et al. extend the objective presented above to infinite time horizons with the introduction of a discount factor $\lambda$:

\begin{align}
\pi^*=\arg\max_\pi\sum_h\myExpu{s_h,a_h\sim d^{\pi}}{\sum_l\gamma^{l-h}\myExpu{s_l,a_l\sim d^{\pi}}{\Rew(s_l,a_l)+\alpha\mathcal{H}(\pi(\cdot|s_l))|s_h,a_h}}.
\end{align}

his objective corresponds to maximizing the discounted expected reward and entropy for future states originating from every state-action tuple $(s_h,a_h)$ weighted by its probability $d^{\pi}$ under the current policy. Then, they define a soft Q-value $Q_{soft}^{\pi}$ for any policy $\pi$ as the expectation under $\pi$ of the discounted sum of rewards and entropy:

\begin{align}
Q_{soft}^{\pi}=r_1 + \myExpu{s_0=s,a_0=a,\tau\sim\pi}{\sum_{h=1}^{\infty}r_{h+1}+\alpha\mathcal{H}(\pi(\cdot|s_h))},
\end{align}

from which they derive the soft Bellman equation. The authors build upon this novel setting an algorithm called \emph{Soft Q-learning}, which has similar mechanisms to traditional \emph{Q-learning} and adopts deep function approximation to compute the Q-values over a continuous domain. Among the advantages presented over other deep \gls{RL} approaches, Soft Q-learning turns out to be more effective for learning multi-modal policies for exploration. In fact, similarly to \gls{MAB}s, during the learning process it is often best to keep trying multiple available options until the agent is confident that one of them is the best. However, deep \gls{RL} algorithms for continuous control typically use unimodal action distributions, which are not well suited to capture such multi-modality. As a consequence, such algorithms may prematurely commit to one mode and converge to suboptimal behaviour, as it happens in the practice. \\\\
Following the same pattern adopted for \gls{MAB}s, we proceed now with presenting some techniques that deals with the exploration challange in a more directed way, \ie by leveraging on relevant information collected during the learning process to lead exploration more effectively.

\subsection{Count-based exploration and Intrinsic Motivation}
The classic, theoretically-justified exploration methods based on counting state-action visitations and turning this count into a bonus reward have been introduced in (\ref{count&UCB}). In recent works \cite{bellemare2016unifying, tang2017exploration, ostrovski2017count, choshen2018dora}, they have also been adapted to large, non-tabular \gls{RL} problems . A relevant example is that of Bellamare et al. \cite{bellemare2016unifying}, later developed in \cite{ostrovski2017count}, which proposes a novel algorithm for deriving a \emph{pseudo-count} from an arbitrary density model. 
Let $\rho$ be a density model on a finite space $\Xspace$ and $\rho_n(x)$ the probability assigned by the model to $x$ after being trained on a sequence of stated $x_1,x_2,\dots,x_n\in\Xspace$. Assume $\rho_n(x)>0$ for all $x,n$. The \emph{recording probability} $\rho_n'(x)$ is then the probability the model would assign to $x$ one more time. The \emph{prediciton gain} of $\rho$ is:

\begin{align}
PG_n(x) = \log\rho_n'(x) - \log\rho_n(x).
\end{align}

$PG_n(x)\geq0$ for all $x\in\Xspace$ whenever the the density model $\rho$ is \emph{learning-positive}, \ie if $\rho_n'(x)\geq\rho_n(x)$ for all $x_1,x_2,\dots,x_n$. For learning -positive $\rho$, the authors define the \emph{pseudo-count} as:

\begin{align}
\wh{N}_n(x) = \frac{\rho_n(x)(1-\rho_n'(x))}{\rho_n'(x)-\rho_n(x)},
\end{align}

derived from postulating that a single observation of $x\in\Xspace$ should lead to a unit increase in pseudo-count:

\begin{align}
\rho_n(x) = \frac{\wh{N}_n(x)}{\hat{n}}, \; \rho_n'(x) = \frac{\wh{N}_n(x)+1}{\hat{n}+1},
\end{align}

where $\hat{n}$ is the \emph{pseudo-count total}. The pseudo-count generalizes the usual state visitation count function $N_n(x)$. Moreover, under certain assumptions on $\rho_n$, pseudo-counts grow approximately linearly with real counts. Crucially, the pseudo-count can be approximated using the prediction gain of the density model:

\begin{align}
\wh{N}_n(x) = \left(e^{PG_n(x)}-1\right)^{-1}
\end{align}

Its main use is to define an \emph{exploration bonus} that can be added to every extrinsic reward $r_n$ received by the agent. For example, at step $n$, the agent would face a total reward $R_n$ given by:

\begin{align}
r'_n = r_n + (\wh{N}_n(x))^{-\frac{1}{2}},
\end{align}

which incentivize the agent to try to re-experience surprising situations. In \cite{ostrovski2017count} the authors adopt PixelCNN, an advanced neural density model for images first introduced in \cite{oord2016pixel}, to supply a pseudo-count. They manage to combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the
state of the art on several hard Atari games, a popular set of \gls{RL} environments for  benchmarking. \\

Interestingly, this notion of pseudo-counts and their application is closely related to the notion of \emph{intrinsic motivation}. In fact, quantities related to prediction gain have been used for similar purposes in the intrinsic motivation literature \cite{lopes2012exploration}, where they measure an agent’s \emph{learning progress} \cite{oudeyer2007intrinsic}. The concept of \emph{intrinsic motivation} has been introduced in the \gls{RL} literature by Singh et al. in \cite{chentanez2005intrinsically}, who borrowed it from psychology. Psychologists call behaviour intrinsically motivated when it is engaged for its own sake rather than as a step toward solving a specific problem of clear practical value. This concept builds on the one of \emph{curiosity}, which has been also extensively investigated in the \gls{RL} literature after being introduced in it by professor Jürgen Schmidhuber in 1991 \cite{schmidhuber1991possibility}. The rationale behind it is that, in some \gls{RL} scenarios, as well as in human life, rewards are supplied to the agent so sparsely that traditional techniques fails miserably in the learning challenge. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state. Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments. Intrinsic motivation / curiosity have been used to instil the need to explore the environment and discover novel states in the agent, regardless of the reward scheme. Most formulations of intrinsic motivation can be grouped into two broad classes:

\begin{enumerate}
\item encourage the agent to explore \emph{novel} states;
\item encourage the agent to perform actions that reduce the error/uncertainty in the agent’s ability to predict the consequence of its own actions, \ie its knowledge about the environment.
\end{enumerate}

Measuring \emph{novelty} requires a statistical model of the distribution of the environmental states, as the one described above, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state $s_{h+1}$ given the current state $s_h$ and the action $a_h$ executed at time $h$. Evidently, these mechanisms, particularly the first one, are closely related to the concept of count-based exploration. Indeed, Bellamare et al. showed in \cite{bellemare2016unifying} the close relationship between the two.\\

\textbf{Variational Information Maximizing Exploration}\\
To help the reader grasping these concepts, we proceed by describing a successful implementation of the principle of intrinsic motivation, the \gls{VIME} algorithm \cite{houthooft2016vime}. \gls{VIME} makes use of the information gain about the agent’s internal belief of the dynamics model as a driving force, as described in the following. The approach taken is model-based, where the agent models the environment dynamics via a model $p(s_{h+1}|s_h,a_h,\theta)$, parametrized by the random variable $\Theta$ with values $\theta\in\boldsymbol{\Theta}$. Assuming a prior $p(\theta)$, it keeps a distribution over dynamic models through a distribution over $\theta$, which is updated in a Bayesian fashion. The authors formalize the intrinsic goal of taking actions that maximize the reduction in uncertainty about the dynamics as taking actions that lead to states that are maximally informative about the dynamics model. In other terms, the agent is encouraged to maximize the mutual information between the next state distribution $S_{h+1}$ and the model parameter $\Theta$:

\begin{align}
I(S_{h+1};\Theta|\xi_h,a_h)=\myExpu{s_{h+1}\sim\Tran(\cdot|\xi_h,a_h)}{D_{KL}[p(\theta|\xi_h,a_h,s_{h+1})||p(\theta|\xi_h)]},
\end{align}

where $D_{KL}(P||Q)$ is the \gls{KL} divergence between probabilities $P$ and $Q$, $\Tran$ are the true environment dynamics and $\xi_h=\{s_1,a_1,...,s_h\}$ is the history of the agent up until time step $t$. This \gls{KL} divergence can be interpreted as information gain. Therefore we can add each term $I(S_{h+1};\Theta|\xi_h,a_h)$ as an \emph{intrinsic reward} to the standard reward $r_h$ obtained by the agent at time step $t$. The trade-off between exploitation and
exploration can now be realized explicitly as follows:

\begin{align}
r'_h = r_h + \eta D_{KL}[p(\theta|\xi_h,a_h,s_{h+1})||p(\theta|\xi_h)], \label{eq:augmentedreward}
\end{align}
 
with $\eta\in\Reals^+$ a hyperparameter controlling the urge to explore. The biggest practical
issue with maximizing information gain for exploration is that the computation of this equation requires calculating the posterior $p(\theta|\xi_h,a_h,s_{h+1})$, which is generally intractable. The practical solution adopted by the authors for calculating the posterior $p(\theta|\mathcal{D})$ for a dataset $\mathcal{D}$ is to approximate it through an alternative distribution $q(\theta;\phi)$, parametrized by $\phi$, by minimizing $D_{KL}[q(\theta;\phi)||p(\theta|\mathcal{D})]$. This is done through maximization of the \emph{variational lower bound} $L[q(\theta;\phi),\mathcal{D}]$, also called \emph{evidence lower bound} \cite{blei2017variational}:

\begin{align}
L[q(\theta;\phi),\mathcal{D}] = \myExpu{\theta\sim q(\cdot|\phi)}{\log p(\mathcal{D}|\theta)}-D_{KL}[q(\theta;\phi)||p(\theta)]. \label{eq:elb}
\end{align}

Rather than computing information gain in Equation (\ref{eq:augmentedreward}) explicitly, we compute an approximation to it, leading to the following total reward:

\begin{align}
r'_h = r_h + \eta D_{KL}[q(\theta;\phi_{h+1})||q(\theta;\phi_h)],
\end{align}

with $\phi_{h+1}$ the updated and $\phi_{t}$ the old parameters representing the agent’s belief. Natural candidates for parametrizing the agent’s dynamics model $q(\theta;\phi)$ are Bayesian neural networks, whose weight distribution ($q(\theta;\phi)$ itself) is given by the fully factorized Gaussian distribution \cite{blundell2015weight}. This is particularly convenient as it allows for a simple analytical formulation of the KL divergence, that can be easily approximated. In fact, \gls{VIME} approximates the KL divergence between the former belief $\phi_{t}$ on the dynamics and the approximate updated belief $\phi'_{t}$. Algorithm \ref{alg:VIME} outline the procedure. For more details about the implementation the reader should refer to \cite{houthooft2016vime}. 

\begin{algorithm}[t]
	\caption{\gls{VIME}}
	\label{alg:VIME}
	\begin{algorithmic}[1]
	\For{each epoch $t$}
		\For{for $h=1,2,\dots,H$}
			\State Generate action $a_h\sim\pi(s_h)$ and sample $s_{h+1}\sim\Tran(\cdot|\xi_h,a_h)$, get $r_h$
			\State Add triplet $(s_h,a_h,s_{h+1})$ to FIFO replay pool R
			\State Compute the approximated divergence $D_{KL}[q(\theta;\phi'_{t}||q(\theta;\phi_{t})]$
			\State Construct $r'_h = r_h + \eta D_{KL}[q(\theta;\phi'_{t}||q(\theta;\phi_{t})]$
		\EndFor
		\State Sample randomly $\mathcal{D}$ from R
		\State Minimize $-L[q(\theta;\phi_t),\mathcal{D}]$ and obtain the updated posterior $q(\theta;\phi_{t+1})$
		\State Use rewards ${r'_1,r'_h,\dots,r'_H,}$ to update policy $\pi$ using any standard \gls{RL} method
	\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Posterior Sampling}
The \gls{TS} technique described in the previous section has been adopted and investigated in the framework of \gls{RL} as well \cite{strens2000bayesian, osband2013more, osband2015bootstrapped, osband2016deep}. Here, \gls{TS} involves sampling a statistically plausibly set of action values (or policies) and selecting the maximizing action (or best policy). These values can be generated, for example, by sampling from the posterior distribution over MDPs and computing the state-action value function of the sampled MDP. This approach, originally proposed in \cite{strens2000bayesian}, is called posterior sampling for reinforcement learning. In \cite{osband2017posterior}, the authors present theoretical and experimental arguments to show that posterior sampling is better than optimism in synthesizing efficient exploration with powerful generalization. Although we build our work leveraging on the principle of optimism in the face of uncertainty, posterior sampling appears to be an interesting option to further investigate. Indeed, as we will see, the formalization of the \gls{RL} problem that we propose in this paper seems to be a good testing ground for \gls{TS}.