% !TEX root = ../thesis.tex

\chapter{Introduction}

With the advent of the 21st century, worldwide economies have been profoundly shaped by an ever-growing access to large amounts of data and fast computers. These two events entailed a new dawn of \gls{AI}, which today represents a dominant field in the technological and scientific landscape, with inevitable repercussions on the whole society. At the heart of this new dawn is \gls{ML} \cite{goodfellow2016deep}, a branch of \gls{AI} that aims at building computers capable of learning from experience, \ie from data. Since computers gather knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge the computer needs to solve complex and intelligent tasks.

Many real-world problems require the solver (the agent) to make a sequence of decisions without knowing the dynamics of the environment in which it is operating and, possibly, without receiving immediate feedback on the goodness of its actions. This type of problems arises in many fields, such as automation control, robotics, finance, operations management, and games. \gls{RL} is a branch of \gls{ML} that deals with such problems and has seen significant advances in the last few decades \cite{sutton2018reinforcement}. In \gls{RL}, an autonomous agent learns how to solve a task by repeated interaction with an unknown environment, which may be only partially observable. The unique information available to the agent is its internal representation of the environment and a feedback (a numerical reward) that serves as an assessment of its actions. Hence, the agent learns by trial and error, as humans would do when learning a new skill without being instructed by a teacher. We refer to the strategy adopted by the agent when interacting with the environment as its \emph{policy}: a mapping, possibly stochastic, that instructs the agent about what action to perform in each state. The goal is to learn a suitable \emph{policy} to maximize the cumulative sum of rewards collected until the end of the task. Since the rewards received by the agent can be infrequent or even very sparse, the agent needs sufficient exploration to improve the knowledge of the unknown environment. Hence, the agent has to \emph{exploit} the most rewarding actions learned, but also has to \emph{explore} in order to improve the action selection in the future. This trade-off, called \emph{exploration-exploitation trade-off}, arises at every decision step of the learning process. Traditional exploration strategies, such as random walks, aim to generate heterogeneous experiences by selecting actions randomly. This is obviously extremely inefficient and quickly leads to unfeasible learning times as the complexity of the problem grows. More advanced exploration techniques, referred to as \emph{directed exploration} \cite{thrun1992efficient}, leverage on the knowledge acquired during learning to explore more efficiently.

One of the current challenges of \gls{RL} is \emph{continuous control}  tasks, such as robotic locomotion, in which states and actions are naturally modelled as real numbers. \gls{PS} is a family of \gls{RL} algorithms that are particularly suited to this class of problems because they scale gracefully with dimensionality and offer appealing convergence guarantees \cite{deisenroth2013survey}. In \gls{PS}, the behaviour of the agent, or \textit{policy}, is explicitly modelled, typically as a stochastic parametric function from states to actions. Learning corresponds to the optimization of a performance measure, typically an estimation of the cumulative sum of rewards, \wrt the policy parameters. 

\section{Motivation and Goal}

The available literature on \gls{PS} focuses mainly on the problem of \textit{finding} the optimal policy with the minimum amount of interaction \cite{sutton2000policy, sehnke2008policy, silver2014deterministic, schulman2015trust, mnih2016asynchronous, espeholt2018impala}. This is well motivated, as interacting with some environments can be very expensive. However, in many cases, we are also interested in the performance of the agent \textit{during} the learning process. This goal is particularly relevant in applications where an agent must be deployed in the real world to perfect its behaviour (\eg robot learning) or to learn from scratch (\eg recommender systems). In such cases, the \textit{exploration-exploitation} dilemma arises naturally, as the agent must continually find the right trade-off between complying with its current expertise or widening it by trial and error. In the context of \gls{PS}, exploration is carried out in the space of policy parameters and this makes exploration particularly necessary. In fact, the parameter space is often characterised by multiple local optima and the common optimization strategies such as gradient ascent frequently get stuck in sub-optimal solutions. To tackle this problem, the scientific community has proposed various strategies, mainly focused on random exploration through the maximization of the entropy of the policy. These solutions do not exploit the information gathered by the agent across iterations to guide exploration. Instead, they only push the agent to consistently explore, in an experience-agnostic fashion, often without any convergence guarantee \cite{ziebart2008maximum, haarnoja2017reinforcement, haarnoja2018soft}. At the moment, the \gls{PS} literature lacks directed strategies for exploring the parameter space. This lack of direct exploration solutions for \gls{PS}, which is of paramount importance for nowadays challenges such as robotics, motivates the research direction of this thesis project. 

The goal of this thesis is to devise novel algorithms, backed by solid theoretical guarantees, which aim at solving the problem of exploration in \gls{PS}. 

\section{Original Contributions}

The problem of maximizing \emph{online} performance, \ie performance during learning, is equivalent to minimizing the agent's total \emph{regret} \wrt an optimal policy, \ie a policy that maximizes performance. This problem has been widely studied in the \gls{MAB} field \cite{auer2002finite,lattimore2019bandit}. In a \gls{MAB} problem, an agent has to repeatedly select an action, called \textit{arm}, in order to maximize an unknown, stochastic reward. This problem can be seen as an \gls{RL} problem in which the environment has a unique state. Since the problem of directed exploration is thoroughly investigated in the \gls{MAB} literature, we started our work by reviewing it to find exploration methods that could be transferred to the \gls{PS} setting.

The first contribution of this thesis is the direct result of this literature review, namely a new formulation of the \gls{PS} problem. Our new formulation looks at \gls{PS} as a peculiar \gls{MAB}-like problem, where the set of available actions (arms) is the parameter space of the agent's policy. This allows us to apply some of the theoretical and algorithmic ideas developed in the \gls{MAB} literature to the problem of exploration in continuous-action \gls{RL}, whose solutions proposed so far have been largely heuristic and undirected \cite{houthooft2016vime,haarnoja2017reinforcement,haarnoja2018soft}. 

In particular, the \gls{OFU} principle at the heart of the \gls{UCB} \cite{lai1985asymptotically,agrawal1995sample,auer2002using} family of \gls{MAB} algorithms lends itself to a relatively straightforward application to \gls{PS}. The core idea of this principle is simply to overestimate the expected reward of arms, which, in our scenario, are the policies that the agent can play. The overestimation is larger for those arms the agent knows less about. To apply the \gls{OFU} principle to policy optimization, we exploited the structure in the way arms (policy parameters) concur to generate rewards. This was both necessary, as the parameter space is typically continuous, and desirable, as there exists an evident correlation between arms (different policies can lead to similar performances) that we exploited in order to devise novel algorithms with sub-linear regret. In practice, we used a statistical technique called \gls{MIS} \cite{veach_optimally_1995} to capture the information shared by different policies and we employed robust estimators inspired by \cite{bubeck2013bandits} to overcome the heavy-tail behaviour typical of importance sampling. We adapted techniques from \cite{metelli2018policy} to build confidence intervals for the expected performance of policy parameters via robust \gls{MIS}. We employed these tools to design two \gls{UCB}-like algorithms for \gls{PS}.

Furthermore, we show how these algorithms can be used both in discrete and continuous parameter spaces and we prove that in both cases they attain a regret bound of $\wt{\mathcal{O}}(\sqrt{T})$. Since the optimization problem can be challenging in the continuous case, we propose a general discretization method that allows to trade computational complexity with regret, preserving the sub-linearity of this latter.

Our final contribution consists of a set of numerical simulations that prove the effectiveness of our algorithms in exploring the parameter space. We show competitive results on the \gls{LQG} and Continuous Mountain Car. Instead, the simulations on the Inverted Pendulum shed light on the limitations of our approach and provided interesting considerations for future developments.

In summary, our original contribution is threefold:
\begin{enumerate}
\item an algorithmic contribution, consisting of two new algorithms for exploration in \gls{PS}, for both discrete and continuous parameter spaces;
\item an ample theoretical contribution, consisting of a novel formulation of the \gls{PS} problem, the definition of a new robust estimator for \gls{MIS}, and high probability regret bounds for both our algorithms;
\item an experimental contribution, consisting of numerical simulations on three \gls{RL} benchmarking tasks of increasing difficulty.
\end{enumerate}

\section{Overview}

In the following, we begin by providing the essential background in Chapter \ref{Preliminaries}. We introduce the reader to the \gls{MAB} problem, describing two settings of particular interest to our research: the stationary stochastic bandits with finitely many arms and the $\mathcal{X}$-armed bandits. We proceed by outlining the \gls{MDP} problem and \gls{RL}, which aims at solving it. Particular attention is paid to \gls{PS} techniques. We conclude by introducing the basic notions of importance sampling and \gls{MIS}, which will be used to devise our algorithms.

In Chapter \ref{ch:sota} we present the highlights of our broad literature review on exploration techniques for \gls{MAB}s and \gls{RL}. In both cases, we start with presenting undirected exploration techniques, then we proceed to analyse more directed techniques, in greater details. This chapter will acquaint the reader with many concepts and algorithms that inspired our work.

Our algorithmic and theoretical contributions are discussed in Chapter \ref{ch:core}. We start by devising a robust importance sampling estimator. Then we re-frame the \gls{PS} framework in a suitable and original fashion. In the second half of this chapter, we derive our algorithms for exploration in \gls{PS}, for both discrete and continuous action-spaces, backed by suitable theorems that guarantee their sub-linear regret.

In Chapter \ref{ch:experiments} we empirically evaluate the proposed methods on three continuous control tasks: \gls{LQG}, Continuous Mountain Car and Inverted Pendulum, and we discuss several insights provided by the experimental results.

In Chapter \ref{ch:conclusions}, we conclude by summarising the results achieved along this thesis, outlining the limitations encountered and possible directions for future work.

Finally, in Appendix \ref{app:proof} we present the proofs of our lemmas and theorems. Every proof goes with comments and remarks to facilitate their understanding and to shed light on the mechanisms underlying our algorithms. 
