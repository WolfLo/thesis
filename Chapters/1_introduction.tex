% !TEX root = ../thesis.tex

\chapter{Introduction}

With the advent of the 21st century, worldwide economies have been profoundly shaped by an ever increasing access to large amounts of data and faster computers. This two elements entailed a new dawn of \gls{AI}, which represents today a dominant field in the technological and scientific landscape, with inevitable repercussions on whole society. At the heart of this new dawn is \gls{ML} \cite{goodfellow2016deep}, a branch of \gls{AI} that aims at building computers capable of learning from experience, \ie the data. Since computers gather knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs to solve complex, intelligent tasks.

Many real-world problems require the solver (the agent) to make a sequence of decisions without knowing the dynamics of the environment in which it is operating and, possibly, without receiving an immediate feedback on the goodness of its actions. This type of problems arises in many fields such as automation control, robotics, finance, operations management and games. \gls{RL} is a branch of \gls{ML} that deals with such problems, and which has seen important advances in the in the last few decades \cite{sutton2018reinforcement}. In \gls{RL}, an autonomous agent learns by repeated interaction with an unknown environment, which is partially observed by the agent. The unique information available to the agent is its internal representation of the environment, and a feedback (a numerical reward) that serves as an evaluation of its actions. Hence, the agent learns by trial and error, as humans would do when learning a new skill without the presence of a teacher. We refer to the strategy adopted by the agent when interacting with the environment as its \emph{policy}: a, possibly stochastic, mapping that instructs the agent about what action to perform in each state. The goal is to learn a suitable \emph{policy} to maximize the cumulative sum of rewards collected until the end of the task. Since the reward received by the agent can be infrequent or even very sparse, the agent needs sufficient exploration in order to improve the knowledge of the unknown environment. Hence, the agent has to \emph{exploit} the most rewarding actions learned, but also has to \emph{explore} in order to make better action selection in the future. This trade-off, called \emph{exploration-exploitation trade-off}, arises at every decision step of the learning process. Traditional exploration strategies, such as random walks, aim to generate heterogeneous experiences by selecting actions randomly. This is obviously extremely inefficient and quickly leads to unfeasible learning times as the complexity of the problem grows. More advanced exploration techniques, referred to as \emph{directed exploration} \cite{thrun1992efficient}, leverage on the knowledge acquired while learning to explore more efficiently.

One of the current challenges of \gls{RL} is to \emph{continuous control}  tasks, such as robotic locomotion, in which states and actions are naturally modelled as real numbers. \gls{PS} \cite{deisenroth2013survey} is a family of \gls{RL} algorithms that are particularly suited to this class of problems because they scale gracefully with dimensionality and offer appealing convergence guarantees. In \gls{PS}, the behaviour of the agent, or \textit{policy}, is explicitly modelled, typically as a stochastic parametric function from states to actions. Learning corresponds to the optimization of a performance measure, typically an estimation of the cumulative sum of rewards, \wrt the policy parameters. 

\section{Motivation and goal}

The available literature on \gls{PS} focuses mainly on the problem of \textit{finding} the optimal policy with the minimum amount of interaction \cite{sutton2000policy, sehnke2008policy, silver2014deterministic, schulman2015trust, mnih2016asynchronous, espeholt2018impala}. This is well motivated, as interacting with some environments can be very expensive. However, in many cases, we are also interested in the performance of the agent \textit{during} the learning process. This goal is particularly relevant in applications where an agent must be deployed in the real world to perfect its behaviour (\eg robot learning) or to learn at all (\eg recommender systems). In such cases, the \textit{exploration-exploitation} dilemma arises naturally, as the agent must continually find the right trade-off between complying with its current expertise or widening it by trial and error. In the context of \gls{PS}, exploration is carried out in the space of policy parameters, and this makes exploration especially needed. In fact, the parameter space is often characterized by multiple local optima, and common optimization strategies such as gradient ascent get stuck in sub-optimal solutions. To tackle this problem, the scientific community has proposed various strategies, mainly focused on random exploration through the maximization of the entropy of the policy. These solutions do not exploit the information gathered by the agent across iterations for guiding exploration. Instead, they only push the agent to consistently explore in an experience-agnostic fashion, often without any convergence guarantees \cite{ziebart2008maximum, haarnoja2017reinforcement, haarnoja2018soft}. At present, the \gls{PS} literature lacks directed strategies for exploring the parameter space.

This lack of direct exploration solutions for \gls{PS}, of paramount importance for nowadays challenges such as robotics, motivates the research direction of this thesis project.

Equivalently, it must minimize its total \textit{regret} \wrt the optimal behaviour. This problem has been thoroughly studied in the field of \gls{MAB} \cite{auer2002finite,lattimore2019bandit}. In this simple framework, an agent has to repeatedly select an action, called an \textit{arm} in this context, in order to maximize an unknown, stochastic reward. This can be seen as \gls{RL} without states.
